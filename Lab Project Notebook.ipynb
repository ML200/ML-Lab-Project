{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dab942d",
   "metadata": {},
   "source": [
    "# Machine Learning Lab Project - Credit Card Overdue Likelihood Prediction\n",
    "##  Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e57de",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "The data for this task is taken from [this](https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction) kaggle dataset. The kaggle page provides two `.csv` files:\n",
    "- application_record.csv\n",
    "- credit_record.csv\n",
    "\n",
    "On a simple level, `application_record.csv` contains the customer data and `credit_record.csv` contains the customers credit history. The specific content is now investigated further.\n",
    "\n",
    "For beeing able to analyse the datasets, the necessary libraries are imported first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511113b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c65ba",
   "metadata": {},
   "source": [
    "In this next step the two `.csv` files are loaded into a pandas datafram. This enables an analysis with the full pandas funcionality, which makes the data understanding process way easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7027e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pd.read_csv(\"Data/application_record.csv\")\n",
    "credit_df = pd.read_csv(\"Data/credit_record.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d875a2",
   "metadata": {},
   "source": [
    "### application_record.csv\n",
    "First, it is important to analyse the columns of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd8bf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 438557 entries, 0 to 438556\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   ID                   438557 non-null  int64  \n",
      " 1   CODE_GENDER          438557 non-null  object \n",
      " 2   FLAG_OWN_CAR         438557 non-null  object \n",
      " 3   FLAG_OWN_REALTY      438557 non-null  object \n",
      " 4   CNT_CHILDREN         438557 non-null  int64  \n",
      " 5   AMT_INCOME_TOTAL     438557 non-null  float64\n",
      " 6   NAME_INCOME_TYPE     438557 non-null  object \n",
      " 7   NAME_EDUCATION_TYPE  438557 non-null  object \n",
      " 8   NAME_FAMILY_STATUS   438557 non-null  object \n",
      " 9   NAME_HOUSING_TYPE    438557 non-null  object \n",
      " 10  DAYS_BIRTH           438557 non-null  int64  \n",
      " 11  DAYS_EMPLOYED        438557 non-null  int64  \n",
      " 12  FLAG_MOBIL           438557 non-null  int64  \n",
      " 13  FLAG_WORK_PHONE      438557 non-null  int64  \n",
      " 14  FLAG_PHONE           438557 non-null  int64  \n",
      " 15  FLAG_EMAIL           438557 non-null  int64  \n",
      " 16  OCCUPATION_TYPE      304354 non-null  object \n",
      " 17  CNT_FAM_MEMBERS      438557 non-null  float64\n",
      "dtypes: float64(2), int64(8), object(8)\n",
      "memory usage: 60.2+ MB\n"
     ]
    }
   ],
   "source": [
    "customer_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829b5e2",
   "metadata": {},
   "source": [
    "As can be seen above, the dataset consists of 17 columns, containing numeral as well as textual data. It also seems as if there is already an unique identifier for every customer in the column `ID`. A concrete description fo these different columns can be retrieved from the datasets [kaggle page](https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction/data):\n",
    "\n",
    "|Feature name|Explanation|Remarks|\n",
    "|:-----------|:----------|:------|\n",
    "|ID \t     |Client number||\n",
    "|CODE_GENDER |\tGender \t||\n",
    "|FLAG_OWN_CAR| \tIs there a car \t||\n",
    "|FLAG_OWN_REALTY| \tIs there a property|| \t\n",
    "|CNT_CHILDREN| \tNumber of children \t||\n",
    "|AMT_INCOME_TOTAL| \tAnnual income \t||\n",
    "|NAME_INCOME_TYPE| \tIncome category \t||\n",
    "|NAME_EDUCATION_TYPE| \tEducation level ||\t\n",
    "|NAME_FAMILY_STATUS| \tMarital status \t||\n",
    "|NAME_HOUSING_TYPE| \tWay of living \t||\n",
    "|DAYS_BIRTH| \tBirthday |\tCount backwards from current day (0), -1 means yesterday|\n",
    "|DAYS_EMPLOYED| \tStart date of employment |\tCount backwards from current day(0). If positive, it means the person currently  unemployed.|\n",
    "|FLAG_MOBIL| \tIs there a mobile phone \t||\n",
    "|FLAG_WORK_PHONE| \tIs there a work phone \t||\n",
    "|FLAG_PHONE| \tIs there a phone \t||\n",
    "|FLAG_EMAIL| \tIs there an email \t||\n",
    "|OCCUPATION_TYPE| \tOccupation \t||\n",
    "|CNT_FAM_MEMBERS| \tFamily size||\n",
    "\n",
    "Now that the purpose of the columns is clear, the actual data can be analyzed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "104517cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>NAME_INCOME_TYPE</th>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "      <th>NAME_FAMILY_STATUS</th>\n",
       "      <th>NAME_HOUSING_TYPE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>FLAG_MOBIL</th>\n",
       "      <th>FLAG_WORK_PHONE</th>\n",
       "      <th>FLAG_PHONE</th>\n",
       "      <th>FLAG_EMAIL</th>\n",
       "      <th>OCCUPATION_TYPE</th>\n",
       "      <th>CNT_FAM_MEMBERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5008804</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>427500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>Rented apartment</td>\n",
       "      <td>-12005</td>\n",
       "      <td>-4542</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5008805</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>427500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>Rented apartment</td>\n",
       "      <td>-12005</td>\n",
       "      <td>-4542</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5008806</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-21474</td>\n",
       "      <td>-1134</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Security staff</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5008808</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-19110</td>\n",
       "      <td>-3051</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sales staff</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5008809</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-19110</td>\n",
       "      <td>-3051</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sales staff</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY  CNT_CHILDREN  \\\n",
       "0  5008804           M            Y               Y             0   \n",
       "1  5008805           M            Y               Y             0   \n",
       "2  5008806           M            Y               Y             0   \n",
       "3  5008808           F            N               Y             0   \n",
       "4  5008809           F            N               Y             0   \n",
       "\n",
       "   AMT_INCOME_TOTAL      NAME_INCOME_TYPE            NAME_EDUCATION_TYPE  \\\n",
       "0          427500.0               Working               Higher education   \n",
       "1          427500.0               Working               Higher education   \n",
       "2          112500.0               Working  Secondary / secondary special   \n",
       "3          270000.0  Commercial associate  Secondary / secondary special   \n",
       "4          270000.0  Commercial associate  Secondary / secondary special   \n",
       "\n",
       "     NAME_FAMILY_STATUS  NAME_HOUSING_TYPE  DAYS_BIRTH  DAYS_EMPLOYED  \\\n",
       "0        Civil marriage   Rented apartment      -12005          -4542   \n",
       "1        Civil marriage   Rented apartment      -12005          -4542   \n",
       "2               Married  House / apartment      -21474          -1134   \n",
       "3  Single / not married  House / apartment      -19110          -3051   \n",
       "4  Single / not married  House / apartment      -19110          -3051   \n",
       "\n",
       "   FLAG_MOBIL  FLAG_WORK_PHONE  FLAG_PHONE  FLAG_EMAIL OCCUPATION_TYPE  \\\n",
       "0           1                1           0           0             NaN   \n",
       "1           1                1           0           0             NaN   \n",
       "2           1                0           0           0  Security staff   \n",
       "3           1                0           1           1     Sales staff   \n",
       "4           1                0           1           1     Sales staff   \n",
       "\n",
       "   CNT_FAM_MEMBERS  \n",
       "0              2.0  \n",
       "1              2.0  \n",
       "2              2.0  \n",
       "3              1.0  \n",
       "4              1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef6505",
   "metadata": {},
   "source": [
    "From these first five entries several things can be observed:\n",
    "1. The ID does not start from 0 but seems to be unique.\n",
    "2. For the `CODE_GENDER` the flags `F` (Female) and `M` (Male) are used.\n",
    "3. For `FLAG_OWN_CAR` and `FLAG_OWN_REALTY` the flags `Y` (Yes) and `N` (No) are used.\n",
    "4. For `NAME_INCOME_TYPE`, `NAME_EDUCATION_TYPE`, `NAME_FAMILY_STATUS` and `NAME_HOUSING_TYPE` are textual fields, but seem to have only a few different values.\n",
    "5. `OCCUPATION_TYPE` is a textual field and the values seem to be very different (\"Freetext Field\").\n",
    "6. For `FLAG_MOBIL`, `FLAG_WORK_PHONE`, `FLAG_PHONE` and `FLAG_EMAIL` the flags 1 (Yes) and 0 (No) are used.\n",
    "\n",
    "Before basing the data perparation on these findings, the assumptions have to be validated:\n",
    "\n",
    "#### 1. Unique `ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21461759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "7137299    2\n",
       "7702238    2\n",
       "7282535    2\n",
       "7243768    2\n",
       "7050948    2\n",
       "          ..\n",
       "5690727    1\n",
       "6621262    1\n",
       "6621261    1\n",
       "6621260    1\n",
       "6842885    1\n",
       "Name: count, Length: 438510, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df['ID'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7df86a",
   "metadata": {},
   "source": [
    "The ouptut of the value count clearly shows that not every ID is unique. As some IDs are contained twice, the real amout of customers contained in the dataset is only 438510.\n",
    "#### 2. Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97081a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CODE_GENDER\n",
       "F    294440\n",
       "M    144117\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df['CODE_GENDER'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f68775",
   "metadata": {},
   "source": [
    "For the gender the assumption that only the flags `F` and `M` are used was correct.\n",
    "#### 3. Flag Car / Real-Estate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea31e73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAG_OWN_CAR\n",
      "N    275459\n",
      "Y    163098\n",
      "Name: count, dtype: int64\n",
      "FLAG_OWN_REALTY\n",
      "Y    304074\n",
      "N    134483\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(customer_df['FLAG_OWN_CAR'].value_counts())\n",
    "print(customer_df['FLAG_OWN_REALTY'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959af81",
   "metadata": {},
   "source": [
    "Also for the flags for the possesion of car and real-estate the assumption that there are only the flags `Y` and `N` was correct.\n",
    "#### 4. Text fields income, education, family and housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d09f567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAME_INCOME_TYPE\n",
       "Working                 226104\n",
       "Commercial associate    100757\n",
       "Pensioner                75493\n",
       "State servant            36186\n",
       "Student                     17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df['NAME_INCOME_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a80281",
   "metadata": {},
   "source": [
    "As can be seen above, there are five different types of income. This means the column can be encoded without any problems and can be used for the modeling later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e98f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAME_EDUCATION_TYPE\n",
       "Secondary / secondary special    301821\n",
       "Higher education                 117522\n",
       "Incomplete higher                 14851\n",
       "Lower secondary                    4051\n",
       "Academic degree                     312\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df['NAME_EDUCATION_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773a454",
   "metadata": {},
   "source": [
    "The education field also consists of only five types and can therefore also be used for modelling without any problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac363c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAME_FAMILY_STATUS\n",
       "Married                 299828\n",
       "Single / not married     55271\n",
       "Civil marriage           36532\n",
       "Separated                27251\n",
       "Widow                    19675\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df['NAME_FAMILY_STATUS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e7be1",
   "metadata": {},
   "source": [
    "Also the family status has five different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75506a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAME_HOUSING_TYPE\n",
       "House / apartment      393831\n",
       "With parents            19077\n",
       "Municipal apartment     14214\n",
       "Rented apartment         5974\n",
       "Office apartment         3922\n",
       "Co-op apartment          1539\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df['NAME_HOUSING_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fc144",
   "metadata": {},
   "source": [
    "The housing has six different values, which is still acceptable.\n",
    "#### 5. Text field occupation type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8524c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCCUPATION_TYPE\n",
      "Laborers                 78240\n",
      "Core staff               43007\n",
      "Sales staff              41098\n",
      "Managers                 35487\n",
      "Drivers                  26090\n",
      "High skill tech staff    17289\n",
      "Accountants              15985\n",
      "Medicine staff           13520\n",
      "Cooking staff             8076\n",
      "Security staff            7993\n",
      "Cleaning staff            5845\n",
      "Private service staff     3456\n",
      "Low-skill Laborers        2140\n",
      "Secretaries               2044\n",
      "Waiters/barmen staff      1665\n",
      "Realty agents             1041\n",
      "HR staff                   774\n",
      "IT staff                   604\n",
      "Name: count, dtype: int64\n",
      "Amount of null values:  134203\n"
     ]
    }
   ],
   "source": [
    "print(customer_df['OCCUPATION_TYPE'].value_counts())\n",
    "print(\"Amount of null values: \", customer_df['OCCUPATION_TYPE'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c95a81d",
   "metadata": {},
   "source": [
    "Even though there are way less different values than expected (only 18), the column contains many null values, which may make it difficult to work with it.\n",
    "#### 6. Contact method flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc0977c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAG_MOBIL\n",
      "1    438557\n",
      "Name: count, dtype: int64\n",
      "FLAG_WORK_PHONE\n",
      "0    348156\n",
      "1     90401\n",
      "Name: count, dtype: int64\n",
      "FLAG_PHONE\n",
      "0    312353\n",
      "1    126204\n",
      "Name: count, dtype: int64\n",
      "FLAG_EMAIL\n",
      "0    391102\n",
      "1     47455\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(customer_df['FLAG_MOBIL'].value_counts())\n",
    "print(customer_df['FLAG_WORK_PHONE'].value_counts())\n",
    "print(customer_df['FLAG_PHONE'].value_counts())\n",
    "print(customer_df['FLAG_EMAIL'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476de097",
   "metadata": {},
   "source": [
    "As expected, these columns only use the flags `1` and `0`. On top of that, column `FLAG_MOBIL` only contains the value `1`, which means all customers at least are registered with a mobile phone. Therefore, this column can be left out completely.\n",
    "### credit_record.csv\n",
    "Again, first analyze the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "476f8561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   ID              1048575 non-null  int64 \n",
      " 1   MONTHS_BALANCE  1048575 non-null  int64 \n",
      " 2   STATUS          1048575 non-null  object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 24.0+ MB\n"
     ]
    }
   ],
   "source": [
    "credit_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11dd607",
   "metadata": {},
   "source": [
    "As can be seen above, this table only contains three columns. According to the [kaggle page](https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction/data), these columns mean the following:\n",
    "\n",
    "|Feature name| \tExplanation |\tRemarks|\n",
    "|:-|:-|:-|\n",
    "|ID |\tClient number \t||\n",
    "|MONTHS_BALANCE |\tRecord month |\tThe month of the extracted data is the starting point, backwards, 0 is the current month, -1 is the previous month, and so on|\n",
    "|STATUS |\tStatus| \t0: 1-29 days past due 1: 30-59 days past due 2: 60-89 days overdue 3: 90-119 days overdue 4: 120-149 days overdue 5: Overdue or bad debts, write-offs for more than 150 days C: paid off that month X: No loan for the month|\n",
    "\n",
    "Checking for unique `ID` values now reveals for how many customers there exists credit data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04598c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45985"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(credit_df['ID'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6412dea0",
   "metadata": {},
   "source": [
    "There is only credit data for 45985 customers, that means only parts of the `customer_df` can be used.\n",
    "As a last step check the values of the `STATUS` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d0e561b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STATUS\n",
       "C    442031\n",
       "0    383120\n",
       "X    209230\n",
       "1     11090\n",
       "5      1693\n",
       "2       868\n",
       "3       320\n",
       "4       223\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_df['STATUS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef28f9",
   "metadata": {},
   "source": [
    "The information given on the kaggle page is correct, only the stated flags are used.\n",
    "### Summary\n",
    "Overall, the dataset consists of two parts: the customer data and the credit data. The customer data mostly contains information about income, job, family situation and contact methods as these are important aspects for evaluating the creditworthiness. The credit data is basically a credit history overview, showing for a given custumer and month if the credit was paid back on time. This credit data can now be used for calculating an \"overdue_likelyhood\" for every customer which states how likely it is for this specific customer to not pay it's credit back in time. This is an important information for a credit institute. Based on all the findings in this section the two datasets can now be prepared, connected and finally used for training a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61340257",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e164e",
   "metadata": {},
   "source": [
    "Get the overdue likelyhood for the customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d62cc308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS       ID  overdue_likelihood\n",
      "0       5001711            1.000000\n",
      "1       5001712            0.526316\n",
      "2       5001713                 NaN\n",
      "3       5001714                 NaN\n",
      "4       5001715                 NaN\n",
      "The number of entries with NaN values in overdue_likelihood: 4536\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "grouped_df = credit_df.groupby('ID')['STATUS'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "grouped_df['overdue_likelihood'] = 1 - (grouped_df['C'] / (grouped_df.sum(axis=1) - grouped_df['X']))\n",
    "\n",
    "result_df = grouped_df.reset_index()[['ID', 'overdue_likelihood']]\n",
    "\n",
    "print(result_df.head())\n",
    "\n",
    "nan_count = result_df['overdue_likelihood'].isna().sum()\n",
    "\n",
    "print(f'The number of entries with NaN values in overdue_likelihood: {nan_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff13a5",
   "metadata": {},
   "source": [
    "That means ca. 33110 customers are really usable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2aa80c",
   "metadata": {},
   "source": [
    "Remove NaN entrys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2461462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of entries with NaN values in overdue_likelihood: 0\n"
     ]
    }
   ],
   "source": [
    "result_df.dropna(subset=['overdue_likelihood'], inplace=True)\n",
    "nan_count = result_df['overdue_likelihood'].isna().sum()\n",
    "\n",
    "print(f'The number of entries with NaN values in overdue_likelihood: {nan_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c5c50",
   "metadata": {},
   "source": [
    "Merge the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79178d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY  CNT_CHILDREN  \\\n",
      "0  5008804           M            Y               Y             0   \n",
      "1  5008805           M            Y               Y             0   \n",
      "2  5008806           M            Y               Y             0   \n",
      "3  5008808           F            N               Y             0   \n",
      "4  5008810           F            N               Y             0   \n",
      "\n",
      "   AMT_INCOME_TOTAL      NAME_INCOME_TYPE            NAME_EDUCATION_TYPE  \\\n",
      "0          427500.0               Working               Higher education   \n",
      "1          427500.0               Working               Higher education   \n",
      "2          112500.0               Working  Secondary / secondary special   \n",
      "3          270000.0  Commercial associate  Secondary / secondary special   \n",
      "4          270000.0  Commercial associate  Secondary / secondary special   \n",
      "\n",
      "     NAME_FAMILY_STATUS  NAME_HOUSING_TYPE  DAYS_BIRTH  DAYS_EMPLOYED  \\\n",
      "0        Civil marriage   Rented apartment      -12005          -4542   \n",
      "1        Civil marriage   Rented apartment      -12005          -4542   \n",
      "2               Married  House / apartment      -21474          -1134   \n",
      "3  Single / not married  House / apartment      -19110          -3051   \n",
      "4  Single / not married  House / apartment      -19110          -3051   \n",
      "\n",
      "   FLAG_MOBIL  FLAG_WORK_PHONE  FLAG_PHONE  FLAG_EMAIL OCCUPATION_TYPE  \\\n",
      "0           1                1           0           0             NaN   \n",
      "1           1                1           0           0             NaN   \n",
      "2           1                0           0           0  Security staff   \n",
      "3           1                0           1           1     Sales staff   \n",
      "4           1                0           1           1     Sales staff   \n",
      "\n",
      "   CNT_FAM_MEMBERS  overdue_likelihood  \n",
      "0              2.0            0.133333  \n",
      "1              2.0            0.142857  \n",
      "2              2.0            0.500000  \n",
      "3              1.0            1.000000  \n",
      "4              1.0            0.285714  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged_df = pd.merge(customer_df, result_df, on='ID', how='inner')\n",
    "print(merged_df.head())\n",
    "#print(len(merged_df['ID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9c2bf99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "#labels = ['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%']\n",
    "\n",
    "#bins = [0, 0.2, 0.4, 0.6,  0.8, 1]\n",
    "#labels = ['1', '2', '3', '4', '5']\n",
    "\n",
    "# Define bins with 5% steps\n",
    "#bins = [i/20 for i in range(21)]  # 0, 0.05, 0.1, ..., 0.95, 1\n",
    "\n",
    "# Define corresponding labels for each bin\n",
    "#labels = ['0-5%', '5-10%', '10-15%', '15-20%', '20-25%', '25-30%', '30-35%', '35-40%', \n",
    " #         '40-45%', '45-50%', '50-55%', '55-60%', '60-65%', '65-70%', '70-75%', '75-80%',\n",
    "  #        '80-85%', '85-90%', '90-95%', '95-100%']\n",
    "\n",
    "#merged_df['overdue_class'] = pd.cut(merged_df['overdue_likelihood'], bins=bins, labels=labels, include_lowest=True)\n",
    "#merged_df['overdue_class'].value_counts()\n",
    "\n",
    "\n",
    "#Manual resampling\n",
    "\n",
    "# Find the number of samples in the smallest class\n",
    "#min_class_size = merged_df['overdue_class'].value_counts().min()\n",
    "#print(f\"Minimum class size: {min_class_size}\")\n",
    "\n",
    "# Resample each class to have the same number of samples as the smallest class\n",
    "#resampled_df = merged_df.groupby('overdue_class').apply(lambda x: x.sample(min_class_size, replace=True)).reset_index(drop=True)\n",
    "#merged_df = resampled_df\n",
    "# Check the class distribution after resampling\n",
    "#print(resampled_df['overdue_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "606f7d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  \\\n",
      "0          1.0           1.0              1.0             0          427500.0   \n",
      "1          1.0           1.0              1.0             0          427500.0   \n",
      "2          1.0           1.0              1.0             0          112500.0   \n",
      "3          0.0           0.0              1.0             0          270000.0   \n",
      "4          0.0           0.0              1.0             0          270000.0   \n",
      "\n",
      "   NAME_EDUCATION_TYPE  DAYS_BIRTH  DAYS_EMPLOYED  FLAG_MOBIL  \\\n",
      "0                  1.0      -12005          -4542           1   \n",
      "1                  1.0      -12005          -4542           1   \n",
      "2                  4.0      -21474          -1134           1   \n",
      "3                  4.0      -19110          -3051           1   \n",
      "4                  4.0      -19110          -3051           1   \n",
      "\n",
      "   FLAG_WORK_PHONE  FLAG_PHONE  FLAG_EMAIL  CNT_FAM_MEMBERS  \\\n",
      "0                1           0           0              2.0   \n",
      "1                1           0           0              2.0   \n",
      "2                0           0           0              2.0   \n",
      "3                0           1           1              1.0   \n",
      "4                0           1           1              1.0   \n",
      "\n",
      "   overdue_likelihood  \n",
      "0            0.133333  \n",
      "1            0.142857  \n",
      "2            0.500000  \n",
      "3            1.000000  \n",
      "4            0.285714  \n"
     ]
    }
   ],
   "source": [
    "one_hot_cols = []\n",
    "ordinal_cols = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_EDUCATION_TYPE']\n",
    "\n",
    "\n",
    "merged_df = merged_df.drop('OCCUPATION_TYPE', axis=1)\n",
    "merged_df = merged_df.drop('NAME_HOUSING_TYPE', axis=1)\n",
    "merged_df = merged_df.drop('NAME_INCOME_TYPE', axis=1)\n",
    "merged_df = merged_df.drop('NAME_FAMILY_STATUS', axis=1)\n",
    "#merged_df = merged_df.drop('overdue_likelihood', axis=1)\n",
    "merged_df = merged_df.drop('ID', axis=1)\n",
    "\n",
    "df_ord = merged_df.copy()\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "df_ord[ordinal_cols] = ordinal_encoder.fit_transform(df_ord[ordinal_cols])\n",
    "\n",
    "df_enc = pd.get_dummies(df_ord, columns=one_hot_cols)\n",
    "\n",
    "print(df_enc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8bbbeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  \\\n",
      "0          1.0           1.0              1.0           0.0          0.258721   \n",
      "1          1.0           1.0              1.0           0.0          0.258721   \n",
      "2          1.0           1.0              1.0           0.0          0.055233   \n",
      "3          0.0           0.0              1.0           0.0          0.156977   \n",
      "4          0.0           0.0              1.0           0.0          0.156977   \n",
      "\n",
      "   DAYS_BIRTH  DAYS_EMPLOYED  CNT_FAM_MEMBERS  \n",
      "0    0.753539       0.029324         0.052632  \n",
      "1    0.753539       0.029324         0.052632  \n",
      "2    0.210810       0.038270         0.052632  \n",
      "3    0.346306       0.033237         0.000000  \n",
      "4    0.346306       0.033237         0.000000  \n"
     ]
    }
   ],
   "source": [
    "#X_enc = df_enc.drop('overdue_class', axis=1)\n",
    "X_enc = df_enc.drop('overdue_likelihood', axis=1)\n",
    "X_enc = X_enc.drop('FLAG_WORK_PHONE', axis=1)\n",
    "X_enc = X_enc.drop('FLAG_PHONE', axis=1)\n",
    "X_enc = X_enc.drop('FLAG_EMAIL', axis=1)\n",
    "X_enc = X_enc.drop('FLAG_MOBIL', axis=1)\n",
    "X_enc = X_enc.drop('NAME_EDUCATION_TYPE', axis=1)\n",
    "Y = df_enc['overdue_likelihood']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_enc)\n",
    "df_scaled = pd.DataFrame(X_scaled, columns= X_enc.columns)\n",
    "print(df_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae813e4",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- overdue-likelyhood zu labels\n",
    "- encoden\n",
    "    - Eig geht fast alles ordinal encoded\n",
    "    - OCCUPATION_TYPE fliegt ganz raus\n",
    "- scalen\n",
    "- random forest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51efbdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8de87fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/3000], Loss: 0.5825\n",
      "Epoch [2/3000], Loss: 2.4994\n",
      "Epoch [3/3000], Loss: 0.3509\n",
      "Epoch [4/3000], Loss: 0.4840\n",
      "Epoch [5/3000], Loss: 0.5182\n",
      "Epoch [6/3000], Loss: 0.5143\n",
      "Epoch [7/3000], Loss: 0.4925\n",
      "Epoch [8/3000], Loss: 0.4572\n",
      "Epoch [9/3000], Loss: 0.4134\n",
      "Epoch [10/3000], Loss: 0.3715\n",
      "Epoch [11/3000], Loss: 0.3463\n",
      "Epoch [12/3000], Loss: 0.3603\n",
      "Epoch [13/3000], Loss: 0.3862\n",
      "Epoch [14/3000], Loss: 0.3477\n",
      "Epoch [15/3000], Loss: 0.3426\n",
      "Epoch [16/3000], Loss: 0.3436\n",
      "Epoch [17/3000], Loss: 0.3473\n",
      "Epoch [18/3000], Loss: 0.3514\n",
      "Epoch [19/3000], Loss: 0.3540\n",
      "Epoch [20/3000], Loss: 0.3545\n",
      "Epoch [21/3000], Loss: 0.3529\n",
      "Epoch [22/3000], Loss: 0.3498\n",
      "Epoch [23/3000], Loss: 0.3462\n",
      "Epoch [24/3000], Loss: 0.3433\n",
      "Epoch [25/3000], Loss: 0.3418\n",
      "Epoch [26/3000], Loss: 0.3416\n",
      "Epoch [27/3000], Loss: 0.3428\n",
      "Epoch [28/3000], Loss: 0.3444\n",
      "Epoch [29/3000], Loss: 0.3456\n",
      "Epoch [30/3000], Loss: 0.3458\n",
      "Epoch [31/3000], Loss: 0.3450\n",
      "Epoch [32/3000], Loss: 0.3438\n",
      "Epoch [33/3000], Loss: 0.3425\n",
      "Epoch [34/3000], Loss: 0.3417\n",
      "Epoch [35/3000], Loss: 0.3415\n",
      "Epoch [36/3000], Loss: 0.3417\n",
      "Epoch [37/3000], Loss: 0.3421\n",
      "Epoch [38/3000], Loss: 0.3426\n",
      "Epoch [39/3000], Loss: 0.3428\n",
      "Epoch [40/3000], Loss: 0.3428\n",
      "Epoch [41/3000], Loss: 0.3425\n",
      "Epoch [42/3000], Loss: 0.3421\n",
      "Epoch [43/3000], Loss: 0.3417\n",
      "Epoch [44/3000], Loss: 0.3414\n",
      "Epoch [45/3000], Loss: 0.3413\n",
      "Epoch [46/3000], Loss: 0.3414\n",
      "Epoch [47/3000], Loss: 0.3415\n",
      "Epoch [48/3000], Loss: 0.3417\n",
      "Epoch [49/3000], Loss: 0.3417\n",
      "Epoch [50/3000], Loss: 0.3417\n",
      "Epoch [51/3000], Loss: 0.3415\n",
      "Epoch [52/3000], Loss: 0.3413\n",
      "Epoch [53/3000], Loss: 0.3412\n",
      "Epoch [54/3000], Loss: 0.3411\n",
      "Epoch [55/3000], Loss: 0.3411\n",
      "Epoch [56/3000], Loss: 0.3411\n",
      "Epoch [57/3000], Loss: 0.3412\n",
      "Epoch [58/3000], Loss: 0.3412\n",
      "Epoch [59/3000], Loss: 0.3412\n",
      "Epoch [60/3000], Loss: 0.3411\n",
      "Epoch [61/3000], Loss: 0.3410\n",
      "Epoch [62/3000], Loss: 0.3409\n",
      "Epoch [63/3000], Loss: 0.3408\n",
      "Epoch [64/3000], Loss: 0.3408\n",
      "Epoch [65/3000], Loss: 0.3409\n",
      "Epoch [66/3000], Loss: 0.3409\n",
      "Epoch [67/3000], Loss: 0.3408\n",
      "Epoch [68/3000], Loss: 0.3408\n",
      "Epoch [69/3000], Loss: 0.3407\n",
      "Epoch [70/3000], Loss: 0.3406\n",
      "Epoch [71/3000], Loss: 0.3406\n",
      "Epoch [72/3000], Loss: 0.3406\n",
      "Epoch [73/3000], Loss: 0.3406\n",
      "Epoch [74/3000], Loss: 0.3405\n",
      "Epoch [75/3000], Loss: 0.3405\n",
      "Epoch [76/3000], Loss: 0.3404\n",
      "Epoch [77/3000], Loss: 0.3403\n",
      "Epoch [78/3000], Loss: 0.3403\n",
      "Epoch [79/3000], Loss: 0.3402\n",
      "Epoch [80/3000], Loss: 0.3401\n",
      "Epoch [81/3000], Loss: 0.3401\n",
      "Epoch [82/3000], Loss: 0.3400\n",
      "Epoch [83/3000], Loss: 0.3399\n",
      "Epoch [84/3000], Loss: 0.3399\n",
      "Epoch [85/3000], Loss: 0.3398\n",
      "Epoch [86/3000], Loss: 0.3399\n",
      "Epoch [87/3000], Loss: 0.3397\n",
      "Epoch [88/3000], Loss: 0.3397\n",
      "Epoch [89/3000], Loss: 0.3396\n",
      "Epoch [90/3000], Loss: 0.3395\n",
      "Epoch [91/3000], Loss: 0.3395\n",
      "Epoch [92/3000], Loss: 0.3394\n",
      "Epoch [93/3000], Loss: 0.3394\n",
      "Epoch [94/3000], Loss: 0.3393\n",
      "Epoch [95/3000], Loss: 0.3393\n",
      "Epoch [96/3000], Loss: 0.3392\n",
      "Epoch [97/3000], Loss: 0.3392\n",
      "Epoch [98/3000], Loss: 0.3391\n",
      "Epoch [99/3000], Loss: 0.3391\n",
      "Epoch [100/3000], Loss: 0.3390\n",
      "Epoch [101/3000], Loss: 0.3390\n",
      "Epoch [102/3000], Loss: 0.3389\n",
      "Epoch [103/3000], Loss: 0.3389\n",
      "Epoch [104/3000], Loss: 0.3388\n",
      "Epoch [105/3000], Loss: 0.3388\n",
      "Epoch [106/3000], Loss: 0.3388\n",
      "Epoch [107/3000], Loss: 0.3387\n",
      "Epoch [108/3000], Loss: 0.3386\n",
      "Epoch [109/3000], Loss: 0.3386\n",
      "Epoch [110/3000], Loss: 0.3385\n",
      "Epoch [111/3000], Loss: 0.3385\n",
      "Epoch [112/3000], Loss: 0.3384\n",
      "Epoch [113/3000], Loss: 0.3384\n",
      "Epoch [114/3000], Loss: 0.3384\n",
      "Epoch [115/3000], Loss: 0.3383\n",
      "Epoch [116/3000], Loss: 0.3383\n",
      "Epoch [117/3000], Loss: 0.3382\n",
      "Epoch [118/3000], Loss: 0.3382\n",
      "Epoch [119/3000], Loss: 0.3381\n",
      "Epoch [120/3000], Loss: 0.3380\n",
      "Epoch [121/3000], Loss: 0.3380\n",
      "Epoch [122/3000], Loss: 0.3380\n",
      "Epoch [123/3000], Loss: 0.3380\n",
      "Epoch [124/3000], Loss: 0.3380\n",
      "Epoch [125/3000], Loss: 0.3379\n",
      "Epoch [126/3000], Loss: 0.3378\n",
      "Epoch [127/3000], Loss: 0.3379\n",
      "Epoch [128/3000], Loss: 0.3383\n",
      "Epoch [129/3000], Loss: 0.3382\n",
      "Epoch [130/3000], Loss: 0.3378\n",
      "Epoch [131/3000], Loss: 0.3381\n",
      "Epoch [132/3000], Loss: 0.3376\n",
      "Epoch [133/3000], Loss: 0.3378\n",
      "Epoch [134/3000], Loss: 0.3375\n",
      "Epoch [135/3000], Loss: 0.3377\n",
      "Epoch [136/3000], Loss: 0.3374\n",
      "Epoch [137/3000], Loss: 0.3375\n",
      "Epoch [138/3000], Loss: 0.3373\n",
      "Epoch [139/3000], Loss: 0.3374\n",
      "Epoch [140/3000], Loss: 0.3372\n",
      "Epoch [141/3000], Loss: 0.3373\n",
      "Epoch [142/3000], Loss: 0.3371\n",
      "Epoch [143/3000], Loss: 0.3371\n",
      "Epoch [144/3000], Loss: 0.3371\n",
      "Epoch [145/3000], Loss: 0.3370\n",
      "Epoch [146/3000], Loss: 0.3370\n",
      "Epoch [147/3000], Loss: 0.3369\n",
      "Epoch [148/3000], Loss: 0.3368\n",
      "Epoch [149/3000], Loss: 0.3368\n",
      "Epoch [150/3000], Loss: 0.3368\n",
      "Epoch [151/3000], Loss: 0.3367\n",
      "Epoch [152/3000], Loss: 0.3367\n",
      "Epoch [153/3000], Loss: 0.3365\n",
      "Epoch [154/3000], Loss: 0.3365\n",
      "Epoch [155/3000], Loss: 0.3365\n",
      "Epoch [156/3000], Loss: 0.3365\n",
      "Epoch [157/3000], Loss: 0.3365\n",
      "Epoch [158/3000], Loss: 0.3364\n",
      "Epoch [159/3000], Loss: 0.3364\n",
      "Epoch [160/3000], Loss: 0.3363\n",
      "Epoch [161/3000], Loss: 0.3361\n",
      "Epoch [162/3000], Loss: 0.3362\n",
      "Epoch [163/3000], Loss: 0.3362\n",
      "Epoch [164/3000], Loss: 0.3364\n",
      "Epoch [165/3000], Loss: 0.3362\n",
      "Epoch [166/3000], Loss: 0.3360\n",
      "Epoch [167/3000], Loss: 0.3358\n",
      "Epoch [168/3000], Loss: 0.3360\n",
      "Epoch [169/3000], Loss: 0.3364\n",
      "Epoch [170/3000], Loss: 0.3359\n",
      "Epoch [171/3000], Loss: 0.3356\n",
      "Epoch [172/3000], Loss: 0.3356\n",
      "Epoch [173/3000], Loss: 0.3357\n",
      "Epoch [174/3000], Loss: 0.3359\n",
      "Epoch [175/3000], Loss: 0.3357\n",
      "Epoch [176/3000], Loss: 0.3358\n",
      "Epoch [177/3000], Loss: 0.3354\n",
      "Epoch [178/3000], Loss: 0.3353\n",
      "Epoch [179/3000], Loss: 0.3353\n",
      "Epoch [180/3000], Loss: 0.3355\n",
      "Epoch [181/3000], Loss: 0.3359\n",
      "Epoch [182/3000], Loss: 0.3354\n",
      "Epoch [183/3000], Loss: 0.3351\n",
      "Epoch [184/3000], Loss: 0.3350\n",
      "Epoch [185/3000], Loss: 0.3352\n",
      "Epoch [186/3000], Loss: 0.3357\n",
      "Epoch [187/3000], Loss: 0.3355\n",
      "Epoch [188/3000], Loss: 0.3358\n",
      "Epoch [189/3000], Loss: 0.3348\n",
      "Epoch [190/3000], Loss: 0.3362\n",
      "Epoch [191/3000], Loss: 0.3391\n",
      "Epoch [192/3000], Loss: 0.3369\n",
      "Epoch [193/3000], Loss: 0.3400\n",
      "Epoch [194/3000], Loss: 0.3364\n",
      "Epoch [195/3000], Loss: 0.3392\n",
      "Epoch [196/3000], Loss: 0.3358\n",
      "Epoch [197/3000], Loss: 0.3382\n",
      "Epoch [198/3000], Loss: 0.3355\n",
      "Epoch [199/3000], Loss: 0.3362\n",
      "Epoch [200/3000], Loss: 0.3365\n",
      "Epoch [201/3000], Loss: 0.3353\n",
      "Epoch [202/3000], Loss: 0.3365\n",
      "Epoch [203/3000], Loss: 0.3351\n",
      "Epoch [204/3000], Loss: 0.3356\n",
      "Epoch [205/3000], Loss: 0.3354\n",
      "Epoch [206/3000], Loss: 0.3351\n",
      "Epoch [207/3000], Loss: 0.3353\n",
      "Epoch [208/3000], Loss: 0.3348\n",
      "Epoch [209/3000], Loss: 0.3353\n",
      "Epoch [210/3000], Loss: 0.3349\n",
      "Epoch [211/3000], Loss: 0.3351\n",
      "Epoch [212/3000], Loss: 0.3346\n",
      "Epoch [213/3000], Loss: 0.3348\n",
      "Epoch [214/3000], Loss: 0.3346\n",
      "Epoch [215/3000], Loss: 0.3347\n",
      "Epoch [216/3000], Loss: 0.3344\n",
      "Epoch [217/3000], Loss: 0.3344\n",
      "Epoch [218/3000], Loss: 0.3344\n",
      "Epoch [219/3000], Loss: 0.3342\n",
      "Epoch [220/3000], Loss: 0.3341\n",
      "Epoch [221/3000], Loss: 0.3340\n",
      "Epoch [222/3000], Loss: 0.3341\n",
      "Epoch [223/3000], Loss: 0.3339\n",
      "Epoch [224/3000], Loss: 0.3338\n",
      "Epoch [225/3000], Loss: 0.3338\n",
      "Epoch [226/3000], Loss: 0.3337\n",
      "Epoch [227/3000], Loss: 0.3336\n",
      "Epoch [228/3000], Loss: 0.3336\n",
      "Epoch [229/3000], Loss: 0.3335\n",
      "Epoch [230/3000], Loss: 0.3336\n",
      "Epoch [231/3000], Loss: 0.3337\n",
      "Epoch [232/3000], Loss: 0.3337\n",
      "Epoch [233/3000], Loss: 0.3335\n",
      "Epoch [234/3000], Loss: 0.3333\n",
      "Epoch [235/3000], Loss: 0.3332\n",
      "Epoch [236/3000], Loss: 0.3331\n",
      "Epoch [237/3000], Loss: 0.3335\n",
      "Epoch [238/3000], Loss: 0.3350\n",
      "Epoch [239/3000], Loss: 0.3332\n",
      "Epoch [240/3000], Loss: 0.3336\n",
      "Epoch [241/3000], Loss: 0.3357\n",
      "Epoch [242/3000], Loss: 0.3332\n",
      "Epoch [243/3000], Loss: 0.3388\n",
      "Epoch [244/3000], Loss: 0.3432\n",
      "Epoch [245/3000], Loss: 0.3464\n",
      "Epoch [246/3000], Loss: 0.3364\n",
      "Epoch [247/3000], Loss: 0.3512\n",
      "Epoch [248/3000], Loss: 0.3448\n",
      "Epoch [249/3000], Loss: 0.3546\n",
      "Epoch [250/3000], Loss: 0.3534\n",
      "Epoch [251/3000], Loss: 0.3474\n",
      "Epoch [252/3000], Loss: 0.3419\n",
      "Epoch [253/3000], Loss: 0.3406\n",
      "Epoch [254/3000], Loss: 0.3431\n",
      "Epoch [255/3000], Loss: 0.3481\n",
      "Epoch [256/3000], Loss: 0.3432\n",
      "Epoch [257/3000], Loss: 0.3407\n",
      "Epoch [258/3000], Loss: 0.3403\n",
      "Epoch [259/3000], Loss: 0.3416\n",
      "Epoch [260/3000], Loss: 0.3430\n",
      "Epoch [261/3000], Loss: 0.3433\n",
      "Epoch [262/3000], Loss: 0.3424\n",
      "Epoch [263/3000], Loss: 0.3410\n",
      "Epoch [264/3000], Loss: 0.3401\n",
      "Epoch [265/3000], Loss: 0.3402\n",
      "Epoch [266/3000], Loss: 0.3409\n",
      "Epoch [267/3000], Loss: 0.3416\n",
      "Epoch [268/3000], Loss: 0.3414\n",
      "Epoch [269/3000], Loss: 0.3406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [270/3000], Loss: 0.3399\n",
      "Epoch [271/3000], Loss: 0.3398\n",
      "Epoch [272/3000], Loss: 0.3401\n",
      "Epoch [273/3000], Loss: 0.3404\n",
      "Epoch [274/3000], Loss: 0.3404\n",
      "Epoch [275/3000], Loss: 0.3401\n",
      "Epoch [276/3000], Loss: 0.3396\n",
      "Epoch [277/3000], Loss: 0.3395\n",
      "Epoch [278/3000], Loss: 0.3396\n",
      "Epoch [279/3000], Loss: 0.3398\n",
      "Epoch [280/3000], Loss: 0.3398\n",
      "Epoch [281/3000], Loss: 0.3396\n",
      "Epoch [282/3000], Loss: 0.3393\n",
      "Epoch [283/3000], Loss: 0.3392\n",
      "Epoch [284/3000], Loss: 0.3392\n",
      "Epoch [285/3000], Loss: 0.3393\n",
      "Epoch [286/3000], Loss: 0.3393\n",
      "Epoch [287/3000], Loss: 0.3391\n",
      "Epoch [288/3000], Loss: 0.3390\n",
      "Epoch [289/3000], Loss: 0.3389\n",
      "Epoch [290/3000], Loss: 0.3389\n",
      "Epoch [291/3000], Loss: 0.3389\n",
      "Epoch [292/3000], Loss: 0.3389\n",
      "Epoch [293/3000], Loss: 0.3388\n",
      "Epoch [294/3000], Loss: 0.3386\n",
      "Epoch [295/3000], Loss: 0.3386\n",
      "Epoch [296/3000], Loss: 0.3386\n",
      "Epoch [297/3000], Loss: 0.3385\n",
      "Epoch [298/3000], Loss: 0.3384\n",
      "Epoch [299/3000], Loss: 0.3383\n",
      "Epoch [300/3000], Loss: 0.3383\n",
      "Epoch [301/3000], Loss: 0.3382\n",
      "Epoch [302/3000], Loss: 0.3381\n",
      "Epoch [303/3000], Loss: 0.3380\n",
      "Epoch [304/3000], Loss: 0.3380\n",
      "Epoch [305/3000], Loss: 0.3378\n",
      "Epoch [306/3000], Loss: 0.3377\n",
      "Epoch [307/3000], Loss: 0.3377\n",
      "Epoch [308/3000], Loss: 0.3376\n",
      "Epoch [309/3000], Loss: 0.3376\n",
      "Epoch [310/3000], Loss: 0.3375\n",
      "Epoch [311/3000], Loss: 0.3374\n",
      "Epoch [312/3000], Loss: 0.3373\n",
      "Epoch [313/3000], Loss: 0.3374\n",
      "Epoch [314/3000], Loss: 0.3372\n",
      "Epoch [315/3000], Loss: 0.3371\n",
      "Epoch [316/3000], Loss: 0.3371\n",
      "Epoch [317/3000], Loss: 0.3369\n",
      "Epoch [318/3000], Loss: 0.3369\n",
      "Epoch [319/3000], Loss: 0.3368\n",
      "Epoch [320/3000], Loss: 0.3367\n",
      "Epoch [321/3000], Loss: 0.3366\n",
      "Epoch [322/3000], Loss: 0.3365\n",
      "Epoch [323/3000], Loss: 0.3365\n",
      "Epoch [324/3000], Loss: 0.3363\n",
      "Epoch [325/3000], Loss: 0.3362\n",
      "Epoch [326/3000], Loss: 0.3362\n",
      "Epoch [327/3000], Loss: 0.3361\n",
      "Epoch [328/3000], Loss: 0.3360\n",
      "Epoch [329/3000], Loss: 0.3359\n",
      "Epoch [330/3000], Loss: 0.3358\n",
      "Epoch [331/3000], Loss: 0.3357\n",
      "Epoch [332/3000], Loss: 0.3357\n",
      "Epoch [333/3000], Loss: 0.3356\n",
      "Epoch [334/3000], Loss: 0.3356\n",
      "Epoch [335/3000], Loss: 0.3362\n",
      "Epoch [336/3000], Loss: 0.3368\n",
      "Epoch [337/3000], Loss: 0.3355\n",
      "Epoch [338/3000], Loss: 0.3369\n",
      "Epoch [339/3000], Loss: 0.3363\n",
      "Epoch [340/3000], Loss: 0.3364\n",
      "Epoch [341/3000], Loss: 0.3357\n",
      "Epoch [342/3000], Loss: 0.3358\n",
      "Epoch [343/3000], Loss: 0.3358\n",
      "Epoch [344/3000], Loss: 0.3358\n",
      "Epoch [345/3000], Loss: 0.3353\n",
      "Epoch [346/3000], Loss: 0.3353\n",
      "Epoch [347/3000], Loss: 0.3355\n",
      "Epoch [348/3000], Loss: 0.3348\n",
      "Epoch [349/3000], Loss: 0.3355\n",
      "Epoch [350/3000], Loss: 0.3348\n",
      "Epoch [351/3000], Loss: 0.3348\n",
      "Epoch [352/3000], Loss: 0.3352\n",
      "Epoch [353/3000], Loss: 0.3346\n",
      "Epoch [354/3000], Loss: 0.3348\n",
      "Epoch [355/3000], Loss: 0.3348\n",
      "Epoch [356/3000], Loss: 0.3344\n",
      "Epoch [357/3000], Loss: 0.3346\n",
      "Epoch [358/3000], Loss: 0.3346\n",
      "Epoch [359/3000], Loss: 0.3342\n",
      "Epoch [360/3000], Loss: 0.3343\n",
      "Epoch [361/3000], Loss: 0.3344\n",
      "Epoch [362/3000], Loss: 0.3342\n",
      "Epoch [363/3000], Loss: 0.3340\n",
      "Epoch [364/3000], Loss: 0.3342\n",
      "Epoch [365/3000], Loss: 0.3345\n",
      "Epoch [366/3000], Loss: 0.3342\n",
      "Epoch [367/3000], Loss: 0.3338\n",
      "Epoch [368/3000], Loss: 0.3337\n",
      "Epoch [369/3000], Loss: 0.3337\n",
      "Epoch [370/3000], Loss: 0.3339\n",
      "Epoch [371/3000], Loss: 0.3340\n",
      "Epoch [372/3000], Loss: 0.3344\n",
      "Epoch [373/3000], Loss: 0.3346\n",
      "Epoch [374/3000], Loss: 0.3338\n",
      "Epoch [375/3000], Loss: 0.3334\n",
      "Epoch [376/3000], Loss: 0.3341\n",
      "Epoch [377/3000], Loss: 0.3344\n",
      "Epoch [378/3000], Loss: 0.3337\n",
      "Epoch [379/3000], Loss: 0.3331\n",
      "Epoch [380/3000], Loss: 0.3335\n",
      "Epoch [381/3000], Loss: 0.3350\n",
      "Epoch [382/3000], Loss: 0.3370\n",
      "Epoch [383/3000], Loss: 0.3334\n",
      "Epoch [384/3000], Loss: 0.3361\n",
      "Epoch [385/3000], Loss: 0.3356\n",
      "Epoch [386/3000], Loss: 0.3360\n",
      "Epoch [387/3000], Loss: 0.3340\n",
      "Epoch [388/3000], Loss: 0.3356\n",
      "Epoch [389/3000], Loss: 0.3354\n",
      "Epoch [390/3000], Loss: 0.3360\n",
      "Epoch [391/3000], Loss: 0.3346\n",
      "Epoch [392/3000], Loss: 0.3349\n",
      "Epoch [393/3000], Loss: 0.3340\n",
      "Epoch [394/3000], Loss: 0.3348\n",
      "Epoch [395/3000], Loss: 0.3349\n",
      "Epoch [396/3000], Loss: 0.3329\n",
      "Epoch [397/3000], Loss: 0.3336\n",
      "Epoch [398/3000], Loss: 0.3339\n",
      "Epoch [399/3000], Loss: 0.3329\n",
      "Epoch [400/3000], Loss: 0.3329\n",
      "Epoch [401/3000], Loss: 0.3340\n",
      "Epoch [402/3000], Loss: 0.3345\n",
      "Epoch [403/3000], Loss: 0.3325\n",
      "Epoch [404/3000], Loss: 0.3342\n",
      "Epoch [405/3000], Loss: 0.3339\n",
      "Epoch [406/3000], Loss: 0.3327\n",
      "Epoch [407/3000], Loss: 0.3352\n",
      "Epoch [408/3000], Loss: 0.3349\n",
      "Epoch [409/3000], Loss: 0.3327\n",
      "Epoch [410/3000], Loss: 0.3357\n",
      "Epoch [411/3000], Loss: 0.3331\n",
      "Epoch [412/3000], Loss: 0.3338\n",
      "Epoch [413/3000], Loss: 0.3337\n",
      "Epoch [414/3000], Loss: 0.3328\n",
      "Epoch [415/3000], Loss: 0.3338\n",
      "Epoch [416/3000], Loss: 0.3324\n",
      "Epoch [417/3000], Loss: 0.3337\n",
      "Epoch [418/3000], Loss: 0.3322\n",
      "Epoch [419/3000], Loss: 0.3325\n",
      "Epoch [420/3000], Loss: 0.3331\n",
      "Epoch [421/3000], Loss: 0.3317\n",
      "Epoch [422/3000], Loss: 0.3323\n",
      "Epoch [423/3000], Loss: 0.3325\n",
      "Epoch [424/3000], Loss: 0.3314\n",
      "Epoch [425/3000], Loss: 0.3319\n",
      "Epoch [426/3000], Loss: 0.3324\n",
      "Epoch [427/3000], Loss: 0.3317\n",
      "Epoch [428/3000], Loss: 0.3312\n",
      "Epoch [429/3000], Loss: 0.3313\n",
      "Epoch [430/3000], Loss: 0.3314\n",
      "Epoch [431/3000], Loss: 0.3314\n",
      "Epoch [432/3000], Loss: 0.3311\n",
      "Epoch [433/3000], Loss: 0.3308\n",
      "Epoch [434/3000], Loss: 0.3307\n",
      "Epoch [435/3000], Loss: 0.3307\n",
      "Epoch [436/3000], Loss: 0.3311\n",
      "Epoch [437/3000], Loss: 0.3329\n",
      "Epoch [438/3000], Loss: 0.3328\n",
      "Epoch [439/3000], Loss: 0.3318\n",
      "Epoch [440/3000], Loss: 0.3308\n",
      "Epoch [441/3000], Loss: 0.3328\n",
      "Epoch [442/3000], Loss: 0.3349\n",
      "Epoch [443/3000], Loss: 0.3313\n",
      "Epoch [444/3000], Loss: 0.3311\n",
      "Epoch [445/3000], Loss: 0.3326\n",
      "Epoch [446/3000], Loss: 0.3308\n",
      "Epoch [447/3000], Loss: 0.3319\n",
      "Epoch [448/3000], Loss: 0.3350\n",
      "Epoch [449/3000], Loss: 0.3311\n",
      "Epoch [450/3000], Loss: 0.3343\n",
      "Epoch [451/3000], Loss: 0.3337\n",
      "Epoch [452/3000], Loss: 0.3315\n",
      "Epoch [453/3000], Loss: 0.3364\n",
      "Epoch [454/3000], Loss: 0.3336\n",
      "Epoch [455/3000], Loss: 0.3335\n",
      "Epoch [456/3000], Loss: 0.3332\n",
      "Epoch [457/3000], Loss: 0.3319\n",
      "Epoch [458/3000], Loss: 0.3328\n",
      "Epoch [459/3000], Loss: 0.3317\n",
      "Epoch [460/3000], Loss: 0.3340\n",
      "Epoch [461/3000], Loss: 0.3316\n",
      "Epoch [462/3000], Loss: 0.3315\n",
      "Epoch [463/3000], Loss: 0.3321\n",
      "Epoch [464/3000], Loss: 0.3305\n",
      "Epoch [465/3000], Loss: 0.3315\n",
      "Epoch [466/3000], Loss: 0.3313\n",
      "Epoch [467/3000], Loss: 0.3306\n",
      "Epoch [468/3000], Loss: 0.3307\n",
      "Epoch [469/3000], Loss: 0.3310\n",
      "Epoch [470/3000], Loss: 0.3305\n",
      "Epoch [471/3000], Loss: 0.3304\n",
      "Epoch [472/3000], Loss: 0.3303\n",
      "Epoch [473/3000], Loss: 0.3300\n",
      "Epoch [474/3000], Loss: 0.3300\n",
      "Epoch [475/3000], Loss: 0.3307\n",
      "Epoch [476/3000], Loss: 0.3299\n",
      "Epoch [477/3000], Loss: 0.3296\n",
      "Epoch [478/3000], Loss: 0.3306\n",
      "Epoch [479/3000], Loss: 0.3305\n",
      "Epoch [480/3000], Loss: 0.3293\n",
      "Epoch [481/3000], Loss: 0.3293\n",
      "Epoch [482/3000], Loss: 0.3304\n",
      "Epoch [483/3000], Loss: 0.3305\n",
      "Epoch [484/3000], Loss: 0.3302\n",
      "Epoch [485/3000], Loss: 0.3292\n",
      "Epoch [486/3000], Loss: 0.3289\n",
      "Epoch [487/3000], Loss: 0.3299\n",
      "Epoch [488/3000], Loss: 0.3312\n",
      "Epoch [489/3000], Loss: 0.3308\n",
      "Epoch [490/3000], Loss: 0.3289\n",
      "Epoch [491/3000], Loss: 0.3304\n",
      "Epoch [492/3000], Loss: 0.3318\n",
      "Epoch [493/3000], Loss: 0.3291\n",
      "Epoch [494/3000], Loss: 0.3287\n",
      "Epoch [495/3000], Loss: 0.3303\n",
      "Epoch [496/3000], Loss: 0.3291\n",
      "Epoch [497/3000], Loss: 0.3281\n",
      "Epoch [498/3000], Loss: 0.3287\n",
      "Epoch [499/3000], Loss: 0.3309\n",
      "Epoch [500/3000], Loss: 0.3338\n",
      "Epoch [501/3000], Loss: 0.3294\n",
      "Epoch [502/3000], Loss: 0.3365\n",
      "Epoch [503/3000], Loss: 0.3348\n",
      "Epoch [504/3000], Loss: 0.3365\n",
      "Epoch [505/3000], Loss: 0.3304\n",
      "Epoch [506/3000], Loss: 0.3351\n",
      "Epoch [507/3000], Loss: 0.3332\n",
      "Epoch [508/3000], Loss: 0.3351\n",
      "Epoch [509/3000], Loss: 0.3317\n",
      "Epoch [510/3000], Loss: 0.3334\n",
      "Epoch [511/3000], Loss: 0.3309\n",
      "Epoch [512/3000], Loss: 0.3337\n",
      "Epoch [513/3000], Loss: 0.3321\n",
      "Epoch [514/3000], Loss: 0.3305\n",
      "Epoch [515/3000], Loss: 0.3306\n",
      "Epoch [516/3000], Loss: 0.3316\n",
      "Epoch [517/3000], Loss: 0.3309\n",
      "Epoch [518/3000], Loss: 0.3296\n",
      "Epoch [519/3000], Loss: 0.3294\n",
      "Epoch [520/3000], Loss: 0.3301\n",
      "Epoch [521/3000], Loss: 0.3292\n",
      "Epoch [522/3000], Loss: 0.3285\n",
      "Epoch [523/3000], Loss: 0.3290\n",
      "Epoch [524/3000], Loss: 0.3293\n",
      "Epoch [525/3000], Loss: 0.3285\n",
      "Epoch [526/3000], Loss: 0.3283\n",
      "Epoch [527/3000], Loss: 0.3285\n",
      "Epoch [528/3000], Loss: 0.3286\n",
      "Epoch [529/3000], Loss: 0.3283\n",
      "Epoch [530/3000], Loss: 0.3283\n",
      "Epoch [531/3000], Loss: 0.3283\n",
      "Epoch [532/3000], Loss: 0.3284\n",
      "Epoch [533/3000], Loss: 0.3285\n",
      "Epoch [534/3000], Loss: 0.3277\n",
      "Epoch [535/3000], Loss: 0.3275\n",
      "Epoch [536/3000], Loss: 0.3277\n",
      "Epoch [537/3000], Loss: 0.3275\n",
      "Epoch [538/3000], Loss: 0.3272\n",
      "Epoch [539/3000], Loss: 0.3271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [540/3000], Loss: 0.3270\n",
      "Epoch [541/3000], Loss: 0.3269\n",
      "Epoch [542/3000], Loss: 0.3270\n",
      "Epoch [543/3000], Loss: 0.3272\n",
      "Epoch [544/3000], Loss: 0.3280\n",
      "Epoch [545/3000], Loss: 0.3292\n",
      "Epoch [546/3000], Loss: 0.3302\n",
      "Epoch [547/3000], Loss: 0.3271\n",
      "Epoch [548/3000], Loss: 0.3304\n",
      "Epoch [549/3000], Loss: 0.3331\n",
      "Epoch [550/3000], Loss: 0.3276\n",
      "Epoch [551/3000], Loss: 0.3359\n",
      "Epoch [552/3000], Loss: 0.3355\n",
      "Epoch [553/3000], Loss: 0.3397\n",
      "Epoch [554/3000], Loss: 0.3366\n",
      "Epoch [555/3000], Loss: 0.3339\n",
      "Epoch [556/3000], Loss: 0.3374\n",
      "Epoch [557/3000], Loss: 0.3323\n",
      "Epoch [558/3000], Loss: 0.3323\n",
      "Epoch [559/3000], Loss: 0.3339\n",
      "Epoch [560/3000], Loss: 0.3322\n",
      "Epoch [561/3000], Loss: 0.3324\n",
      "Epoch [562/3000], Loss: 0.3298\n",
      "Epoch [563/3000], Loss: 0.3318\n",
      "Epoch [564/3000], Loss: 0.3308\n",
      "Epoch [565/3000], Loss: 0.3303\n",
      "Epoch [566/3000], Loss: 0.3288\n",
      "Epoch [567/3000], Loss: 0.3288\n",
      "Epoch [568/3000], Loss: 0.3290\n",
      "Epoch [569/3000], Loss: 0.3292\n",
      "Epoch [570/3000], Loss: 0.3294\n",
      "Epoch [571/3000], Loss: 0.3282\n",
      "Epoch [572/3000], Loss: 0.3296\n",
      "Epoch [573/3000], Loss: 0.3287\n",
      "Epoch [574/3000], Loss: 0.3278\n",
      "Epoch [575/3000], Loss: 0.3289\n",
      "Epoch [576/3000], Loss: 0.3283\n",
      "Epoch [577/3000], Loss: 0.3271\n",
      "Epoch [578/3000], Loss: 0.3281\n",
      "Epoch [579/3000], Loss: 0.3276\n",
      "Epoch [580/3000], Loss: 0.3269\n",
      "Epoch [581/3000], Loss: 0.3275\n",
      "Epoch [582/3000], Loss: 0.3279\n",
      "Epoch [583/3000], Loss: 0.3270\n",
      "Epoch [584/3000], Loss: 0.3266\n",
      "Epoch [585/3000], Loss: 0.3267\n",
      "Epoch [586/3000], Loss: 0.3267\n",
      "Epoch [587/3000], Loss: 0.3269\n",
      "Epoch [588/3000], Loss: 0.3264\n",
      "Epoch [589/3000], Loss: 0.3261\n",
      "Epoch [590/3000], Loss: 0.3259\n",
      "Epoch [591/3000], Loss: 0.3257\n",
      "Epoch [592/3000], Loss: 0.3262\n",
      "Epoch [593/3000], Loss: 0.3267\n",
      "Epoch [594/3000], Loss: 0.3265\n",
      "Epoch [595/3000], Loss: 0.3257\n",
      "Epoch [596/3000], Loss: 0.3253\n",
      "Epoch [597/3000], Loss: 0.3260\n",
      "Epoch [598/3000], Loss: 0.3285\n",
      "Epoch [599/3000], Loss: 0.3322\n",
      "Epoch [600/3000], Loss: 0.3270\n",
      "Epoch [601/3000], Loss: 0.3351\n",
      "Epoch [602/3000], Loss: 0.3330\n",
      "Epoch [603/3000], Loss: 0.3360\n",
      "Epoch [604/3000], Loss: 0.3285\n",
      "Epoch [605/3000], Loss: 0.3392\n",
      "Epoch [606/3000], Loss: 0.3297\n",
      "Epoch [607/3000], Loss: 0.3354\n",
      "Epoch [608/3000], Loss: 0.3318\n",
      "Epoch [609/3000], Loss: 0.3357\n",
      "Epoch [610/3000], Loss: 0.3305\n",
      "Epoch [611/3000], Loss: 0.3337\n",
      "Epoch [612/3000], Loss: 0.3332\n",
      "Epoch [613/3000], Loss: 0.3327\n",
      "Epoch [614/3000], Loss: 0.3287\n",
      "Epoch [615/3000], Loss: 0.3281\n",
      "Epoch [616/3000], Loss: 0.3292\n",
      "Epoch [617/3000], Loss: 0.3298\n",
      "Epoch [618/3000], Loss: 0.3282\n",
      "Epoch [619/3000], Loss: 0.3273\n",
      "Epoch [620/3000], Loss: 0.3271\n",
      "Epoch [621/3000], Loss: 0.3278\n",
      "Epoch [622/3000], Loss: 0.3271\n",
      "Epoch [623/3000], Loss: 0.3266\n",
      "Epoch [624/3000], Loss: 0.3270\n",
      "Epoch [625/3000], Loss: 0.3264\n",
      "Epoch [626/3000], Loss: 0.3260\n",
      "Epoch [627/3000], Loss: 0.3263\n",
      "Epoch [628/3000], Loss: 0.3261\n",
      "Epoch [629/3000], Loss: 0.3256\n",
      "Epoch [630/3000], Loss: 0.3255\n",
      "Epoch [631/3000], Loss: 0.3255\n",
      "Epoch [632/3000], Loss: 0.3253\n",
      "Epoch [633/3000], Loss: 0.3251\n",
      "Epoch [634/3000], Loss: 0.3251\n",
      "Epoch [635/3000], Loss: 0.3250\n",
      "Epoch [636/3000], Loss: 0.3248\n",
      "Epoch [637/3000], Loss: 0.3247\n",
      "Epoch [638/3000], Loss: 0.3246\n",
      "Epoch [639/3000], Loss: 0.3245\n",
      "Epoch [640/3000], Loss: 0.3244\n",
      "Epoch [641/3000], Loss: 0.3243\n",
      "Epoch [642/3000], Loss: 0.3242\n",
      "Epoch [643/3000], Loss: 0.3242\n",
      "Epoch [644/3000], Loss: 0.3241\n",
      "Epoch [645/3000], Loss: 0.3240\n",
      "Epoch [646/3000], Loss: 0.3240\n",
      "Epoch [647/3000], Loss: 0.3241\n",
      "Epoch [648/3000], Loss: 0.3247\n",
      "Epoch [649/3000], Loss: 0.3269\n",
      "Epoch [650/3000], Loss: 0.3283\n",
      "Epoch [651/3000], Loss: 0.3250\n",
      "Epoch [652/3000], Loss: 0.3318\n",
      "Epoch [653/3000], Loss: 0.3374\n",
      "Epoch [654/3000], Loss: 0.3269\n",
      "Epoch [655/3000], Loss: 0.3383\n",
      "Epoch [656/3000], Loss: 0.3338\n",
      "Epoch [657/3000], Loss: 0.3377\n",
      "Epoch [658/3000], Loss: 0.3383\n",
      "Epoch [659/3000], Loss: 0.3381\n",
      "Epoch [660/3000], Loss: 0.3374\n",
      "Epoch [661/3000], Loss: 0.3369\n",
      "Epoch [662/3000], Loss: 0.3368\n",
      "Epoch [663/3000], Loss: 0.3360\n",
      "Epoch [664/3000], Loss: 0.3352\n",
      "Epoch [665/3000], Loss: 0.3346\n",
      "Epoch [666/3000], Loss: 0.3338\n",
      "Epoch [667/3000], Loss: 0.3332\n",
      "Epoch [668/3000], Loss: 0.3325\n",
      "Epoch [669/3000], Loss: 0.3323\n",
      "Epoch [670/3000], Loss: 0.3319\n",
      "Epoch [671/3000], Loss: 0.3313\n",
      "Epoch [672/3000], Loss: 0.3310\n",
      "Epoch [673/3000], Loss: 0.3304\n",
      "Epoch [674/3000], Loss: 0.3299\n",
      "Epoch [675/3000], Loss: 0.3295\n",
      "Epoch [676/3000], Loss: 0.3293\n",
      "Epoch [677/3000], Loss: 0.3287\n",
      "Epoch [678/3000], Loss: 0.3279\n",
      "Epoch [679/3000], Loss: 0.3277\n",
      "Epoch [680/3000], Loss: 0.3275\n",
      "Epoch [681/3000], Loss: 0.3273\n",
      "Epoch [682/3000], Loss: 0.3270\n",
      "Epoch [683/3000], Loss: 0.3267\n",
      "Epoch [684/3000], Loss: 0.3268\n",
      "Epoch [685/3000], Loss: 0.3267\n",
      "Epoch [686/3000], Loss: 0.3262\n",
      "Epoch [687/3000], Loss: 0.3257\n",
      "Epoch [688/3000], Loss: 0.3259\n",
      "Epoch [689/3000], Loss: 0.3256\n",
      "Epoch [690/3000], Loss: 0.3249\n",
      "Epoch [691/3000], Loss: 0.3247\n",
      "Epoch [692/3000], Loss: 0.3247\n",
      "Epoch [693/3000], Loss: 0.3249\n",
      "Epoch [694/3000], Loss: 0.3260\n",
      "Epoch [695/3000], Loss: 0.3277\n",
      "Epoch [696/3000], Loss: 0.3256\n",
      "Epoch [697/3000], Loss: 0.3249\n",
      "Epoch [698/3000], Loss: 0.3253\n",
      "Epoch [699/3000], Loss: 0.3242\n",
      "Epoch [700/3000], Loss: 0.3242\n",
      "Epoch [701/3000], Loss: 0.3237\n",
      "Epoch [702/3000], Loss: 0.3236\n",
      "Epoch [703/3000], Loss: 0.3237\n",
      "Epoch [704/3000], Loss: 0.3236\n",
      "Epoch [705/3000], Loss: 0.3234\n",
      "Epoch [706/3000], Loss: 0.3242\n",
      "Epoch [707/3000], Loss: 0.3271\n",
      "Epoch [708/3000], Loss: 0.3327\n",
      "Epoch [709/3000], Loss: 0.3315\n",
      "Epoch [710/3000], Loss: 0.3341\n",
      "Epoch [711/3000], Loss: 0.3284\n",
      "Epoch [712/3000], Loss: 0.3361\n",
      "Epoch [713/3000], Loss: 0.3273\n",
      "Epoch [714/3000], Loss: 0.3323\n",
      "Epoch [715/3000], Loss: 0.3256\n",
      "Epoch [716/3000], Loss: 0.3295\n",
      "Epoch [717/3000], Loss: 0.3316\n",
      "Epoch [718/3000], Loss: 0.3255\n",
      "Epoch [719/3000], Loss: 0.3302\n",
      "Epoch [720/3000], Loss: 0.3273\n",
      "Epoch [721/3000], Loss: 0.3292\n",
      "Epoch [722/3000], Loss: 0.3265\n",
      "Epoch [723/3000], Loss: 0.3267\n",
      "Epoch [724/3000], Loss: 0.3251\n",
      "Epoch [725/3000], Loss: 0.3263\n",
      "Epoch [726/3000], Loss: 0.3272\n",
      "Epoch [727/3000], Loss: 0.3241\n",
      "Epoch [728/3000], Loss: 0.3254\n",
      "Epoch [729/3000], Loss: 0.3253\n",
      "Epoch [730/3000], Loss: 0.3237\n",
      "Epoch [731/3000], Loss: 0.3243\n",
      "Epoch [732/3000], Loss: 0.3243\n",
      "Epoch [733/3000], Loss: 0.3238\n",
      "Epoch [734/3000], Loss: 0.3237\n",
      "Epoch [735/3000], Loss: 0.3245\n",
      "Epoch [736/3000], Loss: 0.3243\n",
      "Epoch [737/3000], Loss: 0.3238\n",
      "Epoch [738/3000], Loss: 0.3243\n",
      "Epoch [739/3000], Loss: 0.3235\n",
      "Epoch [740/3000], Loss: 0.3233\n",
      "Epoch [741/3000], Loss: 0.3229\n",
      "Epoch [742/3000], Loss: 0.3224\n",
      "Epoch [743/3000], Loss: 0.3229\n",
      "Epoch [744/3000], Loss: 0.3231\n",
      "Epoch [745/3000], Loss: 0.3229\n",
      "Epoch [746/3000], Loss: 0.3224\n",
      "Epoch [747/3000], Loss: 0.3221\n",
      "Epoch [748/3000], Loss: 0.3220\n",
      "Epoch [749/3000], Loss: 0.3228\n",
      "Epoch [750/3000], Loss: 0.3236\n",
      "Epoch [751/3000], Loss: 0.3236\n",
      "Epoch [752/3000], Loss: 0.3225\n",
      "Epoch [753/3000], Loss: 0.3218\n",
      "Epoch [754/3000], Loss: 0.3219\n",
      "Epoch [755/3000], Loss: 0.3219\n",
      "Epoch [756/3000], Loss: 0.3255\n",
      "Epoch [757/3000], Loss: 0.3465\n",
      "Epoch [758/3000], Loss: 0.3352\n",
      "Epoch [759/3000], Loss: 0.3407\n",
      "Epoch [760/3000], Loss: 0.3401\n",
      "Epoch [761/3000], Loss: 0.3391\n",
      "Epoch [762/3000], Loss: 0.3392\n",
      "Epoch [763/3000], Loss: 0.3396\n",
      "Epoch [764/3000], Loss: 0.3393\n",
      "Epoch [765/3000], Loss: 0.3383\n",
      "Epoch [766/3000], Loss: 0.3379\n",
      "Epoch [767/3000], Loss: 0.3384\n",
      "Epoch [768/3000], Loss: 0.3380\n",
      "Epoch [769/3000], Loss: 0.3373\n",
      "Epoch [770/3000], Loss: 0.3374\n",
      "Epoch [771/3000], Loss: 0.3376\n",
      "Epoch [772/3000], Loss: 0.3373\n",
      "Epoch [773/3000], Loss: 0.3368\n",
      "Epoch [774/3000], Loss: 0.3368\n",
      "Epoch [775/3000], Loss: 0.3368\n",
      "Epoch [776/3000], Loss: 0.3363\n",
      "Epoch [777/3000], Loss: 0.3361\n",
      "Epoch [778/3000], Loss: 0.3360\n",
      "Epoch [779/3000], Loss: 0.3358\n",
      "Epoch [780/3000], Loss: 0.3355\n",
      "Epoch [781/3000], Loss: 0.3354\n",
      "Epoch [782/3000], Loss: 0.3353\n",
      "Epoch [783/3000], Loss: 0.3351\n",
      "Epoch [784/3000], Loss: 0.3350\n",
      "Epoch [785/3000], Loss: 0.3348\n",
      "Epoch [786/3000], Loss: 0.3346\n",
      "Epoch [787/3000], Loss: 0.3345\n",
      "Epoch [788/3000], Loss: 0.3343\n",
      "Epoch [789/3000], Loss: 0.3342\n",
      "Epoch [790/3000], Loss: 0.3341\n",
      "Epoch [791/3000], Loss: 0.3340\n",
      "Epoch [792/3000], Loss: 0.3337\n",
      "Epoch [793/3000], Loss: 0.3336\n",
      "Epoch [794/3000], Loss: 0.3334\n",
      "Epoch [795/3000], Loss: 0.3333\n",
      "Epoch [796/3000], Loss: 0.3330\n",
      "Epoch [797/3000], Loss: 0.3329\n",
      "Epoch [798/3000], Loss: 0.3327\n",
      "Epoch [799/3000], Loss: 0.3326\n",
      "Epoch [800/3000], Loss: 0.3323\n",
      "Epoch [801/3000], Loss: 0.3321\n",
      "Epoch [802/3000], Loss: 0.3320\n",
      "Epoch [803/3000], Loss: 0.3317\n",
      "Epoch [804/3000], Loss: 0.3316\n",
      "Epoch [805/3000], Loss: 0.3314\n",
      "Epoch [806/3000], Loss: 0.3312\n",
      "Epoch [807/3000], Loss: 0.3310\n",
      "Epoch [808/3000], Loss: 0.3308\n",
      "Epoch [809/3000], Loss: 0.3306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [810/3000], Loss: 0.3304\n",
      "Epoch [811/3000], Loss: 0.3303\n",
      "Epoch [812/3000], Loss: 0.3304\n",
      "Epoch [813/3000], Loss: 0.3310\n",
      "Epoch [814/3000], Loss: 0.3330\n",
      "Epoch [815/3000], Loss: 0.3300\n",
      "Epoch [816/3000], Loss: 0.3348\n",
      "Epoch [817/3000], Loss: 0.3373\n",
      "Epoch [818/3000], Loss: 0.3383\n",
      "Epoch [819/3000], Loss: 0.3303\n",
      "Epoch [820/3000], Loss: 0.3432\n",
      "Epoch [821/3000], Loss: 0.3358\n",
      "Epoch [822/3000], Loss: 0.3475\n",
      "Epoch [823/3000], Loss: 0.3384\n",
      "Epoch [824/3000], Loss: 0.3324\n",
      "Epoch [825/3000], Loss: 0.3486\n",
      "Epoch [826/3000], Loss: 0.3329\n",
      "Epoch [827/3000], Loss: 0.3437\n",
      "Epoch [828/3000], Loss: 0.3457\n",
      "Epoch [829/3000], Loss: 0.3373\n",
      "Epoch [830/3000], Loss: 0.3330\n",
      "Epoch [831/3000], Loss: 0.3432\n",
      "Epoch [832/3000], Loss: 0.3333\n",
      "Epoch [833/3000], Loss: 0.3338\n",
      "Epoch [834/3000], Loss: 0.3368\n",
      "Epoch [835/3000], Loss: 0.3375\n",
      "Epoch [836/3000], Loss: 0.3353\n",
      "Epoch [837/3000], Loss: 0.3336\n",
      "Epoch [838/3000], Loss: 0.3347\n",
      "Epoch [839/3000], Loss: 0.3349\n",
      "Epoch [840/3000], Loss: 0.3327\n",
      "Epoch [841/3000], Loss: 0.3327\n",
      "Epoch [842/3000], Loss: 0.3334\n",
      "Epoch [843/3000], Loss: 0.3337\n",
      "Epoch [844/3000], Loss: 0.3333\n",
      "Epoch [845/3000], Loss: 0.3327\n",
      "Epoch [846/3000], Loss: 0.3327\n",
      "Epoch [847/3000], Loss: 0.3328\n",
      "Epoch [848/3000], Loss: 0.3323\n",
      "Epoch [849/3000], Loss: 0.3317\n",
      "Epoch [850/3000], Loss: 0.3316\n",
      "Epoch [851/3000], Loss: 0.3317\n",
      "Epoch [852/3000], Loss: 0.3315\n",
      "Epoch [853/3000], Loss: 0.3313\n",
      "Epoch [854/3000], Loss: 0.3311\n",
      "Epoch [855/3000], Loss: 0.3309\n",
      "Epoch [856/3000], Loss: 0.3307\n",
      "Epoch [857/3000], Loss: 0.3307\n",
      "Epoch [858/3000], Loss: 0.3304\n",
      "Epoch [859/3000], Loss: 0.3301\n",
      "Epoch [860/3000], Loss: 0.3299\n",
      "Epoch [861/3000], Loss: 0.3296\n",
      "Epoch [862/3000], Loss: 0.3296\n",
      "Epoch [863/3000], Loss: 0.3293\n",
      "Epoch [864/3000], Loss: 0.3292\n",
      "Epoch [865/3000], Loss: 0.3289\n",
      "Epoch [866/3000], Loss: 0.3288\n",
      "Epoch [867/3000], Loss: 0.3287\n",
      "Epoch [868/3000], Loss: 0.3285\n",
      "Epoch [869/3000], Loss: 0.3283\n",
      "Epoch [870/3000], Loss: 0.3282\n",
      "Epoch [871/3000], Loss: 0.3281\n",
      "Epoch [872/3000], Loss: 0.3279\n",
      "Epoch [873/3000], Loss: 0.3277\n",
      "Epoch [874/3000], Loss: 0.3275\n",
      "Epoch [875/3000], Loss: 0.3273\n",
      "Epoch [876/3000], Loss: 0.3271\n",
      "Epoch [877/3000], Loss: 0.3269\n",
      "Epoch [878/3000], Loss: 0.3268\n",
      "Epoch [879/3000], Loss: 0.3266\n",
      "Epoch [880/3000], Loss: 0.3264\n",
      "Epoch [881/3000], Loss: 0.3264\n",
      "Epoch [882/3000], Loss: 0.3265\n",
      "Epoch [883/3000], Loss: 0.3270\n",
      "Epoch [884/3000], Loss: 0.3273\n",
      "Epoch [885/3000], Loss: 0.3259\n",
      "Epoch [886/3000], Loss: 0.3268\n",
      "Epoch [887/3000], Loss: 0.3282\n",
      "Epoch [888/3000], Loss: 0.3256\n",
      "Epoch [889/3000], Loss: 0.3269\n",
      "Epoch [890/3000], Loss: 0.3297\n",
      "Epoch [891/3000], Loss: 0.3254\n",
      "Epoch [892/3000], Loss: 0.3291\n",
      "Epoch [893/3000], Loss: 0.3287\n",
      "Epoch [894/3000], Loss: 0.3267\n",
      "Epoch [895/3000], Loss: 0.3283\n",
      "Epoch [896/3000], Loss: 0.3263\n",
      "Epoch [897/3000], Loss: 0.3270\n",
      "Epoch [898/3000], Loss: 0.3256\n",
      "Epoch [899/3000], Loss: 0.3268\n",
      "Epoch [900/3000], Loss: 0.3252\n",
      "Epoch [901/3000], Loss: 0.3253\n",
      "Epoch [902/3000], Loss: 0.3274\n",
      "Epoch [903/3000], Loss: 0.3277\n",
      "Epoch [904/3000], Loss: 0.3245\n",
      "Epoch [905/3000], Loss: 0.3272\n",
      "Epoch [906/3000], Loss: 0.3264\n",
      "Epoch [907/3000], Loss: 0.3253\n",
      "Epoch [908/3000], Loss: 0.3266\n",
      "Epoch [909/3000], Loss: 0.3247\n",
      "Epoch [910/3000], Loss: 0.3261\n",
      "Epoch [911/3000], Loss: 0.3247\n",
      "Epoch [912/3000], Loss: 0.3246\n",
      "Epoch [913/3000], Loss: 0.3266\n",
      "Epoch [914/3000], Loss: 0.3240\n",
      "Epoch [915/3000], Loss: 0.3248\n",
      "Epoch [916/3000], Loss: 0.3276\n",
      "Epoch [917/3000], Loss: 0.3240\n",
      "Epoch [918/3000], Loss: 0.3252\n",
      "Epoch [919/3000], Loss: 0.3271\n",
      "Epoch [920/3000], Loss: 0.3238\n",
      "Epoch [921/3000], Loss: 0.3269\n",
      "Epoch [922/3000], Loss: 0.3281\n",
      "Epoch [923/3000], Loss: 0.3242\n",
      "Epoch [924/3000], Loss: 0.3282\n",
      "Epoch [925/3000], Loss: 0.3242\n",
      "Epoch [926/3000], Loss: 0.3269\n",
      "Epoch [927/3000], Loss: 0.3246\n",
      "Epoch [928/3000], Loss: 0.3259\n",
      "Epoch [929/3000], Loss: 0.3263\n",
      "Epoch [930/3000], Loss: 0.3237\n",
      "Epoch [931/3000], Loss: 0.3279\n",
      "Epoch [932/3000], Loss: 0.3279\n",
      "Epoch [933/3000], Loss: 0.3239\n",
      "Epoch [934/3000], Loss: 0.3282\n",
      "Epoch [935/3000], Loss: 0.3239\n",
      "Epoch [936/3000], Loss: 0.3276\n",
      "Epoch [937/3000], Loss: 0.3240\n",
      "Epoch [938/3000], Loss: 0.3260\n",
      "Epoch [939/3000], Loss: 0.3243\n",
      "Epoch [940/3000], Loss: 0.3243\n",
      "Epoch [941/3000], Loss: 0.3260\n",
      "Epoch [942/3000], Loss: 0.3243\n",
      "Epoch [943/3000], Loss: 0.3233\n",
      "Epoch [944/3000], Loss: 0.3252\n",
      "Epoch [945/3000], Loss: 0.3234\n",
      "Epoch [946/3000], Loss: 0.3234\n",
      "Epoch [947/3000], Loss: 0.3243\n",
      "Epoch [948/3000], Loss: 0.3229\n",
      "Epoch [949/3000], Loss: 0.3229\n",
      "Epoch [950/3000], Loss: 0.3243\n",
      "Epoch [951/3000], Loss: 0.3248\n",
      "Epoch [952/3000], Loss: 0.3224\n",
      "Epoch [953/3000], Loss: 0.3238\n",
      "Epoch [954/3000], Loss: 0.3247\n",
      "Epoch [955/3000], Loss: 0.3223\n",
      "Epoch [956/3000], Loss: 0.3249\n",
      "Epoch [957/3000], Loss: 0.3264\n",
      "Epoch [958/3000], Loss: 0.3223\n",
      "Epoch [959/3000], Loss: 0.3256\n",
      "Epoch [960/3000], Loss: 0.3250\n",
      "Epoch [961/3000], Loss: 0.3228\n",
      "Epoch [962/3000], Loss: 0.3256\n",
      "Epoch [963/3000], Loss: 0.3228\n",
      "Epoch [964/3000], Loss: 0.3231\n",
      "Epoch [965/3000], Loss: 0.3245\n",
      "Epoch [966/3000], Loss: 0.3224\n",
      "Epoch [967/3000], Loss: 0.3226\n",
      "Epoch [968/3000], Loss: 0.3258\n",
      "Epoch [969/3000], Loss: 0.3288\n",
      "Epoch [970/3000], Loss: 0.3228\n",
      "Epoch [971/3000], Loss: 0.3257\n",
      "Epoch [972/3000], Loss: 0.3239\n",
      "Epoch [973/3000], Loss: 0.3236\n",
      "Epoch [974/3000], Loss: 0.3245\n",
      "Epoch [975/3000], Loss: 0.3225\n",
      "Epoch [976/3000], Loss: 0.3250\n",
      "Epoch [977/3000], Loss: 0.3220\n",
      "Epoch [978/3000], Loss: 0.3224\n",
      "Epoch [979/3000], Loss: 0.3269\n",
      "Epoch [980/3000], Loss: 0.3228\n",
      "Epoch [981/3000], Loss: 0.3216\n",
      "Epoch [982/3000], Loss: 0.3229\n",
      "Epoch [983/3000], Loss: 0.3220\n",
      "Epoch [984/3000], Loss: 0.3214\n",
      "Epoch [985/3000], Loss: 0.3222\n",
      "Epoch [986/3000], Loss: 0.3220\n",
      "Epoch [987/3000], Loss: 0.3216\n",
      "Epoch [988/3000], Loss: 0.3212\n",
      "Epoch [989/3000], Loss: 0.3210\n",
      "Epoch [990/3000], Loss: 0.3211\n",
      "Epoch [991/3000], Loss: 0.3210\n",
      "Epoch [992/3000], Loss: 0.3216\n",
      "Epoch [993/3000], Loss: 0.3224\n",
      "Epoch [994/3000], Loss: 0.3242\n",
      "Epoch [995/3000], Loss: 0.3219\n",
      "Epoch [996/3000], Loss: 0.3211\n",
      "Epoch [997/3000], Loss: 0.3223\n",
      "Epoch [998/3000], Loss: 0.3215\n",
      "Epoch [999/3000], Loss: 0.3207\n",
      "Epoch [1000/3000], Loss: 0.3204\n",
      "Epoch [1001/3000], Loss: 0.3211\n",
      "Epoch [1002/3000], Loss: 0.3252\n",
      "Epoch [1003/3000], Loss: 0.3266\n",
      "Epoch [1004/3000], Loss: 0.3280\n",
      "Epoch [1005/3000], Loss: 0.3223\n",
      "Epoch [1006/3000], Loss: 0.3251\n",
      "Epoch [1007/3000], Loss: 0.3225\n",
      "Epoch [1008/3000], Loss: 0.3259\n",
      "Epoch [1009/3000], Loss: 0.3224\n",
      "Epoch [1010/3000], Loss: 0.3237\n",
      "Epoch [1011/3000], Loss: 0.3235\n",
      "Epoch [1012/3000], Loss: 0.3207\n",
      "Epoch [1013/3000], Loss: 0.3238\n",
      "Epoch [1014/3000], Loss: 0.3336\n",
      "Epoch [1015/3000], Loss: 0.3244\n",
      "Epoch [1016/3000], Loss: 0.3222\n",
      "Epoch [1017/3000], Loss: 0.3253\n",
      "Epoch [1018/3000], Loss: 0.3217\n",
      "Epoch [1019/3000], Loss: 0.3241\n",
      "Epoch [1020/3000], Loss: 0.3216\n",
      "Epoch [1021/3000], Loss: 0.3235\n",
      "Epoch [1022/3000], Loss: 0.3210\n",
      "Epoch [1023/3000], Loss: 0.3214\n",
      "Epoch [1024/3000], Loss: 0.3247\n",
      "Epoch [1025/3000], Loss: 0.3216\n",
      "Epoch [1026/3000], Loss: 0.3205\n",
      "Epoch [1027/3000], Loss: 0.3216\n",
      "Epoch [1028/3000], Loss: 0.3211\n",
      "Epoch [1029/3000], Loss: 0.3202\n",
      "Epoch [1030/3000], Loss: 0.3204\n",
      "Epoch [1031/3000], Loss: 0.3206\n",
      "Epoch [1032/3000], Loss: 0.3207\n",
      "Epoch [1033/3000], Loss: 0.3204\n",
      "Epoch [1034/3000], Loss: 0.3203\n",
      "Epoch [1035/3000], Loss: 0.3201\n",
      "Epoch [1036/3000], Loss: 0.3198\n",
      "Epoch [1037/3000], Loss: 0.3199\n",
      "Epoch [1038/3000], Loss: 0.3199\n",
      "Epoch [1039/3000], Loss: 0.3204\n",
      "Epoch [1040/3000], Loss: 0.3216\n",
      "Epoch [1041/3000], Loss: 0.3217\n",
      "Epoch [1042/3000], Loss: 0.3210\n",
      "Epoch [1043/3000], Loss: 0.3200\n",
      "Epoch [1044/3000], Loss: 0.3205\n",
      "Epoch [1045/3000], Loss: 0.3212\n",
      "Epoch [1046/3000], Loss: 0.3202\n",
      "Epoch [1047/3000], Loss: 0.3195\n",
      "Epoch [1048/3000], Loss: 0.3192\n",
      "Epoch [1049/3000], Loss: 0.3193\n",
      "Epoch [1050/3000], Loss: 0.3193\n",
      "Epoch [1051/3000], Loss: 0.3203\n",
      "Epoch [1052/3000], Loss: 0.3223\n",
      "Epoch [1053/3000], Loss: 0.3274\n",
      "Epoch [1054/3000], Loss: 0.3227\n",
      "Epoch [1055/3000], Loss: 0.3220\n",
      "Epoch [1056/3000], Loss: 0.3237\n",
      "Epoch [1057/3000], Loss: 0.3205\n",
      "Epoch [1058/3000], Loss: 0.3224\n",
      "Epoch [1059/3000], Loss: 0.3201\n",
      "Epoch [1060/3000], Loss: 0.3197\n",
      "Epoch [1061/3000], Loss: 0.3244\n",
      "Epoch [1062/3000], Loss: 0.3352\n",
      "Epoch [1063/3000], Loss: 0.3245\n",
      "Epoch [1064/3000], Loss: 0.3223\n",
      "Epoch [1065/3000], Loss: 0.3249\n",
      "Epoch [1066/3000], Loss: 0.3215\n",
      "Epoch [1067/3000], Loss: 0.3234\n",
      "Epoch [1068/3000], Loss: 0.3216\n",
      "Epoch [1069/3000], Loss: 0.3226\n",
      "Epoch [1070/3000], Loss: 0.3197\n",
      "Epoch [1071/3000], Loss: 0.3234\n",
      "Epoch [1072/3000], Loss: 0.3290\n",
      "Epoch [1073/3000], Loss: 0.3201\n",
      "Epoch [1074/3000], Loss: 0.3237\n",
      "Epoch [1075/3000], Loss: 0.3268\n",
      "Epoch [1076/3000], Loss: 0.3202\n",
      "Epoch [1077/3000], Loss: 0.3257\n",
      "Epoch [1078/3000], Loss: 0.3210\n",
      "Epoch [1079/3000], Loss: 0.3260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1080/3000], Loss: 0.3206\n",
      "Epoch [1081/3000], Loss: 0.3237\n",
      "Epoch [1082/3000], Loss: 0.3199\n",
      "Epoch [1083/3000], Loss: 0.3226\n",
      "Epoch [1084/3000], Loss: 0.3211\n",
      "Epoch [1085/3000], Loss: 0.3191\n",
      "Epoch [1086/3000], Loss: 0.3209\n",
      "Epoch [1087/3000], Loss: 0.3206\n",
      "Epoch [1088/3000], Loss: 0.3194\n",
      "Epoch [1089/3000], Loss: 0.3189\n",
      "Epoch [1090/3000], Loss: 0.3193\n",
      "Epoch [1091/3000], Loss: 0.3195\n",
      "Epoch [1092/3000], Loss: 0.3187\n",
      "Epoch [1093/3000], Loss: 0.3191\n",
      "Epoch [1094/3000], Loss: 0.3197\n",
      "Epoch [1095/3000], Loss: 0.3197\n",
      "Epoch [1096/3000], Loss: 0.3196\n",
      "Epoch [1097/3000], Loss: 0.3190\n",
      "Epoch [1098/3000], Loss: 0.3184\n",
      "Epoch [1099/3000], Loss: 0.3184\n",
      "Epoch [1100/3000], Loss: 0.3188\n",
      "Epoch [1101/3000], Loss: 0.3198\n",
      "Epoch [1102/3000], Loss: 0.3200\n",
      "Epoch [1103/3000], Loss: 0.3206\n",
      "Epoch [1104/3000], Loss: 0.3191\n",
      "Epoch [1105/3000], Loss: 0.3185\n",
      "Epoch [1106/3000], Loss: 0.3197\n",
      "Epoch [1107/3000], Loss: 0.3196\n",
      "Epoch [1108/3000], Loss: 0.3186\n",
      "Epoch [1109/3000], Loss: 0.3179\n",
      "Epoch [1110/3000], Loss: 0.3182\n",
      "Epoch [1111/3000], Loss: 0.3194\n",
      "Epoch [1112/3000], Loss: 0.3215\n",
      "Epoch [1113/3000], Loss: 0.3267\n",
      "Epoch [1114/3000], Loss: 0.3203\n",
      "Epoch [1115/3000], Loss: 0.3218\n",
      "Epoch [1116/3000], Loss: 0.3206\n",
      "Epoch [1117/3000], Loss: 0.3201\n",
      "Epoch [1118/3000], Loss: 0.3210\n",
      "Epoch [1119/3000], Loss: 0.3186\n",
      "Epoch [1120/3000], Loss: 0.3219\n",
      "Epoch [1121/3000], Loss: 0.3211\n",
      "Epoch [1122/3000], Loss: 0.3207\n",
      "Epoch [1123/3000], Loss: 0.3183\n",
      "Epoch [1124/3000], Loss: 0.3183\n",
      "Epoch [1125/3000], Loss: 0.3197\n",
      "Epoch [1126/3000], Loss: 0.3182\n",
      "Epoch [1127/3000], Loss: 0.3178\n",
      "Epoch [1128/3000], Loss: 0.3190\n",
      "Epoch [1129/3000], Loss: 0.3196\n",
      "Epoch [1130/3000], Loss: 0.3204\n",
      "Epoch [1131/3000], Loss: 0.3186\n",
      "Epoch [1132/3000], Loss: 0.3175\n",
      "Epoch [1133/3000], Loss: 0.3188\n",
      "Epoch [1134/3000], Loss: 0.3193\n",
      "Epoch [1135/3000], Loss: 0.3192\n",
      "Epoch [1136/3000], Loss: 0.3176\n",
      "Epoch [1137/3000], Loss: 0.3177\n",
      "Epoch [1138/3000], Loss: 0.3196\n",
      "Epoch [1139/3000], Loss: 0.3207\n",
      "Epoch [1140/3000], Loss: 0.3225\n",
      "Epoch [1141/3000], Loss: 0.3185\n",
      "Epoch [1142/3000], Loss: 0.3190\n",
      "Epoch [1143/3000], Loss: 0.3206\n",
      "Epoch [1144/3000], Loss: 0.3176\n",
      "Epoch [1145/3000], Loss: 0.3191\n",
      "Epoch [1146/3000], Loss: 0.3226\n",
      "Epoch [1147/3000], Loss: 0.3191\n",
      "Epoch [1148/3000], Loss: 0.3173\n",
      "Epoch [1149/3000], Loss: 0.3180\n",
      "Epoch [1150/3000], Loss: 0.3188\n",
      "Epoch [1151/3000], Loss: 0.3182\n",
      "Epoch [1152/3000], Loss: 0.3169\n",
      "Epoch [1153/3000], Loss: 0.3184\n",
      "Epoch [1154/3000], Loss: 0.3221\n",
      "Epoch [1155/3000], Loss: 0.3196\n",
      "Epoch [1156/3000], Loss: 0.3174\n",
      "Epoch [1157/3000], Loss: 0.3177\n",
      "Epoch [1158/3000], Loss: 0.3194\n",
      "Epoch [1159/3000], Loss: 0.3183\n",
      "Epoch [1160/3000], Loss: 0.3169\n",
      "Epoch [1161/3000], Loss: 0.3175\n",
      "Epoch [1162/3000], Loss: 0.3203\n",
      "Epoch [1163/3000], Loss: 0.3197\n",
      "Epoch [1164/3000], Loss: 0.3189\n",
      "Epoch [1165/3000], Loss: 0.3173\n",
      "Epoch [1166/3000], Loss: 0.3179\n",
      "Epoch [1167/3000], Loss: 0.3185\n",
      "Epoch [1168/3000], Loss: 0.3166\n",
      "Epoch [1169/3000], Loss: 0.3169\n",
      "Epoch [1170/3000], Loss: 0.3196\n",
      "Epoch [1171/3000], Loss: 0.3217\n",
      "Epoch [1172/3000], Loss: 0.3278\n",
      "Epoch [1173/3000], Loss: 0.3195\n",
      "Epoch [1174/3000], Loss: 0.3222\n",
      "Epoch [1175/3000], Loss: 0.3211\n",
      "Epoch [1176/3000], Loss: 0.3186\n",
      "Epoch [1177/3000], Loss: 0.3185\n",
      "Epoch [1178/3000], Loss: 0.3171\n",
      "Epoch [1179/3000], Loss: 0.3173\n",
      "Epoch [1180/3000], Loss: 0.3171\n",
      "Epoch [1181/3000], Loss: 0.3179\n",
      "Epoch [1182/3000], Loss: 0.3189\n",
      "Epoch [1183/3000], Loss: 0.3187\n",
      "Epoch [1184/3000], Loss: 0.3171\n",
      "Epoch [1185/3000], Loss: 0.3183\n",
      "Epoch [1186/3000], Loss: 0.3183\n",
      "Epoch [1187/3000], Loss: 0.3164\n",
      "Epoch [1188/3000], Loss: 0.3166\n",
      "Epoch [1189/3000], Loss: 0.3177\n",
      "Epoch [1190/3000], Loss: 0.3186\n",
      "Epoch [1191/3000], Loss: 0.3207\n",
      "Epoch [1192/3000], Loss: 0.3180\n",
      "Epoch [1193/3000], Loss: 0.3171\n",
      "Epoch [1194/3000], Loss: 0.3187\n",
      "Epoch [1195/3000], Loss: 0.3166\n",
      "Epoch [1196/3000], Loss: 0.3165\n",
      "Epoch [1197/3000], Loss: 0.3195\n",
      "Epoch [1198/3000], Loss: 0.3198\n",
      "Epoch [1199/3000], Loss: 0.3213\n",
      "Epoch [1200/3000], Loss: 0.3175\n",
      "Epoch [1201/3000], Loss: 0.3179\n",
      "Epoch [1202/3000], Loss: 0.3189\n",
      "Epoch [1203/3000], Loss: 0.3162\n",
      "Epoch [1204/3000], Loss: 0.3179\n",
      "Epoch [1205/3000], Loss: 0.3210\n",
      "Epoch [1206/3000], Loss: 0.3170\n",
      "Epoch [1207/3000], Loss: 0.3157\n",
      "Epoch [1208/3000], Loss: 0.3167\n",
      "Epoch [1209/3000], Loss: 0.3173\n",
      "Epoch [1210/3000], Loss: 0.3163\n",
      "Epoch [1211/3000], Loss: 0.3155\n",
      "Epoch [1212/3000], Loss: 0.3170\n",
      "Epoch [1213/3000], Loss: 0.3203\n",
      "Epoch [1214/3000], Loss: 0.3182\n",
      "Epoch [1215/3000], Loss: 0.3163\n",
      "Epoch [1216/3000], Loss: 0.3160\n",
      "Epoch [1217/3000], Loss: 0.3170\n",
      "Epoch [1218/3000], Loss: 0.3158\n",
      "Epoch [1219/3000], Loss: 0.3151\n",
      "Epoch [1220/3000], Loss: 0.3162\n",
      "Epoch [1221/3000], Loss: 0.3200\n",
      "Epoch [1222/3000], Loss: 0.3213\n",
      "Epoch [1223/3000], Loss: 0.3204\n",
      "Epoch [1224/3000], Loss: 0.3171\n",
      "Epoch [1225/3000], Loss: 0.3192\n",
      "Epoch [1226/3000], Loss: 0.3171\n",
      "Epoch [1227/3000], Loss: 0.3182\n",
      "Epoch [1228/3000], Loss: 0.3167\n",
      "Epoch [1229/3000], Loss: 0.3156\n",
      "Epoch [1230/3000], Loss: 0.3191\n",
      "Epoch [1231/3000], Loss: 0.3222\n",
      "Epoch [1232/3000], Loss: 0.3301\n",
      "Epoch [1233/3000], Loss: 0.3189\n",
      "Epoch [1234/3000], Loss: 0.3230\n",
      "Epoch [1235/3000], Loss: 0.3186\n",
      "Epoch [1236/3000], Loss: 0.3243\n",
      "Epoch [1237/3000], Loss: 0.3188\n",
      "Epoch [1238/3000], Loss: 0.3216\n",
      "Epoch [1239/3000], Loss: 0.3167\n",
      "Epoch [1240/3000], Loss: 0.3204\n",
      "Epoch [1241/3000], Loss: 0.3185\n",
      "Epoch [1242/3000], Loss: 0.3160\n",
      "Epoch [1243/3000], Loss: 0.3160\n",
      "Epoch [1244/3000], Loss: 0.3170\n",
      "Epoch [1245/3000], Loss: 0.3163\n",
      "Epoch [1246/3000], Loss: 0.3157\n",
      "Epoch [1247/3000], Loss: 0.3161\n",
      "Epoch [1248/3000], Loss: 0.3157\n",
      "Epoch [1249/3000], Loss: 0.3151\n",
      "Epoch [1250/3000], Loss: 0.3150\n",
      "Epoch [1251/3000], Loss: 0.3158\n",
      "Epoch [1252/3000], Loss: 0.3167\n",
      "Epoch [1253/3000], Loss: 0.3166\n",
      "Epoch [1254/3000], Loss: 0.3152\n",
      "Epoch [1255/3000], Loss: 0.3151\n",
      "Epoch [1256/3000], Loss: 0.3156\n",
      "Epoch [1257/3000], Loss: 0.3150\n",
      "Epoch [1258/3000], Loss: 0.3146\n",
      "Epoch [1259/3000], Loss: 0.3143\n",
      "Epoch [1260/3000], Loss: 0.3144\n",
      "Epoch [1261/3000], Loss: 0.3150\n",
      "Epoch [1262/3000], Loss: 0.3160\n",
      "Epoch [1263/3000], Loss: 0.3168\n",
      "Epoch [1264/3000], Loss: 0.3151\n",
      "Epoch [1265/3000], Loss: 0.3148\n",
      "Epoch [1266/3000], Loss: 0.3160\n",
      "Epoch [1267/3000], Loss: 0.3160\n",
      "Epoch [1268/3000], Loss: 0.3159\n",
      "Epoch [1269/3000], Loss: 0.3146\n",
      "Epoch [1270/3000], Loss: 0.3139\n",
      "Epoch [1271/3000], Loss: 0.3141\n",
      "Epoch [1272/3000], Loss: 0.3151\n",
      "Epoch [1273/3000], Loss: 0.3185\n",
      "Epoch [1274/3000], Loss: 0.3183\n",
      "Epoch [1275/3000], Loss: 0.3166\n",
      "Epoch [1276/3000], Loss: 0.3148\n",
      "Epoch [1277/3000], Loss: 0.3158\n",
      "Epoch [1278/3000], Loss: 0.3148\n",
      "Epoch [1279/3000], Loss: 0.3139\n",
      "Epoch [1280/3000], Loss: 0.3139\n",
      "Epoch [1281/3000], Loss: 0.3157\n",
      "Epoch [1282/3000], Loss: 0.3206\n",
      "Epoch [1283/3000], Loss: 0.3304\n",
      "Epoch [1284/3000], Loss: 0.3186\n",
      "Epoch [1285/3000], Loss: 0.3251\n",
      "Epoch [1286/3000], Loss: 0.3196\n",
      "Epoch [1287/3000], Loss: 0.3247\n",
      "Epoch [1288/3000], Loss: 0.3179\n",
      "Epoch [1289/3000], Loss: 0.3205\n",
      "Epoch [1290/3000], Loss: 0.3176\n",
      "Epoch [1291/3000], Loss: 0.3200\n",
      "Epoch [1292/3000], Loss: 0.3158\n",
      "Epoch [1293/3000], Loss: 0.3173\n",
      "Epoch [1294/3000], Loss: 0.3240\n",
      "Epoch [1295/3000], Loss: 0.3179\n",
      "Epoch [1296/3000], Loss: 0.3188\n",
      "Epoch [1297/3000], Loss: 0.3176\n",
      "Epoch [1298/3000], Loss: 0.3177\n",
      "Epoch [1299/3000], Loss: 0.3178\n",
      "Epoch [1300/3000], Loss: 0.3170\n",
      "Epoch [1301/3000], Loss: 0.3173\n",
      "Epoch [1302/3000], Loss: 0.3152\n",
      "Epoch [1303/3000], Loss: 0.3179\n",
      "Epoch [1304/3000], Loss: 0.3251\n",
      "Epoch [1305/3000], Loss: 0.3163\n",
      "Epoch [1306/3000], Loss: 0.3188\n",
      "Epoch [1307/3000], Loss: 0.3168\n",
      "Epoch [1308/3000], Loss: 0.3175\n",
      "Epoch [1309/3000], Loss: 0.3173\n",
      "Epoch [1310/3000], Loss: 0.3167\n",
      "Epoch [1311/3000], Loss: 0.3176\n",
      "Epoch [1312/3000], Loss: 0.3143\n",
      "Epoch [1313/3000], Loss: 0.3191\n",
      "Epoch [1314/3000], Loss: 0.3240\n",
      "Epoch [1315/3000], Loss: 0.3149\n",
      "Epoch [1316/3000], Loss: 0.3209\n",
      "Epoch [1317/3000], Loss: 0.3154\n",
      "Epoch [1318/3000], Loss: 0.3204\n",
      "Epoch [1319/3000], Loss: 0.3157\n",
      "Epoch [1320/3000], Loss: 0.3183\n",
      "Epoch [1321/3000], Loss: 0.3155\n",
      "Epoch [1322/3000], Loss: 0.3159\n",
      "Epoch [1323/3000], Loss: 0.3189\n",
      "Epoch [1324/3000], Loss: 0.3165\n",
      "Epoch [1325/3000], Loss: 0.3146\n",
      "Epoch [1326/3000], Loss: 0.3175\n",
      "Epoch [1327/3000], Loss: 0.3143\n",
      "Epoch [1328/3000], Loss: 0.3175\n",
      "Epoch [1329/3000], Loss: 0.3143\n",
      "Epoch [1330/3000], Loss: 0.3149\n",
      "Epoch [1331/3000], Loss: 0.3184\n",
      "Epoch [1332/3000], Loss: 0.3136\n",
      "Epoch [1333/3000], Loss: 0.3152\n",
      "Epoch [1334/3000], Loss: 0.3185\n",
      "Epoch [1335/3000], Loss: 0.3137\n",
      "Epoch [1336/3000], Loss: 0.3171\n",
      "Epoch [1337/3000], Loss: 0.3150\n",
      "Epoch [1338/3000], Loss: 0.3143\n",
      "Epoch [1339/3000], Loss: 0.3166\n",
      "Epoch [1340/3000], Loss: 0.3137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1341/3000], Loss: 0.3138\n",
      "Epoch [1342/3000], Loss: 0.3151\n",
      "Epoch [1343/3000], Loss: 0.3134\n",
      "Epoch [1344/3000], Loss: 0.3134\n",
      "Epoch [1345/3000], Loss: 0.3151\n",
      "Epoch [1346/3000], Loss: 0.3147\n",
      "Epoch [1347/3000], Loss: 0.3130\n",
      "Epoch [1348/3000], Loss: 0.3136\n",
      "Epoch [1349/3000], Loss: 0.3144\n",
      "Epoch [1350/3000], Loss: 0.3132\n",
      "Epoch [1351/3000], Loss: 0.3126\n",
      "Epoch [1352/3000], Loss: 0.3137\n",
      "Epoch [1353/3000], Loss: 0.3145\n",
      "Epoch [1354/3000], Loss: 0.3143\n",
      "Epoch [1355/3000], Loss: 0.3129\n",
      "Epoch [1356/3000], Loss: 0.3130\n",
      "Epoch [1357/3000], Loss: 0.3138\n",
      "Epoch [1358/3000], Loss: 0.3132\n",
      "Epoch [1359/3000], Loss: 0.3127\n",
      "Epoch [1360/3000], Loss: 0.3123\n",
      "Epoch [1361/3000], Loss: 0.3124\n",
      "Epoch [1362/3000], Loss: 0.3129\n",
      "Epoch [1363/3000], Loss: 0.3133\n",
      "Epoch [1364/3000], Loss: 0.3130\n",
      "Epoch [1365/3000], Loss: 0.3124\n",
      "Epoch [1366/3000], Loss: 0.3120\n",
      "Epoch [1367/3000], Loss: 0.3123\n",
      "Epoch [1368/3000], Loss: 0.3134\n",
      "Epoch [1369/3000], Loss: 0.3160\n",
      "Epoch [1370/3000], Loss: 0.3157\n",
      "Epoch [1371/3000], Loss: 0.3128\n",
      "Epoch [1372/3000], Loss: 0.3140\n",
      "Epoch [1373/3000], Loss: 0.3155\n",
      "Epoch [1374/3000], Loss: 0.3122\n",
      "Epoch [1375/3000], Loss: 0.3153\n",
      "Epoch [1376/3000], Loss: 0.3210\n",
      "Epoch [1377/3000], Loss: 0.3208\n",
      "Epoch [1378/3000], Loss: 0.3143\n",
      "Epoch [1379/3000], Loss: 0.3179\n",
      "Epoch [1380/3000], Loss: 0.3147\n",
      "Epoch [1381/3000], Loss: 0.3173\n",
      "Epoch [1382/3000], Loss: 0.3139\n",
      "Epoch [1383/3000], Loss: 0.3141\n",
      "Epoch [1384/3000], Loss: 0.3170\n",
      "Epoch [1385/3000], Loss: 0.3158\n",
      "Epoch [1386/3000], Loss: 0.3140\n",
      "Epoch [1387/3000], Loss: 0.3129\n",
      "Epoch [1388/3000], Loss: 0.3142\n",
      "Epoch [1389/3000], Loss: 0.3131\n",
      "Epoch [1390/3000], Loss: 0.3123\n",
      "Epoch [1391/3000], Loss: 0.3141\n",
      "Epoch [1392/3000], Loss: 0.3167\n",
      "Epoch [1393/3000], Loss: 0.3148\n",
      "Epoch [1394/3000], Loss: 0.3126\n",
      "Epoch [1395/3000], Loss: 0.3125\n",
      "Epoch [1396/3000], Loss: 0.3130\n",
      "Epoch [1397/3000], Loss: 0.3121\n",
      "Epoch [1398/3000], Loss: 0.3117\n",
      "Epoch [1399/3000], Loss: 0.3126\n",
      "Epoch [1400/3000], Loss: 0.3146\n",
      "Epoch [1401/3000], Loss: 0.3157\n",
      "Epoch [1402/3000], Loss: 0.3136\n",
      "Epoch [1403/3000], Loss: 0.3128\n",
      "Epoch [1404/3000], Loss: 0.3136\n",
      "Epoch [1405/3000], Loss: 0.3123\n",
      "Epoch [1406/3000], Loss: 0.3117\n",
      "Epoch [1407/3000], Loss: 0.3134\n",
      "Epoch [1408/3000], Loss: 0.3178\n",
      "Epoch [1409/3000], Loss: 0.3163\n",
      "Epoch [1410/3000], Loss: 0.3124\n",
      "Epoch [1411/3000], Loss: 0.3149\n",
      "Epoch [1412/3000], Loss: 0.3156\n",
      "Epoch [1413/3000], Loss: 0.3135\n",
      "Epoch [1414/3000], Loss: 0.3163\n",
      "Epoch [1415/3000], Loss: 0.3143\n",
      "Epoch [1416/3000], Loss: 0.3120\n",
      "Epoch [1417/3000], Loss: 0.3122\n",
      "Epoch [1418/3000], Loss: 0.3128\n",
      "Epoch [1419/3000], Loss: 0.3126\n",
      "Epoch [1420/3000], Loss: 0.3117\n",
      "Epoch [1421/3000], Loss: 0.3116\n",
      "Epoch [1422/3000], Loss: 0.3125\n",
      "Epoch [1423/3000], Loss: 0.3145\n",
      "Epoch [1424/3000], Loss: 0.3168\n",
      "Epoch [1425/3000], Loss: 0.3137\n",
      "Epoch [1426/3000], Loss: 0.3136\n",
      "Epoch [1427/3000], Loss: 0.3156\n",
      "Epoch [1428/3000], Loss: 0.3118\n",
      "Epoch [1429/3000], Loss: 0.3121\n",
      "Epoch [1430/3000], Loss: 0.3156\n",
      "Epoch [1431/3000], Loss: 0.3155\n",
      "Epoch [1432/3000], Loss: 0.3137\n",
      "Epoch [1433/3000], Loss: 0.3121\n",
      "Epoch [1434/3000], Loss: 0.3133\n",
      "Epoch [1435/3000], Loss: 0.3123\n",
      "Epoch [1436/3000], Loss: 0.3110\n",
      "Epoch [1437/3000], Loss: 0.3129\n",
      "Epoch [1438/3000], Loss: 0.3187\n",
      "Epoch [1439/3000], Loss: 0.3164\n",
      "Epoch [1440/3000], Loss: 0.3120\n",
      "Epoch [1441/3000], Loss: 0.3141\n",
      "Epoch [1442/3000], Loss: 0.3149\n",
      "Epoch [1443/3000], Loss: 0.3128\n",
      "Epoch [1444/3000], Loss: 0.3151\n",
      "Epoch [1445/3000], Loss: 0.3145\n",
      "Epoch [1446/3000], Loss: 0.3124\n",
      "Epoch [1447/3000], Loss: 0.3112\n",
      "Epoch [1448/3000], Loss: 0.3123\n",
      "Epoch [1449/3000], Loss: 0.3129\n",
      "Epoch [1450/3000], Loss: 0.3116\n",
      "Epoch [1451/3000], Loss: 0.3111\n",
      "Epoch [1452/3000], Loss: 0.3118\n",
      "Epoch [1453/3000], Loss: 0.3129\n",
      "Epoch [1454/3000], Loss: 0.3148\n",
      "Epoch [1455/3000], Loss: 0.3123\n",
      "Epoch [1456/3000], Loss: 0.3111\n",
      "Epoch [1457/3000], Loss: 0.3137\n",
      "Epoch [1458/3000], Loss: 0.3148\n",
      "Epoch [1459/3000], Loss: 0.3133\n",
      "Epoch [1460/3000], Loss: 0.3108\n",
      "Epoch [1461/3000], Loss: 0.3124\n",
      "Epoch [1462/3000], Loss: 0.3125\n",
      "Epoch [1463/3000], Loss: 0.3111\n",
      "Epoch [1464/3000], Loss: 0.3106\n",
      "Epoch [1465/3000], Loss: 0.3112\n",
      "Epoch [1466/3000], Loss: 0.3123\n",
      "Epoch [1467/3000], Loss: 0.3118\n",
      "Epoch [1468/3000], Loss: 0.3110\n",
      "Epoch [1469/3000], Loss: 0.3104\n",
      "Epoch [1470/3000], Loss: 0.3105\n",
      "Epoch [1471/3000], Loss: 0.3116\n",
      "Epoch [1472/3000], Loss: 0.3146\n",
      "Epoch [1473/3000], Loss: 0.3159\n",
      "Epoch [1474/3000], Loss: 0.3115\n",
      "Epoch [1475/3000], Loss: 0.3185\n",
      "Epoch [1476/3000], Loss: 0.3158\n",
      "Epoch [1477/3000], Loss: 0.3122\n",
      "Epoch [1478/3000], Loss: 0.3206\n",
      "Epoch [1479/3000], Loss: 0.3136\n",
      "Epoch [1480/3000], Loss: 0.3113\n",
      "Epoch [1481/3000], Loss: 0.3158\n",
      "Epoch [1482/3000], Loss: 0.3113\n",
      "Epoch [1483/3000], Loss: 0.3124\n",
      "Epoch [1484/3000], Loss: 0.3144\n",
      "Epoch [1485/3000], Loss: 0.3106\n",
      "Epoch [1486/3000], Loss: 0.3113\n",
      "Epoch [1487/3000], Loss: 0.3147\n",
      "Epoch [1488/3000], Loss: 0.3120\n",
      "Epoch [1489/3000], Loss: 0.3112\n",
      "Epoch [1490/3000], Loss: 0.3135\n",
      "Epoch [1491/3000], Loss: 0.3114\n",
      "Epoch [1492/3000], Loss: 0.3100\n",
      "Epoch [1493/3000], Loss: 0.3100\n",
      "Epoch [1494/3000], Loss: 0.3105\n",
      "Epoch [1495/3000], Loss: 0.3108\n",
      "Epoch [1496/3000], Loss: 0.3097\n",
      "Epoch [1497/3000], Loss: 0.3095\n",
      "Epoch [1498/3000], Loss: 0.3095\n",
      "Epoch [1499/3000], Loss: 0.3099\n",
      "Epoch [1500/3000], Loss: 0.3109\n",
      "Epoch [1501/3000], Loss: 0.3117\n",
      "Epoch [1502/3000], Loss: 0.3107\n",
      "Epoch [1503/3000], Loss: 0.3094\n",
      "Epoch [1504/3000], Loss: 0.3097\n",
      "Epoch [1505/3000], Loss: 0.3117\n",
      "Epoch [1506/3000], Loss: 0.3138\n",
      "Epoch [1507/3000], Loss: 0.3162\n",
      "Epoch [1508/3000], Loss: 0.3134\n",
      "Epoch [1509/3000], Loss: 0.3137\n",
      "Epoch [1510/3000], Loss: 0.3143\n",
      "Epoch [1511/3000], Loss: 0.3102\n",
      "Epoch [1512/3000], Loss: 0.3121\n",
      "Epoch [1513/3000], Loss: 0.3166\n",
      "Epoch [1514/3000], Loss: 0.3140\n",
      "Epoch [1515/3000], Loss: 0.3117\n",
      "Epoch [1516/3000], Loss: 0.3112\n",
      "Epoch [1517/3000], Loss: 0.3123\n",
      "Epoch [1518/3000], Loss: 0.3110\n",
      "Epoch [1519/3000], Loss: 0.3097\n",
      "Epoch [1520/3000], Loss: 0.3112\n",
      "Epoch [1521/3000], Loss: 0.3131\n",
      "Epoch [1522/3000], Loss: 0.3143\n",
      "Epoch [1523/3000], Loss: 0.3130\n",
      "Epoch [1524/3000], Loss: 0.3121\n",
      "Epoch [1525/3000], Loss: 0.3130\n",
      "Epoch [1526/3000], Loss: 0.3122\n",
      "Epoch [1527/3000], Loss: 0.3101\n",
      "Epoch [1528/3000], Loss: 0.3103\n",
      "Epoch [1529/3000], Loss: 0.3116\n",
      "Epoch [1530/3000], Loss: 0.3123\n",
      "Epoch [1531/3000], Loss: 0.3132\n",
      "Epoch [1532/3000], Loss: 0.3140\n",
      "Epoch [1533/3000], Loss: 0.3120\n",
      "Epoch [1534/3000], Loss: 0.3163\n",
      "Epoch [1535/3000], Loss: 0.3121\n",
      "Epoch [1536/3000], Loss: 0.3108\n",
      "Epoch [1537/3000], Loss: 0.3168\n",
      "Epoch [1538/3000], Loss: 0.3171\n",
      "Epoch [1539/3000], Loss: 0.3136\n",
      "Epoch [1540/3000], Loss: 0.3116\n",
      "Epoch [1541/3000], Loss: 0.3138\n",
      "Epoch [1542/3000], Loss: 0.3121\n",
      "Epoch [1543/3000], Loss: 0.3116\n",
      "Epoch [1544/3000], Loss: 0.3115\n",
      "Epoch [1545/3000], Loss: 0.3098\n",
      "Epoch [1546/3000], Loss: 0.3124\n",
      "Epoch [1547/3000], Loss: 0.3153\n",
      "Epoch [1548/3000], Loss: 0.3150\n",
      "Epoch [1549/3000], Loss: 0.3133\n",
      "Epoch [1550/3000], Loss: 0.3151\n",
      "Epoch [1551/3000], Loss: 0.3143\n",
      "Epoch [1552/3000], Loss: 0.3119\n",
      "Epoch [1553/3000], Loss: 0.3124\n",
      "Epoch [1554/3000], Loss: 0.3100\n",
      "Epoch [1555/3000], Loss: 0.3119\n",
      "Epoch [1556/3000], Loss: 0.3153\n",
      "Epoch [1557/3000], Loss: 0.3164\n",
      "Epoch [1558/3000], Loss: 0.3133\n",
      "Epoch [1559/3000], Loss: 0.3139\n",
      "Epoch [1560/3000], Loss: 0.3143\n",
      "Epoch [1561/3000], Loss: 0.3109\n",
      "Epoch [1562/3000], Loss: 0.3117\n",
      "Epoch [1563/3000], Loss: 0.3099\n",
      "Epoch [1564/3000], Loss: 0.3102\n",
      "Epoch [1565/3000], Loss: 0.3115\n",
      "Epoch [1566/3000], Loss: 0.3129\n",
      "Epoch [1567/3000], Loss: 0.3125\n",
      "Epoch [1568/3000], Loss: 0.3114\n",
      "Epoch [1569/3000], Loss: 0.3141\n",
      "Epoch [1570/3000], Loss: 0.3102\n",
      "Epoch [1571/3000], Loss: 0.3101\n",
      "Epoch [1572/3000], Loss: 0.3135\n",
      "Epoch [1573/3000], Loss: 0.3124\n",
      "Epoch [1574/3000], Loss: 0.3097\n",
      "Epoch [1575/3000], Loss: 0.3100\n",
      "Epoch [1576/3000], Loss: 0.3111\n",
      "Epoch [1577/3000], Loss: 0.3092\n",
      "Epoch [1578/3000], Loss: 0.3098\n",
      "Epoch [1579/3000], Loss: 0.3136\n",
      "Epoch [1580/3000], Loss: 0.3159\n",
      "Epoch [1581/3000], Loss: 0.3109\n",
      "Epoch [1582/3000], Loss: 0.3118\n",
      "Epoch [1583/3000], Loss: 0.3126\n",
      "Epoch [1584/3000], Loss: 0.3086\n",
      "Epoch [1585/3000], Loss: 0.3107\n",
      "Epoch [1586/3000], Loss: 0.3128\n",
      "Epoch [1587/3000], Loss: 0.3104\n",
      "Epoch [1588/3000], Loss: 0.3085\n",
      "Epoch [1589/3000], Loss: 0.3093\n",
      "Epoch [1590/3000], Loss: 0.3100\n",
      "Epoch [1591/3000], Loss: 0.3085\n",
      "Epoch [1592/3000], Loss: 0.3086\n",
      "Epoch [1593/3000], Loss: 0.3114\n",
      "Epoch [1594/3000], Loss: 0.3139\n",
      "Epoch [1595/3000], Loss: 0.3115\n",
      "Epoch [1596/3000], Loss: 0.3104\n",
      "Epoch [1597/3000], Loss: 0.3135\n",
      "Epoch [1598/3000], Loss: 0.3102\n",
      "Epoch [1599/3000], Loss: 0.3090\n",
      "Epoch [1600/3000], Loss: 0.3127\n",
      "Epoch [1601/3000], Loss: 0.3123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1602/3000], Loss: 0.3092\n",
      "Epoch [1603/3000], Loss: 0.3101\n",
      "Epoch [1604/3000], Loss: 0.3114\n",
      "Epoch [1605/3000], Loss: 0.3090\n",
      "Epoch [1606/3000], Loss: 0.3101\n",
      "Epoch [1607/3000], Loss: 0.3127\n",
      "Epoch [1608/3000], Loss: 0.3125\n",
      "Epoch [1609/3000], Loss: 0.3092\n",
      "Epoch [1610/3000], Loss: 0.3098\n",
      "Epoch [1611/3000], Loss: 0.3111\n",
      "Epoch [1612/3000], Loss: 0.3086\n",
      "Epoch [1613/3000], Loss: 0.3083\n",
      "Epoch [1614/3000], Loss: 0.3102\n",
      "Epoch [1615/3000], Loss: 0.3127\n",
      "Epoch [1616/3000], Loss: 0.3111\n",
      "Epoch [1617/3000], Loss: 0.3095\n",
      "Epoch [1618/3000], Loss: 0.3112\n",
      "Epoch [1619/3000], Loss: 0.3097\n",
      "Epoch [1620/3000], Loss: 0.3079\n",
      "Epoch [1621/3000], Loss: 0.3103\n",
      "Epoch [1622/3000], Loss: 0.3135\n",
      "Epoch [1623/3000], Loss: 0.3108\n",
      "Epoch [1624/3000], Loss: 0.3084\n",
      "Epoch [1625/3000], Loss: 0.3110\n",
      "Epoch [1626/3000], Loss: 0.3105\n",
      "Epoch [1627/3000], Loss: 0.3083\n",
      "Epoch [1628/3000], Loss: 0.3118\n",
      "Epoch [1629/3000], Loss: 0.3141\n",
      "Epoch [1630/3000], Loss: 0.3116\n",
      "Epoch [1631/3000], Loss: 0.3092\n",
      "Epoch [1632/3000], Loss: 0.3114\n",
      "Epoch [1633/3000], Loss: 0.3093\n",
      "Epoch [1634/3000], Loss: 0.3087\n",
      "Epoch [1635/3000], Loss: 0.3111\n",
      "Epoch [1636/3000], Loss: 0.3105\n",
      "Epoch [1637/3000], Loss: 0.3078\n",
      "Epoch [1638/3000], Loss: 0.3078\n",
      "Epoch [1639/3000], Loss: 0.3089\n",
      "Epoch [1640/3000], Loss: 0.3085\n",
      "Epoch [1641/3000], Loss: 0.3073\n",
      "Epoch [1642/3000], Loss: 0.3078\n",
      "Epoch [1643/3000], Loss: 0.3096\n",
      "Epoch [1644/3000], Loss: 0.3116\n",
      "Epoch [1645/3000], Loss: 0.3098\n",
      "Epoch [1646/3000], Loss: 0.3083\n",
      "Epoch [1647/3000], Loss: 0.3109\n",
      "Epoch [1648/3000], Loss: 0.3114\n",
      "Epoch [1649/3000], Loss: 0.3085\n",
      "Epoch [1650/3000], Loss: 0.3074\n",
      "Epoch [1651/3000], Loss: 0.3089\n",
      "Epoch [1652/3000], Loss: 0.3077\n",
      "Epoch [1653/3000], Loss: 0.3072\n",
      "Epoch [1654/3000], Loss: 0.3081\n",
      "Epoch [1655/3000], Loss: 0.3105\n",
      "Epoch [1656/3000], Loss: 0.3106\n",
      "Epoch [1657/3000], Loss: 0.3071\n",
      "Epoch [1658/3000], Loss: 0.3097\n",
      "Epoch [1659/3000], Loss: 0.3121\n",
      "Epoch [1660/3000], Loss: 0.3074\n",
      "Epoch [1661/3000], Loss: 0.3093\n",
      "Epoch [1662/3000], Loss: 0.3144\n",
      "Epoch [1663/3000], Loss: 0.3087\n",
      "Epoch [1664/3000], Loss: 0.3088\n",
      "Epoch [1665/3000], Loss: 0.3115\n",
      "Epoch [1666/3000], Loss: 0.3071\n",
      "Epoch [1667/3000], Loss: 0.3118\n",
      "Epoch [1668/3000], Loss: 0.3199\n",
      "Epoch [1669/3000], Loss: 0.3147\n",
      "Epoch [1670/3000], Loss: 0.3104\n",
      "Epoch [1671/3000], Loss: 0.3138\n",
      "Epoch [1672/3000], Loss: 0.3115\n",
      "Epoch [1673/3000], Loss: 0.3112\n",
      "Epoch [1674/3000], Loss: 0.3107\n",
      "Epoch [1675/3000], Loss: 0.3083\n",
      "Epoch [1676/3000], Loss: 0.3117\n",
      "Epoch [1677/3000], Loss: 0.3148\n",
      "Epoch [1678/3000], Loss: 0.3139\n",
      "Epoch [1679/3000], Loss: 0.3101\n",
      "Epoch [1680/3000], Loss: 0.3117\n",
      "Epoch [1681/3000], Loss: 0.3115\n",
      "Epoch [1682/3000], Loss: 0.3083\n",
      "Epoch [1683/3000], Loss: 0.3096\n",
      "Epoch [1684/3000], Loss: 0.3079\n",
      "Epoch [1685/3000], Loss: 0.3079\n",
      "Epoch [1686/3000], Loss: 0.3087\n",
      "Epoch [1687/3000], Loss: 0.3096\n",
      "Epoch [1688/3000], Loss: 0.3093\n",
      "Epoch [1689/3000], Loss: 0.3086\n",
      "Epoch [1690/3000], Loss: 0.3104\n",
      "Epoch [1691/3000], Loss: 0.3087\n",
      "Epoch [1692/3000], Loss: 0.3072\n",
      "Epoch [1693/3000], Loss: 0.3099\n",
      "Epoch [1694/3000], Loss: 0.3113\n",
      "Epoch [1695/3000], Loss: 0.3092\n",
      "Epoch [1696/3000], Loss: 0.3082\n",
      "Epoch [1697/3000], Loss: 0.3093\n",
      "Epoch [1698/3000], Loss: 0.3087\n",
      "Epoch [1699/3000], Loss: 0.3069\n",
      "Epoch [1700/3000], Loss: 0.3072\n",
      "Epoch [1701/3000], Loss: 0.3093\n",
      "Epoch [1702/3000], Loss: 0.3095\n",
      "Epoch [1703/3000], Loss: 0.3087\n",
      "Epoch [1704/3000], Loss: 0.3079\n",
      "Epoch [1705/3000], Loss: 0.3079\n",
      "Epoch [1706/3000], Loss: 0.3081\n",
      "Epoch [1707/3000], Loss: 0.3079\n",
      "Epoch [1708/3000], Loss: 0.3064\n",
      "Epoch [1709/3000], Loss: 0.3069\n",
      "Epoch [1710/3000], Loss: 0.3082\n",
      "Epoch [1711/3000], Loss: 0.3096\n",
      "Epoch [1712/3000], Loss: 0.3095\n",
      "Epoch [1713/3000], Loss: 0.3080\n",
      "Epoch [1714/3000], Loss: 0.3103\n",
      "Epoch [1715/3000], Loss: 0.3096\n",
      "Epoch [1716/3000], Loss: 0.3061\n",
      "Epoch [1717/3000], Loss: 0.3111\n",
      "Epoch [1718/3000], Loss: 0.3168\n",
      "Epoch [1719/3000], Loss: 0.3112\n",
      "Epoch [1720/3000], Loss: 0.3097\n",
      "Epoch [1721/3000], Loss: 0.3128\n",
      "Epoch [1722/3000], Loss: 0.3105\n",
      "Epoch [1723/3000], Loss: 0.3112\n",
      "Epoch [1724/3000], Loss: 0.3100\n",
      "Epoch [1725/3000], Loss: 0.3079\n",
      "Epoch [1726/3000], Loss: 0.3107\n",
      "Epoch [1727/3000], Loss: 0.3143\n",
      "Epoch [1728/3000], Loss: 0.3131\n",
      "Epoch [1729/3000], Loss: 0.3108\n",
      "Epoch [1730/3000], Loss: 0.3102\n",
      "Epoch [1731/3000], Loss: 0.3134\n",
      "Epoch [1732/3000], Loss: 0.3084\n",
      "Epoch [1733/3000], Loss: 0.3095\n",
      "Epoch [1734/3000], Loss: 0.3088\n",
      "Epoch [1735/3000], Loss: 0.3076\n",
      "Epoch [1736/3000], Loss: 0.3068\n",
      "Epoch [1737/3000], Loss: 0.3062\n",
      "Epoch [1738/3000], Loss: 0.3071\n",
      "Epoch [1739/3000], Loss: 0.3069\n",
      "Epoch [1740/3000], Loss: 0.3062\n",
      "Epoch [1741/3000], Loss: 0.3062\n",
      "Epoch [1742/3000], Loss: 0.3067\n",
      "Epoch [1743/3000], Loss: 0.3080\n",
      "Epoch [1744/3000], Loss: 0.3091\n",
      "Epoch [1745/3000], Loss: 0.3078\n",
      "Epoch [1746/3000], Loss: 0.3066\n",
      "Epoch [1747/3000], Loss: 0.3091\n",
      "Epoch [1748/3000], Loss: 0.3092\n",
      "Epoch [1749/3000], Loss: 0.3064\n",
      "Epoch [1750/3000], Loss: 0.3066\n",
      "Epoch [1751/3000], Loss: 0.3107\n",
      "Epoch [1752/3000], Loss: 0.3076\n",
      "Epoch [1753/3000], Loss: 0.3063\n",
      "Epoch [1754/3000], Loss: 0.3074\n",
      "Epoch [1755/3000], Loss: 0.3055\n",
      "Epoch [1756/3000], Loss: 0.3063\n",
      "Epoch [1757/3000], Loss: 0.3091\n",
      "Epoch [1758/3000], Loss: 0.3130\n",
      "Epoch [1759/3000], Loss: 0.3116\n",
      "Epoch [1760/3000], Loss: 0.3098\n",
      "Epoch [1761/3000], Loss: 0.3136\n",
      "Epoch [1762/3000], Loss: 0.3080\n",
      "Epoch [1763/3000], Loss: 0.3089\n",
      "Epoch [1764/3000], Loss: 0.3131\n",
      "Epoch [1765/3000], Loss: 0.3092\n",
      "Epoch [1766/3000], Loss: 0.3066\n",
      "Epoch [1767/3000], Loss: 0.3089\n",
      "Epoch [1768/3000], Loss: 0.3079\n",
      "Epoch [1769/3000], Loss: 0.3070\n",
      "Epoch [1770/3000], Loss: 0.3076\n",
      "Epoch [1771/3000], Loss: 0.3085\n",
      "Epoch [1772/3000], Loss: 0.3085\n",
      "Epoch [1773/3000], Loss: 0.3075\n",
      "Epoch [1774/3000], Loss: 0.3068\n",
      "Epoch [1775/3000], Loss: 0.3080\n",
      "Epoch [1776/3000], Loss: 0.3072\n",
      "Epoch [1777/3000], Loss: 0.3061\n",
      "Epoch [1778/3000], Loss: 0.3057\n",
      "Epoch [1779/3000], Loss: 0.3075\n",
      "Epoch [1780/3000], Loss: 0.3076\n",
      "Epoch [1781/3000], Loss: 0.3067\n",
      "Epoch [1782/3000], Loss: 0.3065\n",
      "Epoch [1783/3000], Loss: 0.3061\n",
      "Epoch [1784/3000], Loss: 0.3076\n",
      "Epoch [1785/3000], Loss: 0.3079\n",
      "Epoch [1786/3000], Loss: 0.3073\n",
      "Epoch [1787/3000], Loss: 0.3065\n",
      "Epoch [1788/3000], Loss: 0.3060\n",
      "Epoch [1789/3000], Loss: 0.3078\n",
      "Epoch [1790/3000], Loss: 0.3075\n",
      "Epoch [1791/3000], Loss: 0.3052\n",
      "Epoch [1792/3000], Loss: 0.3061\n",
      "Epoch [1793/3000], Loss: 0.3058\n",
      "Epoch [1794/3000], Loss: 0.3060\n",
      "Epoch [1795/3000], Loss: 0.3068\n",
      "Epoch [1796/3000], Loss: 0.3049\n",
      "Epoch [1797/3000], Loss: 0.3057\n",
      "Epoch [1798/3000], Loss: 0.3048\n",
      "Epoch [1799/3000], Loss: 0.3049\n",
      "Epoch [1800/3000], Loss: 0.3055\n",
      "Epoch [1801/3000], Loss: 0.3051\n",
      "Epoch [1802/3000], Loss: 0.3072\n",
      "Epoch [1803/3000], Loss: 0.3079\n",
      "Epoch [1804/3000], Loss: 0.3066\n",
      "Epoch [1805/3000], Loss: 0.3048\n",
      "Epoch [1806/3000], Loss: 0.3047\n",
      "Epoch [1807/3000], Loss: 0.3066\n",
      "Epoch [1808/3000], Loss: 0.3098\n",
      "Epoch [1809/3000], Loss: 0.3127\n",
      "Epoch [1810/3000], Loss: 0.3110\n",
      "Epoch [1811/3000], Loss: 0.3090\n",
      "Epoch [1812/3000], Loss: 0.3143\n",
      "Epoch [1813/3000], Loss: 0.3096\n",
      "Epoch [1814/3000], Loss: 0.3089\n",
      "Epoch [1815/3000], Loss: 0.3119\n",
      "Epoch [1816/3000], Loss: 0.3124\n",
      "Epoch [1817/3000], Loss: 0.3088\n",
      "Epoch [1818/3000], Loss: 0.3074\n",
      "Epoch [1819/3000], Loss: 0.3092\n",
      "Epoch [1820/3000], Loss: 0.3082\n",
      "Epoch [1821/3000], Loss: 0.3061\n",
      "Epoch [1822/3000], Loss: 0.3067\n",
      "Epoch [1823/3000], Loss: 0.3059\n",
      "Epoch [1824/3000], Loss: 0.3061\n",
      "Epoch [1825/3000], Loss: 0.3048\n",
      "Epoch [1826/3000], Loss: 0.3051\n",
      "Epoch [1827/3000], Loss: 0.3057\n",
      "Epoch [1828/3000], Loss: 0.3050\n",
      "Epoch [1829/3000], Loss: 0.3047\n",
      "Epoch [1830/3000], Loss: 0.3062\n",
      "Epoch [1831/3000], Loss: 0.3099\n",
      "Epoch [1832/3000], Loss: 0.3124\n",
      "Epoch [1833/3000], Loss: 0.3059\n",
      "Epoch [1834/3000], Loss: 0.3129\n",
      "Epoch [1835/3000], Loss: 0.3144\n",
      "Epoch [1836/3000], Loss: 0.3113\n",
      "Epoch [1837/3000], Loss: 0.3172\n",
      "Epoch [1838/3000], Loss: 0.3062\n",
      "Epoch [1839/3000], Loss: 0.3096\n",
      "Epoch [1840/3000], Loss: 0.3120\n",
      "Epoch [1841/3000], Loss: 0.3072\n",
      "Epoch [1842/3000], Loss: 0.3101\n",
      "Epoch [1843/3000], Loss: 0.3075\n",
      "Epoch [1844/3000], Loss: 0.3071\n",
      "Epoch [1845/3000], Loss: 0.3093\n",
      "Epoch [1846/3000], Loss: 0.3090\n",
      "Epoch [1847/3000], Loss: 0.3061\n",
      "Epoch [1848/3000], Loss: 0.3067\n",
      "Epoch [1849/3000], Loss: 0.3069\n",
      "Epoch [1850/3000], Loss: 0.3053\n",
      "Epoch [1851/3000], Loss: 0.3056\n",
      "Epoch [1852/3000], Loss: 0.3070\n",
      "Epoch [1853/3000], Loss: 0.3055\n",
      "Epoch [1854/3000], Loss: 0.3045\n",
      "Epoch [1855/3000], Loss: 0.3058\n",
      "Epoch [1856/3000], Loss: 0.3053\n",
      "Epoch [1857/3000], Loss: 0.3043\n",
      "Epoch [1858/3000], Loss: 0.3044\n",
      "Epoch [1859/3000], Loss: 0.3044\n",
      "Epoch [1860/3000], Loss: 0.3062\n",
      "Epoch [1861/3000], Loss: 0.3075\n",
      "Epoch [1862/3000], Loss: 0.3043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1863/3000], Loss: 0.3076\n",
      "Epoch [1864/3000], Loss: 0.3131\n",
      "Epoch [1865/3000], Loss: 0.3075\n",
      "Epoch [1866/3000], Loss: 0.3066\n",
      "Epoch [1867/3000], Loss: 0.3090\n",
      "Epoch [1868/3000], Loss: 0.3064\n",
      "Epoch [1869/3000], Loss: 0.3073\n",
      "Epoch [1870/3000], Loss: 0.3087\n",
      "Epoch [1871/3000], Loss: 0.3059\n",
      "Epoch [1872/3000], Loss: 0.3064\n",
      "Epoch [1873/3000], Loss: 0.3098\n",
      "Epoch [1874/3000], Loss: 0.3084\n",
      "Epoch [1875/3000], Loss: 0.3062\n",
      "Epoch [1876/3000], Loss: 0.3075\n",
      "Epoch [1877/3000], Loss: 0.3065\n",
      "Epoch [1878/3000], Loss: 0.3044\n",
      "Epoch [1879/3000], Loss: 0.3064\n",
      "Epoch [1880/3000], Loss: 0.3095\n",
      "Epoch [1881/3000], Loss: 0.3096\n",
      "Epoch [1882/3000], Loss: 0.3051\n",
      "Epoch [1883/3000], Loss: 0.3106\n",
      "Epoch [1884/3000], Loss: 0.3112\n",
      "Epoch [1885/3000], Loss: 0.3070\n",
      "Epoch [1886/3000], Loss: 0.3150\n",
      "Epoch [1887/3000], Loss: 0.3116\n",
      "Epoch [1888/3000], Loss: 0.3051\n",
      "Epoch [1889/3000], Loss: 0.3095\n",
      "Epoch [1890/3000], Loss: 0.3087\n",
      "Epoch [1891/3000], Loss: 0.3071\n",
      "Epoch [1892/3000], Loss: 0.3093\n",
      "Epoch [1893/3000], Loss: 0.3058\n",
      "Epoch [1894/3000], Loss: 0.3052\n",
      "Epoch [1895/3000], Loss: 0.3076\n",
      "Epoch [1896/3000], Loss: 0.3070\n",
      "Epoch [1897/3000], Loss: 0.3048\n",
      "Epoch [1898/3000], Loss: 0.3059\n",
      "Epoch [1899/3000], Loss: 0.3067\n",
      "Epoch [1900/3000], Loss: 0.3045\n",
      "Epoch [1901/3000], Loss: 0.3055\n",
      "Epoch [1902/3000], Loss: 0.3091\n",
      "Epoch [1903/3000], Loss: 0.3075\n",
      "Epoch [1904/3000], Loss: 0.3059\n",
      "Epoch [1905/3000], Loss: 0.3072\n",
      "Epoch [1906/3000], Loss: 0.3060\n",
      "Epoch [1907/3000], Loss: 0.3044\n",
      "Epoch [1908/3000], Loss: 0.3050\n",
      "Epoch [1909/3000], Loss: 0.3051\n",
      "Epoch [1910/3000], Loss: 0.3051\n",
      "Epoch [1911/3000], Loss: 0.3036\n",
      "Epoch [1912/3000], Loss: 0.3045\n",
      "Epoch [1913/3000], Loss: 0.3045\n",
      "Epoch [1914/3000], Loss: 0.3031\n",
      "Epoch [1915/3000], Loss: 0.3034\n",
      "Epoch [1916/3000], Loss: 0.3038\n",
      "Epoch [1917/3000], Loss: 0.3052\n",
      "Epoch [1918/3000], Loss: 0.3090\n",
      "Epoch [1919/3000], Loss: 0.3042\n",
      "Epoch [1920/3000], Loss: 0.3064\n",
      "Epoch [1921/3000], Loss: 0.3101\n",
      "Epoch [1922/3000], Loss: 0.3035\n",
      "Epoch [1923/3000], Loss: 0.3084\n",
      "Epoch [1924/3000], Loss: 0.3167\n",
      "Epoch [1925/3000], Loss: 0.3050\n",
      "Epoch [1926/3000], Loss: 0.3160\n",
      "Epoch [1927/3000], Loss: 0.3171\n",
      "Epoch [1928/3000], Loss: 0.3188\n",
      "Epoch [1929/3000], Loss: 0.3093\n",
      "Epoch [1930/3000], Loss: 0.3174\n",
      "Epoch [1931/3000], Loss: 0.3109\n",
      "Epoch [1932/3000], Loss: 0.3147\n",
      "Epoch [1933/3000], Loss: 0.3133\n",
      "Epoch [1934/3000], Loss: 0.3072\n",
      "Epoch [1935/3000], Loss: 0.3133\n",
      "Epoch [1936/3000], Loss: 0.3082\n",
      "Epoch [1937/3000], Loss: 0.3118\n",
      "Epoch [1938/3000], Loss: 0.3078\n",
      "Epoch [1939/3000], Loss: 0.3087\n",
      "Epoch [1940/3000], Loss: 0.3070\n",
      "Epoch [1941/3000], Loss: 0.3057\n",
      "Epoch [1942/3000], Loss: 0.3071\n",
      "Epoch [1943/3000], Loss: 0.3069\n",
      "Epoch [1944/3000], Loss: 0.3062\n",
      "Epoch [1945/3000], Loss: 0.3063\n",
      "Epoch [1946/3000], Loss: 0.3056\n",
      "Epoch [1947/3000], Loss: 0.3043\n",
      "Epoch [1948/3000], Loss: 0.3051\n",
      "Epoch [1949/3000], Loss: 0.3054\n",
      "Epoch [1950/3000], Loss: 0.3048\n",
      "Epoch [1951/3000], Loss: 0.3038\n",
      "Epoch [1952/3000], Loss: 0.3039\n",
      "Epoch [1953/3000], Loss: 0.3036\n",
      "Epoch [1954/3000], Loss: 0.3030\n",
      "Epoch [1955/3000], Loss: 0.3034\n",
      "Epoch [1956/3000], Loss: 0.3028\n",
      "Epoch [1957/3000], Loss: 0.3026\n",
      "Epoch [1958/3000], Loss: 0.3027\n",
      "Epoch [1959/3000], Loss: 0.3028\n",
      "Epoch [1960/3000], Loss: 0.3024\n",
      "Epoch [1961/3000], Loss: 0.3025\n",
      "Epoch [1962/3000], Loss: 0.3025\n",
      "Epoch [1963/3000], Loss: 0.3028\n",
      "Epoch [1964/3000], Loss: 0.3035\n",
      "Epoch [1965/3000], Loss: 0.3032\n",
      "Epoch [1966/3000], Loss: 0.3026\n",
      "Epoch [1967/3000], Loss: 0.3019\n",
      "Epoch [1968/3000], Loss: 0.3018\n",
      "Epoch [1969/3000], Loss: 0.3018\n",
      "Epoch [1970/3000], Loss: 0.3018\n",
      "Epoch [1971/3000], Loss: 0.3026\n",
      "Epoch [1972/3000], Loss: 0.3048\n",
      "Epoch [1973/3000], Loss: 0.3056\n",
      "Epoch [1974/3000], Loss: 0.3062\n",
      "Epoch [1975/3000], Loss: 0.3023\n",
      "Epoch [1976/3000], Loss: 0.3061\n",
      "Epoch [1977/3000], Loss: 0.3174\n",
      "Epoch [1978/3000], Loss: 0.3076\n",
      "Epoch [1979/3000], Loss: 0.3079\n",
      "Epoch [1980/3000], Loss: 0.3123\n",
      "Epoch [1981/3000], Loss: 0.3096\n",
      "Epoch [1982/3000], Loss: 0.3131\n",
      "Epoch [1983/3000], Loss: 0.3078\n",
      "Epoch [1984/3000], Loss: 0.3082\n",
      "Epoch [1985/3000], Loss: 0.3076\n",
      "Epoch [1986/3000], Loss: 0.3064\n",
      "Epoch [1987/3000], Loss: 0.3050\n",
      "Epoch [1988/3000], Loss: 0.3044\n",
      "Epoch [1989/3000], Loss: 0.3054\n",
      "Epoch [1990/3000], Loss: 0.3056\n",
      "Epoch [1991/3000], Loss: 0.3041\n",
      "Epoch [1992/3000], Loss: 0.3038\n",
      "Epoch [1993/3000], Loss: 0.3031\n",
      "Epoch [1994/3000], Loss: 0.3045\n",
      "Epoch [1995/3000], Loss: 0.3035\n",
      "Epoch [1996/3000], Loss: 0.3042\n",
      "Epoch [1997/3000], Loss: 0.3039\n",
      "Epoch [1998/3000], Loss: 0.3033\n",
      "Epoch [1999/3000], Loss: 0.3028\n",
      "Epoch [2000/3000], Loss: 0.3025\n",
      "Epoch [2001/3000], Loss: 0.3041\n",
      "Epoch [2002/3000], Loss: 0.3059\n",
      "Epoch [2003/3000], Loss: 0.3068\n",
      "Epoch [2004/3000], Loss: 0.3067\n",
      "Epoch [2005/3000], Loss: 0.3048\n",
      "Epoch [2006/3000], Loss: 0.3078\n",
      "Epoch [2007/3000], Loss: 0.3067\n",
      "Epoch [2008/3000], Loss: 0.3035\n",
      "Epoch [2009/3000], Loss: 0.3095\n",
      "Epoch [2010/3000], Loss: 0.3176\n",
      "Epoch [2011/3000], Loss: 0.3067\n",
      "Epoch [2012/3000], Loss: 0.3070\n",
      "Epoch [2013/3000], Loss: 0.3100\n",
      "Epoch [2014/3000], Loss: 0.3078\n",
      "Epoch [2015/3000], Loss: 0.3097\n",
      "Epoch [2016/3000], Loss: 0.3090\n",
      "Epoch [2017/3000], Loss: 0.3062\n",
      "Epoch [2018/3000], Loss: 0.3086\n",
      "Epoch [2019/3000], Loss: 0.3081\n",
      "Epoch [2020/3000], Loss: 0.3045\n",
      "Epoch [2021/3000], Loss: 0.3036\n",
      "Epoch [2022/3000], Loss: 0.3040\n",
      "Epoch [2023/3000], Loss: 0.3040\n",
      "Epoch [2024/3000], Loss: 0.3034\n",
      "Epoch [2025/3000], Loss: 0.3032\n",
      "Epoch [2026/3000], Loss: 0.3033\n",
      "Epoch [2027/3000], Loss: 0.3031\n",
      "Epoch [2028/3000], Loss: 0.3028\n",
      "Epoch [2029/3000], Loss: 0.3025\n",
      "Epoch [2030/3000], Loss: 0.3020\n",
      "Epoch [2031/3000], Loss: 0.3021\n",
      "Epoch [2032/3000], Loss: 0.3030\n",
      "Epoch [2033/3000], Loss: 0.3034\n",
      "Epoch [2034/3000], Loss: 0.3030\n",
      "Epoch [2035/3000], Loss: 0.3026\n",
      "Epoch [2036/3000], Loss: 0.3017\n",
      "Epoch [2037/3000], Loss: 0.3012\n",
      "Epoch [2038/3000], Loss: 0.3016\n",
      "Epoch [2039/3000], Loss: 0.3017\n",
      "Epoch [2040/3000], Loss: 0.3028\n",
      "Epoch [2041/3000], Loss: 0.3043\n",
      "Epoch [2042/3000], Loss: 0.3018\n",
      "Epoch [2043/3000], Loss: 0.3015\n",
      "Epoch [2044/3000], Loss: 0.3051\n",
      "Epoch [2045/3000], Loss: 0.3073\n",
      "Epoch [2046/3000], Loss: 0.3059\n",
      "Epoch [2047/3000], Loss: 0.3023\n",
      "Epoch [2048/3000], Loss: 0.3084\n",
      "Epoch [2049/3000], Loss: 0.3132\n",
      "Epoch [2050/3000], Loss: 0.3033\n",
      "Epoch [2051/3000], Loss: 0.3148\n",
      "Epoch [2052/3000], Loss: 0.3142\n",
      "Epoch [2053/3000], Loss: 0.3155\n",
      "Epoch [2054/3000], Loss: 0.3099\n",
      "Epoch [2055/3000], Loss: 0.3070\n",
      "Epoch [2056/3000], Loss: 0.3098\n",
      "Epoch [2057/3000], Loss: 0.3081\n",
      "Epoch [2058/3000], Loss: 0.3079\n",
      "Epoch [2059/3000], Loss: 0.3063\n",
      "Epoch [2060/3000], Loss: 0.3076\n",
      "Epoch [2061/3000], Loss: 0.3061\n",
      "Epoch [2062/3000], Loss: 0.3046\n",
      "Epoch [2063/3000], Loss: 0.3057\n",
      "Epoch [2064/3000], Loss: 0.3040\n",
      "Epoch [2065/3000], Loss: 0.3042\n",
      "Epoch [2066/3000], Loss: 0.3045\n",
      "Epoch [2067/3000], Loss: 0.3035\n",
      "Epoch [2068/3000], Loss: 0.3032\n",
      "Epoch [2069/3000], Loss: 0.3032\n",
      "Epoch [2070/3000], Loss: 0.3026\n",
      "Epoch [2071/3000], Loss: 0.3021\n",
      "Epoch [2072/3000], Loss: 0.3025\n",
      "Epoch [2073/3000], Loss: 0.3025\n",
      "Epoch [2074/3000], Loss: 0.3018\n",
      "Epoch [2075/3000], Loss: 0.3016\n",
      "Epoch [2076/3000], Loss: 0.3013\n",
      "Epoch [2077/3000], Loss: 0.3010\n",
      "Epoch [2078/3000], Loss: 0.3017\n",
      "Epoch [2079/3000], Loss: 0.3025\n",
      "Epoch [2080/3000], Loss: 0.3015\n",
      "Epoch [2081/3000], Loss: 0.3008\n",
      "Epoch [2082/3000], Loss: 0.3005\n",
      "Epoch [2083/3000], Loss: 0.3009\n",
      "Epoch [2084/3000], Loss: 0.3020\n",
      "Epoch [2085/3000], Loss: 0.3046\n",
      "Epoch [2086/3000], Loss: 0.3060\n",
      "Epoch [2087/3000], Loss: 0.3017\n",
      "Epoch [2088/3000], Loss: 0.3059\n",
      "Epoch [2089/3000], Loss: 0.3120\n",
      "Epoch [2090/3000], Loss: 0.3039\n",
      "Epoch [2091/3000], Loss: 0.3063\n",
      "Epoch [2092/3000], Loss: 0.3091\n",
      "Epoch [2093/3000], Loss: 0.3059\n",
      "Epoch [2094/3000], Loss: 0.3091\n",
      "Epoch [2095/3000], Loss: 0.3056\n",
      "Epoch [2096/3000], Loss: 0.3052\n",
      "Epoch [2097/3000], Loss: 0.3114\n",
      "Epoch [2098/3000], Loss: 0.3057\n",
      "Epoch [2099/3000], Loss: 0.3035\n",
      "Epoch [2100/3000], Loss: 0.3067\n",
      "Epoch [2101/3000], Loss: 0.3040\n",
      "Epoch [2102/3000], Loss: 0.3042\n",
      "Epoch [2103/3000], Loss: 0.3043\n",
      "Epoch [2104/3000], Loss: 0.3028\n",
      "Epoch [2105/3000], Loss: 0.3016\n",
      "Epoch [2106/3000], Loss: 0.3021\n",
      "Epoch [2107/3000], Loss: 0.3022\n",
      "Epoch [2108/3000], Loss: 0.3014\n",
      "Epoch [2109/3000], Loss: 0.3012\n",
      "Epoch [2110/3000], Loss: 0.3019\n",
      "Epoch [2111/3000], Loss: 0.3029\n",
      "Epoch [2112/3000], Loss: 0.3029\n",
      "Epoch [2113/3000], Loss: 0.3007\n",
      "Epoch [2114/3000], Loss: 0.3027\n",
      "Epoch [2115/3000], Loss: 0.3074\n",
      "Epoch [2116/3000], Loss: 0.3048\n",
      "Epoch [2117/3000], Loss: 0.3023\n",
      "Epoch [2118/3000], Loss: 0.3027\n",
      "Epoch [2119/3000], Loss: 0.3028\n",
      "Epoch [2120/3000], Loss: 0.3022\n",
      "Epoch [2121/3000], Loss: 0.3011\n",
      "Epoch [2122/3000], Loss: 0.3031\n",
      "Epoch [2123/3000], Loss: 0.3042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2124/3000], Loss: 0.3036\n",
      "Epoch [2125/3000], Loss: 0.3032\n",
      "Epoch [2126/3000], Loss: 0.3031\n",
      "Epoch [2127/3000], Loss: 0.3037\n",
      "Epoch [2128/3000], Loss: 0.3026\n",
      "Epoch [2129/3000], Loss: 0.3009\n",
      "Epoch [2130/3000], Loss: 0.3021\n",
      "Epoch [2131/3000], Loss: 0.3039\n",
      "Epoch [2132/3000], Loss: 0.3056\n",
      "Epoch [2133/3000], Loss: 0.3070\n",
      "Epoch [2134/3000], Loss: 0.3045\n",
      "Epoch [2135/3000], Loss: 0.3105\n",
      "Epoch [2136/3000], Loss: 0.3080\n",
      "Epoch [2137/3000], Loss: 0.3048\n",
      "Epoch [2138/3000], Loss: 0.3109\n",
      "Epoch [2139/3000], Loss: 0.3116\n",
      "Epoch [2140/3000], Loss: 0.3028\n",
      "Epoch [2141/3000], Loss: 0.3038\n",
      "Epoch [2142/3000], Loss: 0.3052\n",
      "Epoch [2143/3000], Loss: 0.3026\n",
      "Epoch [2144/3000], Loss: 0.3053\n",
      "Epoch [2145/3000], Loss: 0.3042\n",
      "Epoch [2146/3000], Loss: 0.3013\n",
      "Epoch [2147/3000], Loss: 0.3065\n",
      "Epoch [2148/3000], Loss: 0.3091\n",
      "Epoch [2149/3000], Loss: 0.3021\n",
      "Epoch [2150/3000], Loss: 0.3069\n",
      "Epoch [2151/3000], Loss: 0.3084\n",
      "Epoch [2152/3000], Loss: 0.3049\n",
      "Epoch [2153/3000], Loss: 0.3110\n",
      "Epoch [2154/3000], Loss: 0.3047\n",
      "Epoch [2155/3000], Loss: 0.3052\n",
      "Epoch [2156/3000], Loss: 0.3094\n",
      "Epoch [2157/3000], Loss: 0.3029\n",
      "Epoch [2158/3000], Loss: 0.3036\n",
      "Epoch [2159/3000], Loss: 0.3043\n",
      "Epoch [2160/3000], Loss: 0.3011\n",
      "Epoch [2161/3000], Loss: 0.3019\n",
      "Epoch [2162/3000], Loss: 0.3021\n",
      "Epoch [2163/3000], Loss: 0.3004\n",
      "Epoch [2164/3000], Loss: 0.3004\n",
      "Epoch [2165/3000], Loss: 0.3009\n",
      "Epoch [2166/3000], Loss: 0.3002\n",
      "Epoch [2167/3000], Loss: 0.3002\n",
      "Epoch [2168/3000], Loss: 0.3004\n",
      "Epoch [2169/3000], Loss: 0.3012\n",
      "Epoch [2170/3000], Loss: 0.3012\n",
      "Epoch [2171/3000], Loss: 0.3001\n",
      "Epoch [2172/3000], Loss: 0.2996\n",
      "Epoch [2173/3000], Loss: 0.3004\n",
      "Epoch [2174/3000], Loss: 0.3017\n",
      "Epoch [2175/3000], Loss: 0.3038\n",
      "Epoch [2176/3000], Loss: 0.3049\n",
      "Epoch [2177/3000], Loss: 0.3007\n",
      "Epoch [2178/3000], Loss: 0.3055\n",
      "Epoch [2179/3000], Loss: 0.3085\n",
      "Epoch [2180/3000], Loss: 0.3008\n",
      "Epoch [2181/3000], Loss: 0.3089\n",
      "Epoch [2182/3000], Loss: 0.3107\n",
      "Epoch [2183/3000], Loss: 0.3055\n",
      "Epoch [2184/3000], Loss: 0.3150\n",
      "Epoch [2185/3000], Loss: 0.3040\n",
      "Epoch [2186/3000], Loss: 0.3053\n",
      "Epoch [2187/3000], Loss: 0.3058\n",
      "Epoch [2188/3000], Loss: 0.3013\n",
      "Epoch [2189/3000], Loss: 0.3045\n",
      "Epoch [2190/3000], Loss: 0.3005\n",
      "Epoch [2191/3000], Loss: 0.3042\n",
      "Epoch [2192/3000], Loss: 0.3072\n",
      "Epoch [2193/3000], Loss: 0.3017\n",
      "Epoch [2194/3000], Loss: 0.3046\n",
      "Epoch [2195/3000], Loss: 0.3065\n",
      "Epoch [2196/3000], Loss: 0.3043\n",
      "Epoch [2197/3000], Loss: 0.3067\n",
      "Epoch [2198/3000], Loss: 0.3035\n",
      "Epoch [2199/3000], Loss: 0.3019\n",
      "Epoch [2200/3000], Loss: 0.3040\n",
      "Epoch [2201/3000], Loss: 0.3049\n",
      "Epoch [2202/3000], Loss: 0.3007\n",
      "Epoch [2203/3000], Loss: 0.3012\n",
      "Epoch [2204/3000], Loss: 0.3010\n",
      "Epoch [2205/3000], Loss: 0.3017\n",
      "Epoch [2206/3000], Loss: 0.2999\n",
      "Epoch [2207/3000], Loss: 0.3002\n",
      "Epoch [2208/3000], Loss: 0.3016\n",
      "Epoch [2209/3000], Loss: 0.3006\n",
      "Epoch [2210/3000], Loss: 0.3003\n",
      "Epoch [2211/3000], Loss: 0.3002\n",
      "Epoch [2212/3000], Loss: 0.3002\n",
      "Epoch [2213/3000], Loss: 0.2990\n",
      "Epoch [2214/3000], Loss: 0.2992\n",
      "Epoch [2215/3000], Loss: 0.2996\n",
      "Epoch [2216/3000], Loss: 0.2992\n",
      "Epoch [2217/3000], Loss: 0.2990\n",
      "Epoch [2218/3000], Loss: 0.2991\n",
      "Epoch [2219/3000], Loss: 0.2986\n",
      "Epoch [2220/3000], Loss: 0.2984\n",
      "Epoch [2221/3000], Loss: 0.2987\n",
      "Epoch [2222/3000], Loss: 0.2980\n",
      "Epoch [2223/3000], Loss: 0.2982\n",
      "Epoch [2224/3000], Loss: 0.2983\n",
      "Epoch [2225/3000], Loss: 0.2982\n",
      "Epoch [2226/3000], Loss: 0.2987\n",
      "Epoch [2227/3000], Loss: 0.3008\n",
      "Epoch [2228/3000], Loss: 0.3046\n",
      "Epoch [2229/3000], Loss: 0.3088\n",
      "Epoch [2230/3000], Loss: 0.3019\n",
      "Epoch [2231/3000], Loss: 0.3158\n",
      "Epoch [2232/3000], Loss: 0.3089\n",
      "Epoch [2233/3000], Loss: 0.3055\n",
      "Epoch [2234/3000], Loss: 0.3125\n",
      "Epoch [2235/3000], Loss: 0.3018\n",
      "Epoch [2236/3000], Loss: 0.3071\n",
      "Epoch [2237/3000], Loss: 0.3038\n",
      "Epoch [2238/3000], Loss: 0.3033\n",
      "Epoch [2239/3000], Loss: 0.3035\n",
      "Epoch [2240/3000], Loss: 0.3003\n",
      "Epoch [2241/3000], Loss: 0.3042\n",
      "Epoch [2242/3000], Loss: 0.3033\n",
      "Epoch [2243/3000], Loss: 0.3021\n",
      "Epoch [2244/3000], Loss: 0.3018\n",
      "Epoch [2245/3000], Loss: 0.3032\n",
      "Epoch [2246/3000], Loss: 0.3021\n",
      "Epoch [2247/3000], Loss: 0.3010\n",
      "Epoch [2248/3000], Loss: 0.3004\n",
      "Epoch [2249/3000], Loss: 0.3003\n",
      "Epoch [2250/3000], Loss: 0.3008\n",
      "Epoch [2251/3000], Loss: 0.2992\n",
      "Epoch [2252/3000], Loss: 0.2999\n",
      "Epoch [2253/3000], Loss: 0.2994\n",
      "Epoch [2254/3000], Loss: 0.2994\n",
      "Epoch [2255/3000], Loss: 0.2985\n",
      "Epoch [2256/3000], Loss: 0.2994\n",
      "Epoch [2257/3000], Loss: 0.2984\n",
      "Epoch [2258/3000], Loss: 0.2991\n",
      "Epoch [2259/3000], Loss: 0.2990\n",
      "Epoch [2260/3000], Loss: 0.2986\n",
      "Epoch [2261/3000], Loss: 0.2978\n",
      "Epoch [2262/3000], Loss: 0.2986\n",
      "Epoch [2263/3000], Loss: 0.2999\n",
      "Epoch [2264/3000], Loss: 0.3016\n",
      "Epoch [2265/3000], Loss: 0.3024\n",
      "Epoch [2266/3000], Loss: 0.3002\n",
      "Epoch [2267/3000], Loss: 0.2992\n",
      "Epoch [2268/3000], Loss: 0.3036\n",
      "Epoch [2269/3000], Loss: 0.3055\n",
      "Epoch [2270/3000], Loss: 0.3053\n",
      "Epoch [2271/3000], Loss: 0.3032\n",
      "Epoch [2272/3000], Loss: 0.3045\n",
      "Epoch [2273/3000], Loss: 0.3035\n",
      "Epoch [2274/3000], Loss: 0.3024\n",
      "Epoch [2275/3000], Loss: 0.3061\n",
      "Epoch [2276/3000], Loss: 0.3053\n",
      "Epoch [2277/3000], Loss: 0.3010\n",
      "Epoch [2278/3000], Loss: 0.3028\n",
      "Epoch [2279/3000], Loss: 0.3031\n",
      "Epoch [2280/3000], Loss: 0.3014\n",
      "Epoch [2281/3000], Loss: 0.3010\n",
      "Epoch [2282/3000], Loss: 0.3005\n",
      "Epoch [2283/3000], Loss: 0.3013\n",
      "Epoch [2284/3000], Loss: 0.2994\n",
      "Epoch [2285/3000], Loss: 0.3000\n",
      "Epoch [2286/3000], Loss: 0.2991\n",
      "Epoch [2287/3000], Loss: 0.3003\n",
      "Epoch [2288/3000], Loss: 0.2997\n",
      "Epoch [2289/3000], Loss: 0.2990\n",
      "Epoch [2290/3000], Loss: 0.2988\n",
      "Epoch [2291/3000], Loss: 0.2990\n",
      "Epoch [2292/3000], Loss: 0.2984\n",
      "Epoch [2293/3000], Loss: 0.2989\n",
      "Epoch [2294/3000], Loss: 0.2980\n",
      "Epoch [2295/3000], Loss: 0.2982\n",
      "Epoch [2296/3000], Loss: 0.2978\n",
      "Epoch [2297/3000], Loss: 0.2981\n",
      "Epoch [2298/3000], Loss: 0.2992\n",
      "Epoch [2299/3000], Loss: 0.3006\n",
      "Epoch [2300/3000], Loss: 0.2982\n",
      "Epoch [2301/3000], Loss: 0.2977\n",
      "Epoch [2302/3000], Loss: 0.2988\n",
      "Epoch [2303/3000], Loss: 0.3025\n",
      "Epoch [2304/3000], Loss: 0.3060\n",
      "Epoch [2305/3000], Loss: 0.2992\n",
      "Epoch [2306/3000], Loss: 0.3056\n",
      "Epoch [2307/3000], Loss: 0.3125\n",
      "Epoch [2308/3000], Loss: 0.3016\n",
      "Epoch [2309/3000], Loss: 0.3157\n",
      "Epoch [2310/3000], Loss: 0.3135\n",
      "Epoch [2311/3000], Loss: 0.3150\n",
      "Epoch [2312/3000], Loss: 0.3099\n",
      "Epoch [2313/3000], Loss: 0.3085\n",
      "Epoch [2314/3000], Loss: 0.3088\n",
      "Epoch [2315/3000], Loss: 0.3095\n",
      "Epoch [2316/3000], Loss: 0.3089\n",
      "Epoch [2317/3000], Loss: 0.3035\n",
      "Epoch [2318/3000], Loss: 0.3066\n",
      "Epoch [2319/3000], Loss: 0.3046\n",
      "Epoch [2320/3000], Loss: 0.3061\n",
      "Epoch [2321/3000], Loss: 0.3029\n",
      "Epoch [2322/3000], Loss: 0.3032\n",
      "Epoch [2323/3000], Loss: 0.3029\n",
      "Epoch [2324/3000], Loss: 0.3019\n",
      "Epoch [2325/3000], Loss: 0.3009\n",
      "Epoch [2326/3000], Loss: 0.3007\n",
      "Epoch [2327/3000], Loss: 0.3009\n",
      "Epoch [2328/3000], Loss: 0.2997\n",
      "Epoch [2329/3000], Loss: 0.2995\n",
      "Epoch [2330/3000], Loss: 0.3005\n",
      "Epoch [2331/3000], Loss: 0.3006\n",
      "Epoch [2332/3000], Loss: 0.3005\n",
      "Epoch [2333/3000], Loss: 0.2993\n",
      "Epoch [2334/3000], Loss: 0.3001\n",
      "Epoch [2335/3000], Loss: 0.3017\n",
      "Epoch [2336/3000], Loss: 0.2993\n",
      "Epoch [2337/3000], Loss: 0.2979\n",
      "Epoch [2338/3000], Loss: 0.2994\n",
      "Epoch [2339/3000], Loss: 0.2992\n",
      "Epoch [2340/3000], Loss: 0.2984\n",
      "Epoch [2341/3000], Loss: 0.2980\n",
      "Epoch [2342/3000], Loss: 0.2975\n",
      "Epoch [2343/3000], Loss: 0.2976\n",
      "Epoch [2344/3000], Loss: 0.2983\n",
      "Epoch [2345/3000], Loss: 0.2992\n",
      "Epoch [2346/3000], Loss: 0.2987\n",
      "Epoch [2347/3000], Loss: 0.2977\n",
      "Epoch [2348/3000], Loss: 0.2970\n",
      "Epoch [2349/3000], Loss: 0.2970\n",
      "Epoch [2350/3000], Loss: 0.2970\n",
      "Epoch [2351/3000], Loss: 0.2987\n",
      "Epoch [2352/3000], Loss: 0.3012\n",
      "Epoch [2353/3000], Loss: 0.2985\n",
      "Epoch [2354/3000], Loss: 0.2970\n",
      "Epoch [2355/3000], Loss: 0.2980\n",
      "Epoch [2356/3000], Loss: 0.3001\n",
      "Epoch [2357/3000], Loss: 0.3040\n",
      "Epoch [2358/3000], Loss: 0.3007\n",
      "Epoch [2359/3000], Loss: 0.2997\n",
      "Epoch [2360/3000], Loss: 0.3014\n",
      "Epoch [2361/3000], Loss: 0.3001\n",
      "Epoch [2362/3000], Loss: 0.2994\n",
      "Epoch [2363/3000], Loss: 0.2977\n",
      "Epoch [2364/3000], Loss: 0.2989\n",
      "Epoch [2365/3000], Loss: 0.2990\n",
      "Epoch [2366/3000], Loss: 0.3015\n",
      "Epoch [2367/3000], Loss: 0.3034\n",
      "Epoch [2368/3000], Loss: 0.3008\n",
      "Epoch [2369/3000], Loss: 0.3000\n",
      "Epoch [2370/3000], Loss: 0.3031\n",
      "Epoch [2371/3000], Loss: 0.2993\n",
      "Epoch [2372/3000], Loss: 0.3009\n",
      "Epoch [2373/3000], Loss: 0.3033\n",
      "Epoch [2374/3000], Loss: 0.3001\n",
      "Epoch [2375/3000], Loss: 0.3012\n",
      "Epoch [2376/3000], Loss: 0.3027\n",
      "Epoch [2377/3000], Loss: 0.3013\n",
      "Epoch [2378/3000], Loss: 0.2997\n",
      "Epoch [2379/3000], Loss: 0.2988\n",
      "Epoch [2380/3000], Loss: 0.2994\n",
      "Epoch [2381/3000], Loss: 0.3005\n",
      "Epoch [2382/3000], Loss: 0.3003\n",
      "Epoch [2383/3000], Loss: 0.3024\n",
      "Epoch [2384/3000], Loss: 0.2999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2385/3000], Loss: 0.3013\n",
      "Epoch [2386/3000], Loss: 0.3047\n",
      "Epoch [2387/3000], Loss: 0.2977\n",
      "Epoch [2388/3000], Loss: 0.3031\n",
      "Epoch [2389/3000], Loss: 0.3148\n",
      "Epoch [2390/3000], Loss: 0.3010\n",
      "Epoch [2391/3000], Loss: 0.3106\n",
      "Epoch [2392/3000], Loss: 0.3158\n",
      "Epoch [2393/3000], Loss: 0.3165\n",
      "Epoch [2394/3000], Loss: 0.3093\n",
      "Epoch [2395/3000], Loss: 0.3113\n",
      "Epoch [2396/3000], Loss: 0.3095\n",
      "Epoch [2397/3000], Loss: 0.3102\n",
      "Epoch [2398/3000], Loss: 0.3061\n",
      "Epoch [2399/3000], Loss: 0.3057\n",
      "Epoch [2400/3000], Loss: 0.3081\n",
      "Epoch [2401/3000], Loss: 0.3031\n",
      "Epoch [2402/3000], Loss: 0.3030\n",
      "Epoch [2403/3000], Loss: 0.3034\n",
      "Epoch [2404/3000], Loss: 0.3033\n",
      "Epoch [2405/3000], Loss: 0.3034\n",
      "Epoch [2406/3000], Loss: 0.3005\n",
      "Epoch [2407/3000], Loss: 0.2993\n",
      "Epoch [2408/3000], Loss: 0.3015\n",
      "Epoch [2409/3000], Loss: 0.3007\n",
      "Epoch [2410/3000], Loss: 0.2985\n",
      "Epoch [2411/3000], Loss: 0.2991\n",
      "Epoch [2412/3000], Loss: 0.2998\n",
      "Epoch [2413/3000], Loss: 0.2993\n",
      "Epoch [2414/3000], Loss: 0.2973\n",
      "Epoch [2415/3000], Loss: 0.2983\n",
      "Epoch [2416/3000], Loss: 0.2990\n",
      "Epoch [2417/3000], Loss: 0.3002\n",
      "Epoch [2418/3000], Loss: 0.2985\n",
      "Epoch [2419/3000], Loss: 0.2977\n",
      "Epoch [2420/3000], Loss: 0.2976\n",
      "Epoch [2421/3000], Loss: 0.2975\n",
      "Epoch [2422/3000], Loss: 0.2973\n",
      "Epoch [2423/3000], Loss: 0.2979\n",
      "Epoch [2424/3000], Loss: 0.2980\n",
      "Epoch [2425/3000], Loss: 0.2976\n",
      "Epoch [2426/3000], Loss: 0.2968\n",
      "Epoch [2427/3000], Loss: 0.2966\n",
      "Epoch [2428/3000], Loss: 0.2963\n",
      "Epoch [2429/3000], Loss: 0.2960\n",
      "Epoch [2430/3000], Loss: 0.2965\n",
      "Epoch [2431/3000], Loss: 0.2969\n",
      "Epoch [2432/3000], Loss: 0.2971\n",
      "Epoch [2433/3000], Loss: 0.2975\n",
      "Epoch [2434/3000], Loss: 0.2989\n",
      "Epoch [2435/3000], Loss: 0.3006\n",
      "Epoch [2436/3000], Loss: 0.2988\n",
      "Epoch [2437/3000], Loss: 0.2968\n",
      "Epoch [2438/3000], Loss: 0.2971\n",
      "Epoch [2439/3000], Loss: 0.2972\n",
      "Epoch [2440/3000], Loss: 0.2979\n",
      "Epoch [2441/3000], Loss: 0.2970\n",
      "Epoch [2442/3000], Loss: 0.2970\n",
      "Epoch [2443/3000], Loss: 0.2965\n",
      "Epoch [2444/3000], Loss: 0.2963\n",
      "Epoch [2445/3000], Loss: 0.2965\n",
      "Epoch [2446/3000], Loss: 0.2967\n",
      "Epoch [2447/3000], Loss: 0.2979\n",
      "Epoch [2448/3000], Loss: 0.2984\n",
      "Epoch [2449/3000], Loss: 0.3018\n",
      "Epoch [2450/3000], Loss: 0.3024\n",
      "Epoch [2451/3000], Loss: 0.2990\n",
      "Epoch [2452/3000], Loss: 0.2986\n",
      "Epoch [2453/3000], Loss: 0.3048\n",
      "Epoch [2454/3000], Loss: 0.3040\n",
      "Epoch [2455/3000], Loss: 0.3001\n",
      "Epoch [2456/3000], Loss: 0.3022\n",
      "Epoch [2457/3000], Loss: 0.2993\n",
      "Epoch [2458/3000], Loss: 0.2992\n",
      "Epoch [2459/3000], Loss: 0.2988\n",
      "Epoch [2460/3000], Loss: 0.2983\n",
      "Epoch [2461/3000], Loss: 0.2984\n",
      "Epoch [2462/3000], Loss: 0.2970\n",
      "Epoch [2463/3000], Loss: 0.2979\n",
      "Epoch [2464/3000], Loss: 0.2974\n",
      "Epoch [2465/3000], Loss: 0.2973\n",
      "Epoch [2466/3000], Loss: 0.2968\n",
      "Epoch [2467/3000], Loss: 0.2970\n",
      "Epoch [2468/3000], Loss: 0.2977\n",
      "Epoch [2469/3000], Loss: 0.2986\n",
      "Epoch [2470/3000], Loss: 0.2976\n",
      "Epoch [2471/3000], Loss: 0.2962\n",
      "Epoch [2472/3000], Loss: 0.2959\n",
      "Epoch [2473/3000], Loss: 0.2958\n",
      "Epoch [2474/3000], Loss: 0.2956\n",
      "Epoch [2475/3000], Loss: 0.2962\n",
      "Epoch [2476/3000], Loss: 0.2971\n",
      "Epoch [2477/3000], Loss: 0.3000\n",
      "Epoch [2478/3000], Loss: 0.3030\n",
      "Epoch [2479/3000], Loss: 0.3028\n",
      "Epoch [2480/3000], Loss: 0.2994\n",
      "Epoch [2481/3000], Loss: 0.3013\n",
      "Epoch [2482/3000], Loss: 0.3057\n",
      "Epoch [2483/3000], Loss: 0.3023\n",
      "Epoch [2484/3000], Loss: 0.3041\n",
      "Epoch [2485/3000], Loss: 0.3069\n",
      "Epoch [2486/3000], Loss: 0.3050\n",
      "Epoch [2487/3000], Loss: 0.3010\n",
      "Epoch [2488/3000], Loss: 0.3035\n",
      "Epoch [2489/3000], Loss: 0.3035\n",
      "Epoch [2490/3000], Loss: 0.2998\n",
      "Epoch [2491/3000], Loss: 0.3003\n",
      "Epoch [2492/3000], Loss: 0.2990\n",
      "Epoch [2493/3000], Loss: 0.2997\n",
      "Epoch [2494/3000], Loss: 0.2995\n",
      "Epoch [2495/3000], Loss: 0.2986\n",
      "Epoch [2496/3000], Loss: 0.2998\n",
      "Epoch [2497/3000], Loss: 0.2974\n",
      "Epoch [2498/3000], Loss: 0.2981\n",
      "Epoch [2499/3000], Loss: 0.2975\n",
      "Epoch [2500/3000], Loss: 0.2969\n",
      "Epoch [2501/3000], Loss: 0.2983\n",
      "Epoch [2502/3000], Loss: 0.2990\n",
      "Epoch [2503/3000], Loss: 0.2973\n",
      "Epoch [2504/3000], Loss: 0.2966\n",
      "Epoch [2505/3000], Loss: 0.2962\n",
      "Epoch [2506/3000], Loss: 0.2972\n",
      "Epoch [2507/3000], Loss: 0.2978\n",
      "Epoch [2508/3000], Loss: 0.2981\n",
      "Epoch [2509/3000], Loss: 0.2976\n",
      "Epoch [2510/3000], Loss: 0.2983\n",
      "Epoch [2511/3000], Loss: 0.2981\n",
      "Epoch [2512/3000], Loss: 0.2970\n",
      "Epoch [2513/3000], Loss: 0.2959\n",
      "Epoch [2514/3000], Loss: 0.2960\n",
      "Epoch [2515/3000], Loss: 0.2964\n",
      "Epoch [2516/3000], Loss: 0.2976\n",
      "Epoch [2517/3000], Loss: 0.2975\n",
      "Epoch [2518/3000], Loss: 0.2972\n",
      "Epoch [2519/3000], Loss: 0.2980\n",
      "Epoch [2520/3000], Loss: 0.2983\n",
      "Epoch [2521/3000], Loss: 0.2982\n",
      "Epoch [2522/3000], Loss: 0.2961\n",
      "Epoch [2523/3000], Loss: 0.2965\n",
      "Epoch [2524/3000], Loss: 0.2958\n",
      "Epoch [2525/3000], Loss: 0.2959\n",
      "Epoch [2526/3000], Loss: 0.2956\n",
      "Epoch [2527/3000], Loss: 0.2953\n",
      "Epoch [2528/3000], Loss: 0.2956\n",
      "Epoch [2529/3000], Loss: 0.2949\n",
      "Epoch [2530/3000], Loss: 0.2954\n",
      "Epoch [2531/3000], Loss: 0.2953\n",
      "Epoch [2532/3000], Loss: 0.2971\n",
      "Epoch [2533/3000], Loss: 0.2987\n",
      "Epoch [2534/3000], Loss: 0.3016\n",
      "Epoch [2535/3000], Loss: 0.2975\n",
      "Epoch [2536/3000], Loss: 0.2982\n",
      "Epoch [2537/3000], Loss: 0.3026\n",
      "Epoch [2538/3000], Loss: 0.3013\n",
      "Epoch [2539/3000], Loss: 0.3000\n",
      "Epoch [2540/3000], Loss: 0.2973\n",
      "Epoch [2541/3000], Loss: 0.3000\n",
      "Epoch [2542/3000], Loss: 0.3040\n",
      "Epoch [2543/3000], Loss: 0.2976\n",
      "Epoch [2544/3000], Loss: 0.3023\n",
      "Epoch [2545/3000], Loss: 0.3134\n",
      "Epoch [2546/3000], Loss: 0.2999\n",
      "Epoch [2547/3000], Loss: 0.3138\n",
      "Epoch [2548/3000], Loss: 0.3162\n",
      "Epoch [2549/3000], Loss: 0.3200\n",
      "Epoch [2550/3000], Loss: 0.3152\n",
      "Epoch [2551/3000], Loss: 0.3132\n",
      "Epoch [2552/3000], Loss: 0.3092\n",
      "Epoch [2553/3000], Loss: 0.3100\n",
      "Epoch [2554/3000], Loss: 0.3075\n",
      "Epoch [2555/3000], Loss: 0.3045\n",
      "Epoch [2556/3000], Loss: 0.3045\n",
      "Epoch [2557/3000], Loss: 0.3061\n",
      "Epoch [2558/3000], Loss: 0.3003\n",
      "Epoch [2559/3000], Loss: 0.3027\n",
      "Epoch [2560/3000], Loss: 0.3037\n",
      "Epoch [2561/3000], Loss: 0.3009\n",
      "Epoch [2562/3000], Loss: 0.3016\n",
      "Epoch [2563/3000], Loss: 0.3000\n",
      "Epoch [2564/3000], Loss: 0.2996\n",
      "Epoch [2565/3000], Loss: 0.3000\n",
      "Epoch [2566/3000], Loss: 0.2987\n",
      "Epoch [2567/3000], Loss: 0.2985\n",
      "Epoch [2568/3000], Loss: 0.2978\n",
      "Epoch [2569/3000], Loss: 0.2977\n",
      "Epoch [2570/3000], Loss: 0.2977\n",
      "Epoch [2571/3000], Loss: 0.2969\n",
      "Epoch [2572/3000], Loss: 0.2968\n",
      "Epoch [2573/3000], Loss: 0.2973\n",
      "Epoch [2574/3000], Loss: 0.2970\n",
      "Epoch [2575/3000], Loss: 0.2957\n",
      "Epoch [2576/3000], Loss: 0.2959\n",
      "Epoch [2577/3000], Loss: 0.2960\n",
      "Epoch [2578/3000], Loss: 0.2954\n",
      "Epoch [2579/3000], Loss: 0.2960\n",
      "Epoch [2580/3000], Loss: 0.2954\n",
      "Epoch [2581/3000], Loss: 0.2956\n",
      "Epoch [2582/3000], Loss: 0.2959\n",
      "Epoch [2583/3000], Loss: 0.2963\n",
      "Epoch [2584/3000], Loss: 0.2972\n",
      "Epoch [2585/3000], Loss: 0.2964\n",
      "Epoch [2586/3000], Loss: 0.2953\n",
      "Epoch [2587/3000], Loss: 0.2951\n",
      "Epoch [2588/3000], Loss: 0.2958\n",
      "Epoch [2589/3000], Loss: 0.2962\n",
      "Epoch [2590/3000], Loss: 0.2977\n",
      "Epoch [2591/3000], Loss: 0.2982\n",
      "Epoch [2592/3000], Loss: 0.2990\n",
      "Epoch [2593/3000], Loss: 0.2964\n",
      "Epoch [2594/3000], Loss: 0.2955\n",
      "Epoch [2595/3000], Loss: 0.3006\n",
      "Epoch [2596/3000], Loss: 0.3045\n",
      "Epoch [2597/3000], Loss: 0.3056\n",
      "Epoch [2598/3000], Loss: 0.3004\n",
      "Epoch [2599/3000], Loss: 0.3082\n",
      "Epoch [2600/3000], Loss: 0.3047\n",
      "Epoch [2601/3000], Loss: 0.3024\n",
      "Epoch [2602/3000], Loss: 0.3090\n",
      "Epoch [2603/3000], Loss: 0.3004\n",
      "Epoch [2604/3000], Loss: 0.3041\n",
      "Epoch [2605/3000], Loss: 0.3018\n",
      "Epoch [2606/3000], Loss: 0.2973\n",
      "Epoch [2607/3000], Loss: 0.3016\n",
      "Epoch [2608/3000], Loss: 0.2996\n",
      "Epoch [2609/3000], Loss: 0.2973\n",
      "Epoch [2610/3000], Loss: 0.2994\n",
      "Epoch [2611/3000], Loss: 0.2977\n",
      "Epoch [2612/3000], Loss: 0.2978\n",
      "Epoch [2613/3000], Loss: 0.2987\n",
      "Epoch [2614/3000], Loss: 0.2961\n",
      "Epoch [2615/3000], Loss: 0.2969\n",
      "Epoch [2616/3000], Loss: 0.2978\n",
      "Epoch [2617/3000], Loss: 0.2967\n",
      "Epoch [2618/3000], Loss: 0.2962\n",
      "Epoch [2619/3000], Loss: 0.2955\n",
      "Epoch [2620/3000], Loss: 0.2974\n",
      "Epoch [2621/3000], Loss: 0.2981\n",
      "Epoch [2622/3000], Loss: 0.2961\n",
      "Epoch [2623/3000], Loss: 0.2959\n",
      "Epoch [2624/3000], Loss: 0.2955\n",
      "Epoch [2625/3000], Loss: 0.2955\n",
      "Epoch [2626/3000], Loss: 0.2966\n",
      "Epoch [2627/3000], Loss: 0.2966\n",
      "Epoch [2628/3000], Loss: 0.2965\n",
      "Epoch [2629/3000], Loss: 0.2970\n",
      "Epoch [2630/3000], Loss: 0.2966\n",
      "Epoch [2631/3000], Loss: 0.2952\n",
      "Epoch [2632/3000], Loss: 0.2955\n",
      "Epoch [2633/3000], Loss: 0.2995\n",
      "Epoch [2634/3000], Loss: 0.3040\n",
      "Epoch [2635/3000], Loss: 0.3056\n",
      "Epoch [2636/3000], Loss: 0.2990\n",
      "Epoch [2637/3000], Loss: 0.3134\n",
      "Epoch [2638/3000], Loss: 0.3046\n",
      "Epoch [2639/3000], Loss: 0.3032\n",
      "Epoch [2640/3000], Loss: 0.3072\n",
      "Epoch [2641/3000], Loss: 0.2979\n",
      "Epoch [2642/3000], Loss: 0.3033\n",
      "Epoch [2643/3000], Loss: 0.2980\n",
      "Epoch [2644/3000], Loss: 0.3005\n",
      "Epoch [2645/3000], Loss: 0.2987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2646/3000], Loss: 0.2970\n",
      "Epoch [2647/3000], Loss: 0.2991\n",
      "Epoch [2648/3000], Loss: 0.2973\n",
      "Epoch [2649/3000], Loss: 0.2982\n",
      "Epoch [2650/3000], Loss: 0.2963\n",
      "Epoch [2651/3000], Loss: 0.2981\n",
      "Epoch [2652/3000], Loss: 0.2960\n",
      "Epoch [2653/3000], Loss: 0.2968\n",
      "Epoch [2654/3000], Loss: 0.2975\n",
      "Epoch [2655/3000], Loss: 0.3000\n",
      "Epoch [2656/3000], Loss: 0.2992\n",
      "Epoch [2657/3000], Loss: 0.2955\n",
      "Epoch [2658/3000], Loss: 0.3001\n",
      "Epoch [2659/3000], Loss: 0.3032\n",
      "Epoch [2660/3000], Loss: 0.2959\n",
      "Epoch [2661/3000], Loss: 0.2997\n",
      "Epoch [2662/3000], Loss: 0.3036\n",
      "Epoch [2663/3000], Loss: 0.2980\n",
      "Epoch [2664/3000], Loss: 0.3080\n",
      "Epoch [2665/3000], Loss: 0.3048\n",
      "Epoch [2666/3000], Loss: 0.2983\n",
      "Epoch [2667/3000], Loss: 0.3048\n",
      "Epoch [2668/3000], Loss: 0.2990\n",
      "Epoch [2669/3000], Loss: 0.3009\n",
      "Epoch [2670/3000], Loss: 0.2987\n",
      "Epoch [2671/3000], Loss: 0.2970\n",
      "Epoch [2672/3000], Loss: 0.2987\n",
      "Epoch [2673/3000], Loss: 0.2964\n",
      "Epoch [2674/3000], Loss: 0.2965\n",
      "Epoch [2675/3000], Loss: 0.2974\n",
      "Epoch [2676/3000], Loss: 0.2957\n",
      "Epoch [2677/3000], Loss: 0.2958\n",
      "Epoch [2678/3000], Loss: 0.2971\n",
      "Epoch [2679/3000], Loss: 0.2965\n",
      "Epoch [2680/3000], Loss: 0.2959\n",
      "Epoch [2681/3000], Loss: 0.2949\n",
      "Epoch [2682/3000], Loss: 0.2960\n",
      "Epoch [2683/3000], Loss: 0.2969\n",
      "Epoch [2684/3000], Loss: 0.2956\n",
      "Epoch [2685/3000], Loss: 0.2942\n",
      "Epoch [2686/3000], Loss: 0.2950\n",
      "Epoch [2687/3000], Loss: 0.2947\n",
      "Epoch [2688/3000], Loss: 0.2941\n",
      "Epoch [2689/3000], Loss: 0.2940\n",
      "Epoch [2690/3000], Loss: 0.2940\n",
      "Epoch [2691/3000], Loss: 0.2936\n",
      "Epoch [2692/3000], Loss: 0.2945\n",
      "Epoch [2693/3000], Loss: 0.2935\n",
      "Epoch [2694/3000], Loss: 0.2939\n",
      "Epoch [2695/3000], Loss: 0.2947\n",
      "Epoch [2696/3000], Loss: 0.2946\n",
      "Epoch [2697/3000], Loss: 0.2959\n",
      "Epoch [2698/3000], Loss: 0.2962\n",
      "Epoch [2699/3000], Loss: 0.2968\n",
      "Epoch [2700/3000], Loss: 0.2938\n",
      "Epoch [2701/3000], Loss: 0.2961\n",
      "Epoch [2702/3000], Loss: 0.3037\n",
      "Epoch [2703/3000], Loss: 0.2993\n",
      "Epoch [2704/3000], Loss: 0.2954\n",
      "Epoch [2705/3000], Loss: 0.2975\n",
      "Epoch [2706/3000], Loss: 0.2972\n",
      "Epoch [2707/3000], Loss: 0.2966\n",
      "Epoch [2708/3000], Loss: 0.2960\n",
      "Epoch [2709/3000], Loss: 0.2950\n",
      "Epoch [2710/3000], Loss: 0.2951\n",
      "Epoch [2711/3000], Loss: 0.2948\n",
      "Epoch [2712/3000], Loss: 0.2961\n",
      "Epoch [2713/3000], Loss: 0.2947\n",
      "Epoch [2714/3000], Loss: 0.2951\n",
      "Epoch [2715/3000], Loss: 0.2945\n",
      "Epoch [2716/3000], Loss: 0.2949\n",
      "Epoch [2717/3000], Loss: 0.2957\n",
      "Epoch [2718/3000], Loss: 0.2960\n",
      "Epoch [2719/3000], Loss: 0.2937\n",
      "Epoch [2720/3000], Loss: 0.2946\n",
      "Epoch [2721/3000], Loss: 0.2940\n",
      "Epoch [2722/3000], Loss: 0.2941\n",
      "Epoch [2723/3000], Loss: 0.2935\n",
      "Epoch [2724/3000], Loss: 0.2939\n",
      "Epoch [2725/3000], Loss: 0.2928\n",
      "Epoch [2726/3000], Loss: 0.2928\n",
      "Epoch [2727/3000], Loss: 0.2929\n",
      "Epoch [2728/3000], Loss: 0.2928\n",
      "Epoch [2729/3000], Loss: 0.2943\n",
      "Epoch [2730/3000], Loss: 0.2952\n",
      "Epoch [2731/3000], Loss: 0.2991\n",
      "Epoch [2732/3000], Loss: 0.2957\n",
      "Epoch [2733/3000], Loss: 0.2933\n",
      "Epoch [2734/3000], Loss: 0.2932\n",
      "Epoch [2735/3000], Loss: 0.2970\n",
      "Epoch [2736/3000], Loss: 0.3010\n",
      "Epoch [2737/3000], Loss: 0.2935\n",
      "Epoch [2738/3000], Loss: 0.2942\n",
      "Epoch [2739/3000], Loss: 0.3043\n",
      "Epoch [2740/3000], Loss: 0.2964\n",
      "Epoch [2741/3000], Loss: 0.2963\n",
      "Epoch [2742/3000], Loss: 0.3028\n",
      "Epoch [2743/3000], Loss: 0.2946\n",
      "Epoch [2744/3000], Loss: 0.3023\n",
      "Epoch [2745/3000], Loss: 0.3131\n",
      "Epoch [2746/3000], Loss: 0.2988\n",
      "Epoch [2747/3000], Loss: 0.3187\n",
      "Epoch [2748/3000], Loss: 0.3081\n",
      "Epoch [2749/3000], Loss: 0.3122\n",
      "Epoch [2750/3000], Loss: 0.3066\n",
      "Epoch [2751/3000], Loss: 0.3052\n",
      "Epoch [2752/3000], Loss: 0.3025\n",
      "Epoch [2753/3000], Loss: 0.3054\n",
      "Epoch [2754/3000], Loss: 0.3032\n",
      "Epoch [2755/3000], Loss: 0.2998\n",
      "Epoch [2756/3000], Loss: 0.3008\n",
      "Epoch [2757/3000], Loss: 0.3001\n",
      "Epoch [2758/3000], Loss: 0.3000\n",
      "Epoch [2759/3000], Loss: 0.2985\n",
      "Epoch [2760/3000], Loss: 0.2980\n",
      "Epoch [2761/3000], Loss: 0.2970\n",
      "Epoch [2762/3000], Loss: 0.2971\n",
      "Epoch [2763/3000], Loss: 0.2971\n",
      "Epoch [2764/3000], Loss: 0.2958\n",
      "Epoch [2765/3000], Loss: 0.2959\n",
      "Epoch [2766/3000], Loss: 0.2955\n",
      "Epoch [2767/3000], Loss: 0.2942\n",
      "Epoch [2768/3000], Loss: 0.2949\n",
      "Epoch [2769/3000], Loss: 0.2936\n",
      "Epoch [2770/3000], Loss: 0.2941\n",
      "Epoch [2771/3000], Loss: 0.2932\n",
      "Epoch [2772/3000], Loss: 0.2938\n",
      "Epoch [2773/3000], Loss: 0.2935\n",
      "Epoch [2774/3000], Loss: 0.2938\n",
      "Epoch [2775/3000], Loss: 0.2944\n",
      "Epoch [2776/3000], Loss: 0.2939\n",
      "Epoch [2777/3000], Loss: 0.2934\n",
      "Epoch [2778/3000], Loss: 0.2927\n",
      "Epoch [2779/3000], Loss: 0.2924\n",
      "Epoch [2780/3000], Loss: 0.2920\n",
      "Epoch [2781/3000], Loss: 0.2919\n",
      "Epoch [2782/3000], Loss: 0.2921\n",
      "Epoch [2783/3000], Loss: 0.2927\n",
      "Epoch [2784/3000], Loss: 0.2954\n",
      "Epoch [2785/3000], Loss: 0.2944\n",
      "Epoch [2786/3000], Loss: 0.2945\n",
      "Epoch [2787/3000], Loss: 0.2936\n",
      "Epoch [2788/3000], Loss: 0.2944\n",
      "Epoch [2789/3000], Loss: 0.2920\n",
      "Epoch [2790/3000], Loss: 0.2938\n",
      "Epoch [2791/3000], Loss: 0.2953\n",
      "Epoch [2792/3000], Loss: 0.2964\n",
      "Epoch [2793/3000], Loss: 0.2943\n",
      "Epoch [2794/3000], Loss: 0.2940\n",
      "Epoch [2795/3000], Loss: 0.2927\n",
      "Epoch [2796/3000], Loss: 0.2931\n",
      "Epoch [2797/3000], Loss: 0.2927\n",
      "Epoch [2798/3000], Loss: 0.2919\n",
      "Epoch [2799/3000], Loss: 0.2923\n",
      "Epoch [2800/3000], Loss: 0.2926\n",
      "Epoch [2801/3000], Loss: 0.2935\n",
      "Epoch [2802/3000], Loss: 0.2961\n",
      "Epoch [2803/3000], Loss: 0.2994\n",
      "Epoch [2804/3000], Loss: 0.2936\n",
      "Epoch [2805/3000], Loss: 0.2967\n",
      "Epoch [2806/3000], Loss: 0.3096\n",
      "Epoch [2807/3000], Loss: 0.2980\n",
      "Epoch [2808/3000], Loss: 0.3010\n",
      "Epoch [2809/3000], Loss: 0.3100\n",
      "Epoch [2810/3000], Loss: 0.3066\n",
      "Epoch [2811/3000], Loss: 0.3128\n",
      "Epoch [2812/3000], Loss: 0.2987\n",
      "Epoch [2813/3000], Loss: 0.3008\n",
      "Epoch [2814/3000], Loss: 0.3011\n",
      "Epoch [2815/3000], Loss: 0.2982\n",
      "Epoch [2816/3000], Loss: 0.2978\n",
      "Epoch [2817/3000], Loss: 0.2990\n",
      "Epoch [2818/3000], Loss: 0.2988\n",
      "Epoch [2819/3000], Loss: 0.2964\n",
      "Epoch [2820/3000], Loss: 0.2986\n",
      "Epoch [2821/3000], Loss: 0.2983\n",
      "Epoch [2822/3000], Loss: 0.2947\n",
      "Epoch [2823/3000], Loss: 0.2970\n",
      "Epoch [2824/3000], Loss: 0.2996\n",
      "Epoch [2825/3000], Loss: 0.2962\n",
      "Epoch [2826/3000], Loss: 0.2954\n",
      "Epoch [2827/3000], Loss: 0.2941\n",
      "Epoch [2828/3000], Loss: 0.2955\n",
      "Epoch [2829/3000], Loss: 0.2966\n",
      "Epoch [2830/3000], Loss: 0.2929\n",
      "Epoch [2831/3000], Loss: 0.2930\n",
      "Epoch [2832/3000], Loss: 0.2956\n",
      "Epoch [2833/3000], Loss: 0.2960\n",
      "Epoch [2834/3000], Loss: 0.2946\n",
      "Epoch [2835/3000], Loss: 0.2923\n",
      "Epoch [2836/3000], Loss: 0.2942\n",
      "Epoch [2837/3000], Loss: 0.3001\n",
      "Epoch [2838/3000], Loss: 0.2958\n",
      "Epoch [2839/3000], Loss: 0.2931\n",
      "Epoch [2840/3000], Loss: 0.2943\n",
      "Epoch [2841/3000], Loss: 0.2952\n",
      "Epoch [2842/3000], Loss: 0.2949\n",
      "Epoch [2843/3000], Loss: 0.2934\n",
      "Epoch [2844/3000], Loss: 0.2924\n",
      "Epoch [2845/3000], Loss: 0.2935\n",
      "Epoch [2846/3000], Loss: 0.2947\n",
      "Epoch [2847/3000], Loss: 0.2955\n",
      "Epoch [2848/3000], Loss: 0.2926\n",
      "Epoch [2849/3000], Loss: 0.2928\n",
      "Epoch [2850/3000], Loss: 0.2934\n",
      "Epoch [2851/3000], Loss: 0.2941\n",
      "Epoch [2852/3000], Loss: 0.2943\n",
      "Epoch [2853/3000], Loss: 0.2934\n",
      "Epoch [2854/3000], Loss: 0.2921\n",
      "Epoch [2855/3000], Loss: 0.2934\n",
      "Epoch [2856/3000], Loss: 0.2949\n",
      "Epoch [2857/3000], Loss: 0.2980\n",
      "Epoch [2858/3000], Loss: 0.2934\n",
      "Epoch [2859/3000], Loss: 0.2941\n",
      "Epoch [2860/3000], Loss: 0.3029\n",
      "Epoch [2861/3000], Loss: 0.2976\n",
      "Epoch [2862/3000], Loss: 0.2947\n",
      "Epoch [2863/3000], Loss: 0.2988\n",
      "Epoch [2864/3000], Loss: 0.2959\n",
      "Epoch [2865/3000], Loss: 0.2942\n",
      "Epoch [2866/3000], Loss: 0.2951\n",
      "Epoch [2867/3000], Loss: 0.2939\n",
      "Epoch [2868/3000], Loss: 0.2952\n",
      "Epoch [2869/3000], Loss: 0.2993\n",
      "Epoch [2870/3000], Loss: 0.2952\n",
      "Epoch [2871/3000], Loss: 0.2966\n",
      "Epoch [2872/3000], Loss: 0.3020\n",
      "Epoch [2873/3000], Loss: 0.2947\n",
      "Epoch [2874/3000], Loss: 0.3037\n",
      "Epoch [2875/3000], Loss: 0.3110\n",
      "Epoch [2876/3000], Loss: 0.2973\n",
      "Epoch [2877/3000], Loss: 0.3135\n",
      "Epoch [2878/3000], Loss: 0.3059\n",
      "Epoch [2879/3000], Loss: 0.3077\n",
      "Epoch [2880/3000], Loss: 0.3062\n",
      "Epoch [2881/3000], Loss: 0.3010\n",
      "Epoch [2882/3000], Loss: 0.3016\n",
      "Epoch [2883/3000], Loss: 0.2994\n",
      "Epoch [2884/3000], Loss: 0.3018\n",
      "Epoch [2885/3000], Loss: 0.2991\n",
      "Epoch [2886/3000], Loss: 0.2962\n",
      "Epoch [2887/3000], Loss: 0.2977\n",
      "Epoch [2888/3000], Loss: 0.2961\n",
      "Epoch [2889/3000], Loss: 0.2964\n",
      "Epoch [2890/3000], Loss: 0.2962\n",
      "Epoch [2891/3000], Loss: 0.2936\n",
      "Epoch [2892/3000], Loss: 0.2939\n",
      "Epoch [2893/3000], Loss: 0.2947\n",
      "Epoch [2894/3000], Loss: 0.2937\n",
      "Epoch [2895/3000], Loss: 0.2928\n",
      "Epoch [2896/3000], Loss: 0.2937\n",
      "Epoch [2897/3000], Loss: 0.2926\n",
      "Epoch [2898/3000], Loss: 0.2926\n",
      "Epoch [2899/3000], Loss: 0.2923\n",
      "Epoch [2900/3000], Loss: 0.2921\n",
      "Epoch [2901/3000], Loss: 0.2918\n",
      "Epoch [2902/3000], Loss: 0.2918\n",
      "Epoch [2903/3000], Loss: 0.2912\n",
      "Epoch [2904/3000], Loss: 0.2912\n",
      "Epoch [2905/3000], Loss: 0.2916\n",
      "Epoch [2906/3000], Loss: 0.2916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2907/3000], Loss: 0.2933\n",
      "Epoch [2908/3000], Loss: 0.2923\n",
      "Epoch [2909/3000], Loss: 0.2916\n",
      "Epoch [2910/3000], Loss: 0.2915\n",
      "Epoch [2911/3000], Loss: 0.2927\n",
      "Epoch [2912/3000], Loss: 0.2919\n",
      "Epoch [2913/3000], Loss: 0.2921\n",
      "Epoch [2914/3000], Loss: 0.2924\n",
      "Epoch [2915/3000], Loss: 0.2942\n",
      "Epoch [2916/3000], Loss: 0.2919\n",
      "Epoch [2917/3000], Loss: 0.2918\n",
      "Epoch [2918/3000], Loss: 0.2905\n",
      "Epoch [2919/3000], Loss: 0.2920\n",
      "Epoch [2920/3000], Loss: 0.2916\n",
      "Epoch [2921/3000], Loss: 0.2927\n",
      "Epoch [2922/3000], Loss: 0.2941\n",
      "Epoch [2923/3000], Loss: 0.2954\n",
      "Epoch [2924/3000], Loss: 0.2942\n",
      "Epoch [2925/3000], Loss: 0.2947\n",
      "Epoch [2926/3000], Loss: 0.2922\n",
      "Epoch [2927/3000], Loss: 0.2932\n",
      "Epoch [2928/3000], Loss: 0.2918\n",
      "Epoch [2929/3000], Loss: 0.2922\n",
      "Epoch [2930/3000], Loss: 0.2912\n",
      "Epoch [2931/3000], Loss: 0.2913\n",
      "Epoch [2932/3000], Loss: 0.2927\n",
      "Epoch [2933/3000], Loss: 0.2947\n",
      "Epoch [2934/3000], Loss: 0.2981\n",
      "Epoch [2935/3000], Loss: 0.2945\n",
      "Epoch [2936/3000], Loss: 0.2920\n",
      "Epoch [2937/3000], Loss: 0.2984\n",
      "Epoch [2938/3000], Loss: 0.2940\n",
      "Epoch [2939/3000], Loss: 0.2950\n",
      "Epoch [2940/3000], Loss: 0.2944\n",
      "Epoch [2941/3000], Loss: 0.2939\n",
      "Epoch [2942/3000], Loss: 0.2935\n",
      "Epoch [2943/3000], Loss: 0.2917\n",
      "Epoch [2944/3000], Loss: 0.2913\n",
      "Epoch [2945/3000], Loss: 0.2911\n",
      "Epoch [2946/3000], Loss: 0.2911\n",
      "Epoch [2947/3000], Loss: 0.2908\n",
      "Epoch [2948/3000], Loss: 0.2905\n",
      "Epoch [2949/3000], Loss: 0.2909\n",
      "Epoch [2950/3000], Loss: 0.2905\n",
      "Epoch [2951/3000], Loss: 0.2914\n",
      "Epoch [2952/3000], Loss: 0.2937\n",
      "Epoch [2953/3000], Loss: 0.2934\n",
      "Epoch [2954/3000], Loss: 0.2952\n",
      "Epoch [2955/3000], Loss: 0.2945\n",
      "Epoch [2956/3000], Loss: 0.2912\n",
      "Epoch [2957/3000], Loss: 0.2926\n",
      "Epoch [2958/3000], Loss: 0.2905\n",
      "Epoch [2959/3000], Loss: 0.2922\n",
      "Epoch [2960/3000], Loss: 0.2910\n",
      "Epoch [2961/3000], Loss: 0.2927\n",
      "Epoch [2962/3000], Loss: 0.2931\n",
      "Epoch [2963/3000], Loss: 0.2945\n",
      "Epoch [2964/3000], Loss: 0.2934\n",
      "Epoch [2965/3000], Loss: 0.2947\n",
      "Epoch [2966/3000], Loss: 0.2964\n",
      "Epoch [2967/3000], Loss: 0.2959\n",
      "Epoch [2968/3000], Loss: 0.2910\n",
      "Epoch [2969/3000], Loss: 0.2947\n",
      "Epoch [2970/3000], Loss: 0.2921\n",
      "Epoch [2971/3000], Loss: 0.2940\n",
      "Epoch [2972/3000], Loss: 0.2905\n",
      "Epoch [2973/3000], Loss: 0.2929\n",
      "Epoch [2974/3000], Loss: 0.2945\n",
      "Epoch [2975/3000], Loss: 0.3003\n",
      "Epoch [2976/3000], Loss: 0.2986\n",
      "Epoch [2977/3000], Loss: 0.2964\n",
      "Epoch [2978/3000], Loss: 0.2947\n",
      "Epoch [2979/3000], Loss: 0.2980\n",
      "Epoch [2980/3000], Loss: 0.2936\n",
      "Epoch [2981/3000], Loss: 0.2967\n",
      "Epoch [2982/3000], Loss: 0.3025\n",
      "Epoch [2983/3000], Loss: 0.2950\n",
      "Epoch [2984/3000], Loss: 0.3049\n",
      "Epoch [2985/3000], Loss: 0.3065\n",
      "Epoch [2986/3000], Loss: 0.3020\n",
      "Epoch [2987/3000], Loss: 0.3112\n",
      "Epoch [2988/3000], Loss: 0.2978\n",
      "Epoch [2989/3000], Loss: 0.2993\n",
      "Epoch [2990/3000], Loss: 0.3037\n",
      "Epoch [2991/3000], Loss: 0.2954\n",
      "Epoch [2992/3000], Loss: 0.2975\n",
      "Epoch [2993/3000], Loss: 0.2955\n",
      "Epoch [2994/3000], Loss: 0.2942\n",
      "Epoch [2995/3000], Loss: 0.2943\n",
      "Epoch [2996/3000], Loss: 0.2950\n",
      "Epoch [2997/3000], Loss: 0.2928\n",
      "Epoch [2998/3000], Loss: 0.2925\n",
      "Epoch [2999/3000], Loss: 0.2937\n",
      "Epoch [3000/3000], Loss: 0.2929\n",
      "Mean Squared Error: 0.32502448558807373\n",
      "Mean Squared Error: 0.32502448558807373\n",
      "Custom Accuracy (within 10.0% of actual): 22.74%\n",
      "        Actual  Predicted\n",
      "7695  0.227273   0.715702\n",
      "5853  0.363636   0.519269\n",
      "366   0.153846   0.567070\n",
      "146   0.916667   0.631779\n",
      "5543  0.368421   0.545413\n",
      "1547  0.666667   0.978365\n",
      "6740  1.000000   0.431801\n",
      "969   0.675676   0.895360\n",
      "5887  0.375000   0.974153\n",
      "6321  1.000000   0.537235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class SimpleNNRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleNNRegression, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # Second hidden layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)  # Third hidden layer\n",
    "        self.relu3 = nn.ReLU()\n",
    "        #self.fc4 = nn.Linear(input_size, hidden_size)\n",
    "        #self.relu4 = nn.ReLU()\n",
    "        #self.fc5 = nn.Linear(hidden_size, hidden_size)  # Second hidden layer\n",
    "        #self.relu5 = nn.ReLU()\n",
    "        #self.fc6 = nn.Linear(hidden_size, hidden_size)  # Third hidden layer\n",
    "        #self.relu6 = nn.ReLU()\n",
    "        self.fc7 = nn.Linear(hidden_size, 1)  # Output layer for regression\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        #out = self.fc4(x)\n",
    "        #out = self.relu4(out)\n",
    "        #out = self.fc5(out)\n",
    "        #out = self.relu5(out)\n",
    "        #out = self.fc6(out)\n",
    "        #out = self.relu6(out)\n",
    "        out = self.fc7(out)\n",
    "        return out\n",
    "    \n",
    "# Calculate class weights\n",
    "#class_counts = y_train.value_counts()\n",
    "#class_weights = [len(y_train) / class_counts[i] for i in range(len(class_counts))]\n",
    "\n",
    "# Convert class weights to tensor and move to the same device as your model\n",
    "#class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Use the weights in your loss function\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "# Label encode your target variable if it's categorical\n",
    "#label_encoder = LabelEncoder()\n",
    "#y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "#y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "# Convert DataFrames to NumPy arrays for features\n",
    "X_train_np = X_train.values.astype(np.float32)\n",
    "X_test_np = X_test.values.astype(np.float32)\n",
    "\n",
    "# For regression, directly use the continuous target variable from y_train and y_test\n",
    "y_train_np = y_train.values.astype(np.float32)\n",
    "y_test_np = y_test.values.astype(np.float32)\n",
    "\n",
    "# Convert feature and target arrays to tensors\n",
    "X_train_tensor = torch.tensor(X_train_np).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_np).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_np).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_np).to(device)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 500  # for example\n",
    "num_classes = len(np.unique(y_train))\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.002\n",
    "\n",
    "# Create an instance of your model for regression\n",
    "model = SimpleNNRegression(input_size, hidden_size).to(device)\n",
    "\n",
    "# Use a regression loss function like MSELoss\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs.squeeze(), y_train_tensor)  # Squeeze to match dimensions\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Evaluation for regression\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model(X_test_tensor).squeeze()\n",
    "    mse = criterion(predicted, y_test_tensor)\n",
    "    print(f'Mean Squared Error: {mse.item()}')\n",
    "    \n",
    "    \n",
    "    # Evaluation for regression\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model(X_test_tensor).squeeze()\n",
    "    mse = criterion(predicted, y_test_tensor)\n",
    "    print(f'Mean Squared Error: {mse.item()}')\n",
    "\n",
    "    # Calculate a custom 'accuracy' metric for regression\n",
    "    threshold = 0.10  # 10% of the actual value\n",
    "    within_threshold = torch.abs(predicted - y_test_tensor) <= (threshold * torch.abs(y_test_tensor))\n",
    "    custom_accuracy = torch.mean(within_threshold.float()) * 100\n",
    "    print(f'Custom Accuracy (within {threshold*100}% of actual): {custom_accuracy:.2f}%')\n",
    "    \n",
    "# Convert the predictions and actual values to a Pandas DataFrame for easier handling\n",
    "predicted_np = predicted.cpu().numpy()\n",
    "y_test_np = y_test_tensor.cpu().numpy()\n",
    "comparison_df = pd.DataFrame({'Actual': y_test_np, 'Predicted': predicted_np})\n",
    "\n",
    "# Randomly sample some examples from the DataFrame\n",
    "num_examples_to_show = 10  # You can change this number\n",
    "random_examples = comparison_df.sample(n=num_examples_to_show)\n",
    "\n",
    "print(random_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84505270",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3357a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
